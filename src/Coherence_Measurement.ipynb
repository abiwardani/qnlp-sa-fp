{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6ddab11",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e14f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Parameters\n",
    "\n",
    "# Experiment conditions\n",
    "use_all_rewriter_rules = True\n",
    "convert_bigraph = True\n",
    "not_representation = \"X\"\n",
    "not_placement = \"end\"\n",
    "gate_layers = [\"H\", \"Rx\"]\n",
    "var_layers = 1\n",
    "parameterize_nouns = False\n",
    "# independent_sublayer = False\n",
    "\n",
    "# load params\n",
    "train_params = True\n",
    "load_params = True\n",
    "# param_vals = [0.6931585473443279, 1.0462424956825283, 1.234537983904686, -2.90716906835752, 1.5753587952252757, -2.4968837592255424, 1.6118961289272027, -1.016327328767516, -0.3142432619217712, 1.208915586540799, -1.1801888411140051, -0.4286556908790561, 3.3779520000989214, 0.04286513605390508, 0.5295399156230165, -1.7699845136573933, 0.5233673239374572, -1.748569530235187, -0.26823991786760143, 1.813789372735753, -2.9079130258111228, -0.6129166776514257, 0.3401131125827421, -0.4365025178512618, 1.008439272978763, 0.06192528487377933, 1.6121247434136865, 3.3613679248015504, 4.1316674342901845, 0.8303800120316661, -1.227825304579597, -1.8130593574910823, 0.1848756781930945, -0.676881418623572, 2.591255835558515, 1.6673593894980614, 1.6540892586338571, 1.6143400480675385, 0.23932506846152854, 1.5181285887362506, 0.13241105800686212, 3.8061670131492367, 0.5807166437788316, 1.5116799923186133, 1.5599307705774368, 0.29362851954048996, 0.19967375719855993, 1.914813488873402, 2.7644195885210587, 0.37942837286458875, 2.3383678677227993, -0.5843868818793704, 3.921194364357892, 1.4174799045629805, 2.741638785064822, -0.6304530862551285, -1.8933985338953436, 1.5624277556723982, -1.1677654133728248, -0.8250732415577517, 1.7230226879389658, -0.8418763059243636, 0.9691454037142504, 0.7252400407386729, -1.9797782989274328, 2.2739476528314704, 0.4612678359290186, 2.2453750405051833, 1.2218764611893127, -1.4926462511306318, 1.5083681953206398, -0.6209409896223363, -0.8575621399562154, -1.1360971020856898, -0.32413810704072976, 2.317045883444611, 0.052405537301035005]\n",
    "# params_saved = {'I__n_0': 1.3926202901320248, 'I__n_1': 0.008448820382520305, 'I__n_2': -0.6385783774702594, 'aw__n.r@s_0': 0.004878974097219957, 'aw__n@n.l_0': -1.710437394121971, 'bad__n.r@s_0': -2.0120646244526674, 'bad__n@n.l_0': -0.33328110433838587, 'bland__n.r@s_0': -1.8011492489090937, 'bland__n@n.l_0': -0.23301002690494765, 'cook__n.r@s@n.l_0': 2.666641605278804, 'cook__n.r@s@n.l_1': 0.35782850997078663, 'delici__n.r@s_0': -0.6784353476839611, 'delici__n@n.l_0': 2.5048532581915994, 'dislik__n.r@s@n.l_0': 1.264688638132907, 'dislik__n.r@s@n.l_1': 2.9058201511277773, 'fast__n.r@s_0': 1.5228059027468268, 'fast__n@n.l_0': 1.4318617692717328, 'food__n_0': 0.9510109180885518, 'food__n_1': 1.983545943335132, 'food__n_2': 0.31422754989262236, 'good__n.r@s_0': -0.37898061288101936, 'good__n@n.l_0': 0.3417258364922475, 'great__n.r@s_0': 0.32023762451562343, 'great__n@n.l_0': 1.9344786256084243, 'had__n.r@s@n.l_0': 0.9175005505289897, 'had__n.r@s@n.l_1': -1.126491771734188, 'hate__n.r@s@n.l_0': 1.0178843262642405, 'hate__n.r@s@n.l_1': -0.8544213946932742, 'impecc__n.r@s_0': 0.8774940821788493, 'impecc__n@n.l_0': 0.28533233081922516, 'like__n.r@s@n.l_0': 0.5238711836690633, 'like__n.r@s@n.l_1': -0.0248482491004431, 'love__n.r@s@n.l_0': -1.0716522290529822, 'love__n.r@s@n.l_1': 0.4674574117431039, 'meal__n_0': -0.2913227630628624, 'meal__n_1': -0.7824158049259571, 'meal__n_2': 2.364685233940882, 'nice__n.r@s_0': -0.31906215984000647, 'nice__n@n.l_0': -2.087345064482271, 'restaur__n_0': 1.9567881690203595, 'restaur__n_1': 0.35319298096689367, 'restaur__n_2': -0.5953604557383299, 'rude__n.r@s_0': 2.1631540155864895, 'rude__n@n.l_0': -0.9753409914505806, 'servic__n_0': -2.7047864895209357, 'servic__n_1': 1.2351321590961786, 'servic__n_2': -1.5851261977616085, 'show__n.r@s@n.l_0': -0.6393012792612347, 'show__n.r@s@n.l_1': 1.7857303585103954, 'slow__n.r@s_0': 0.5156333465906835, 'slow__n@n.l_0': 1.0425765284092643, 'staff__n_0': 0.08543447326700565, 'staff__n_1': -2.068154595353262, 'staff__n_2': 0.019379227469981927, 'tasti__n.r@s_0': 1.2246133069435021, 'tasti__n@n.l_0': 1.9668107229444722, 'terribl__n.r@s_0': -0.3440262636975905, 'terribl__n@n.l_0': 2.1861199395142688, 'unappet__n.r@s_0': 1.440589048292482, 'unappet__n@n.l_0': -0.31598863954665035, 'wa__s@s.l_0': -1.0244128683696618}\n",
    "spsa_a = 0.2\n",
    "spsa_c = 0.06\n",
    "spsa_n_iter = 0\n",
    "k0 = 0\n",
    "\n",
    "# SVM type\n",
    "kernel = 'rbf'\n",
    "default_svm = True\n",
    "optimize_svm = False\n",
    "\n",
    "# Experiment metadata\n",
    "experiment_name = \"misc\"\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31bf0143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment folder already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "if (experiment_name == \"\"):\n",
    "    experiment_name = \"misc\"\n",
    "\n",
    "path = '../data/experiment_results/journal/generalQC/'+experiment_name\n",
    "\n",
    "try:\n",
    "    os.mkdir(path)\n",
    "except:\n",
    "    print(\"experiment folder already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c950b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3fba1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc8e7669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_data(filename):\n",
    "    labels, sentences = [], []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            labels.append([1, 0] if line[0] == '1' else [0, 1])\n",
    "            sentences.append(line[1:].strip())\n",
    "    return np.array(labels), sentences\n",
    "\n",
    "test_targets_src, test_data_src = read_data('../data/datasets/restaurant_v5_test.txt')\n",
    "dev_targets_src, dev_data_src = read_data('../data/datasets/restaurant_v5_dev.txt')\n",
    "train_targets_src, train_data_src = read_data('../data/datasets/restaurant_v5_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58ab1ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for converting to bigraph\n",
    "\n",
    "from discopy.rigid import Id as RigidId\n",
    "\n",
    "def checkTrailingCups(diagram):\n",
    "    scanWords = True\n",
    "    \n",
    "    for box in diagram.boxes:\n",
    "        if not box.dom and not scanWords:\n",
    "            return False\n",
    "        else:\n",
    "            scanWords = scanWords and not box.dom\n",
    "    \n",
    "    return True\n",
    "\n",
    "def convertToTrailingCups(diagram):\n",
    "    if (checkTrailingCups(diagram)):\n",
    "        return diagram\n",
    "\n",
    "    words = []\n",
    "    cups = []\n",
    "    \n",
    "    for box in diagram.boxes:\n",
    "        if not box.dom:\n",
    "            words = words + [box]\n",
    "        else:\n",
    "            cups = [box] + cups\n",
    "    \n",
    "    new_diag = words[0]\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if i != 0:\n",
    "            new_diag = new_diag >> RigidId(new_diag.cod) @ word\n",
    "    \n",
    "    for i, cup in enumerate(cups):\n",
    "        if i != len(cups)-1:\n",
    "            new_diag = new_diag >> RigidId(new_diag.cod[:-2]) @ cup\n",
    "        else:\n",
    "            new_diag = new_diag >> cup @ RigidId(new_diag.cod[2:])\n",
    "    \n",
    "    return new_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fbe9d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucntion for stemming and lemmatization of tokens\n",
    "\n",
    "def to_word_tokens(data):\n",
    "    return [word_tokenize(record) for record in data]\n",
    "\n",
    "def build_stem_dictionary(data):\n",
    "    port = PorterStemmer()\n",
    "    wnet = WordNetLemmatizer()\n",
    "    \n",
    "    mapping = {}\n",
    "    \n",
    "    data_as_tokens = to_word_tokens(data)\n",
    "    \n",
    "    for words in data_as_tokens:\n",
    "        for word in words:\n",
    "            if word not in mapping:\n",
    "                stemmed_word = port.stem(word)\n",
    "                lemmatized_word = wnet.lemmatize(stemmed_word)\n",
    "                \n",
    "                mapping[word] = lemmatized_word\n",
    "    \n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6454a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for stemming and lemmatization of diagram boxes\n",
    "\n",
    "from lambeq.rewrite import RewriteRule\n",
    "\n",
    "class StemRewriteRule(RewriteRule):\n",
    "    def __init__(self, data):\n",
    "        self.mapping = build_stem_dictionary(data)\n",
    "    \n",
    "    def matches(self, box):\n",
    "        return box.name in self.mapping\n",
    "\n",
    "    def rewrite(self, box):\n",
    "        new_name = self.mapping[box.name]\n",
    "        return type(box)(name=new_name, dom=box.dom, cod=box.cod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85c3fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq.ansatz import CircuitAnsatz\n",
    "from abc import abstractmethod\n",
    "# from collections.abc import Mapping\n",
    "from itertools import cycle\n",
    "from typing import Callable, Optional, Tuple, Mapping\n",
    "from discopy.quantum.circuit import (Circuit, Functor, Id, qubit)\n",
    "from discopy.quantum.gates import Bra, Ket, Controlled, Rotation\n",
    "from discopy.quantum.gates import H, X, Y, Z, Rx, Ry, Rz\n",
    "from discopy.rigid import Box, Diagram, Ty\n",
    "from discopy.tensor import Dim, Tensor\n",
    "import numpy as np\n",
    "from sympy import Symbol, symbols\n",
    "\n",
    "class GeneralQCLayer(Circuit):\n",
    "    \n",
    "    def __init__(self, n_qubits, params):\n",
    "        from discopy.quantum.gates import Rx, Rz\n",
    "\n",
    "        if len(self.gate_layers) != 2:\n",
    "            raise ValueError(\"Expected gate_layers as tuple of strings\")\n",
    "        \n",
    "        g1, g2 = self.gate_layers\n",
    "\n",
    "        if (g1 == None) or (g2 == None):\n",
    "            raise ValueError(\"gate_layers must be in discopy.quantum.gates\")\n",
    "        # if not (abs(self.r1) == 1 and abs(self.r2) == 1) and ((abs(self.r1) == 1 or abs(self.r2) == 1) and (self.r2 % self.r1 == 0 or self.r1 % self.r2 == 0)) or (n_qubits % self.r1 == 0 and self.r1 != 1) or (n_qubits % self.r2 == 0 and self.r2 != 1):\n",
    "        #     raise ValueError(\"n_qubits, r1, and r2 must be co-prime\")\n",
    "\n",
    "        params_shape = np.shape(params)\n",
    "\n",
    "        if n_qubits == 1:\n",
    "            if len(params) == 0:\n",
    "                circuit = Id(1)\n",
    "            else:\n",
    "                circuit = Rx(params[0]) >> Rz(params[1]) >> Rx(params[2])\n",
    "        elif (len(params_shape) != 2):\n",
    "            raise ValueError(\n",
    "                \"Expected params of shape (depth, {})\".format(n_qubits))\n",
    "        else:\n",
    "            g1_thetas = 0\n",
    "            g2_thetas = 0\n",
    "\n",
    "            if g1.free_symbols != {}:\n",
    "                # g1 is fixed\n",
    "                g1_thetas = n_qubits\n",
    "            if g2.free_symbols != {}:\n",
    "                g2_thetas = n_qubits\n",
    "\n",
    "            if self.reuse_params:\n",
    "                self.k = 1\n",
    "            else:\n",
    "                self.k = 2\n",
    "            \n",
    "            n_thetas = self.k*(g1_thetas + g2_thetas)\n",
    "\n",
    "            if (params_shape[1] != n_thetas):\n",
    "                raise ValueError(\n",
    "                    \"Expected component params with length {}\".format(n_thetas))\n",
    "\n",
    "            # ANSATZ ALGORITHM\n",
    "            circuit = Id(n_qubits)\n",
    "            \n",
    "            # for {theta} in labelled params\n",
    "            for thetas in params:\n",
    "                \n",
    "                # sublayer 1 non-entangling block\n",
    "                if g1_thetas == 0:\n",
    "                    # if g1 is fixed\n",
    "                    sublayer1 = Id().tensor(*([g1 for _ in range(n_qubits)]))\n",
    "                else:\n",
    "                    # if g1 is trainable\n",
    "                    sublayer1 = Id().tensor(*([g1(theta) for theta in thetas[:g1_thetas]]))\n",
    "                \n",
    "                # sublayer 1 entangled block\n",
    "                ctrl = 0\n",
    "\n",
    "                for i in range(n_qubits):\n",
    "                    # shift target := control - r1\n",
    "                    tgt = (ctrl - self.r1) % n_qubits\n",
    "                    \n",
    "                    if g2_thetas == 0:\n",
    "                        sublayer1 = sublayer1._apply_controlled(g2, ctrl, tgt)\n",
    "                        # sublayer1 = sublayer1.CX(ctrl, tgt)\n",
    "                    else:\n",
    "                        sublayer1 = sublayer1._apply_controlled(g2(thetas[g1_thetas + i]), ctrl, tgt)\n",
    "                        # sublayer1 = sublayer1.CRx(thetas[g1_thetas + i], ctrl, tgt)\n",
    "                    \n",
    "                    ctrl = tgt\n",
    "                \n",
    "                # sublayer 2 non-entangling block\n",
    "                if g1_thetas == 0:\n",
    "                    sublayer2 = Id().tensor(*([g1 for _ in range(n_qubits)]))\n",
    "                else:\n",
    "                    if self.reuse_params:\n",
    "                        sublayer2 = Id().tensor(*([g1(theta) for theta in thetas[:g1_thetas]]))\n",
    "                    else:\n",
    "                        sublayer2 = Id().tensor(*([g1(theta) for theta in thetas[g1_thetas+g2_thetas:2*g1_thetas+g2_thetas]]))\n",
    "                \n",
    "                # sublayer 2 entangled block\n",
    "                ctrl = 0\n",
    "\n",
    "                for i in range(n_qubits):\n",
    "                    # shift target := control - r2\n",
    "                    tgt = (ctrl - self.r2) % n_qubits\n",
    "\n",
    "                    if g2_thetas == 0:\n",
    "                        sublayer2 = sublayer2._apply_controlled(g2, ctrl, tgt)\n",
    "                        # sublayer2 = sublayer2.CX(ctrl, tgt)\n",
    "                    else:\n",
    "                        if self.reuse_params:\n",
    "                            sublayer2 = sublayer2._apply_controlled(g2(thetas[g1_thetas + i]), ctrl, tgt)\n",
    "                        else:\n",
    "                            sublayer2 = sublayer2._apply_controlled(g2(thetas[2*g1_thetas+g2_thetas+i]), ctrl, tgt)\n",
    "                            # sublayer2 = sublayer2.CRx(thetas[2*g1_thetas+g2_thetas+i], ctrl, tgt)\n",
    "                    \n",
    "                    ctrl = tgt\n",
    "            \n",
    "                # compose circuit\n",
    "                circuit >>= sublayer1 >> sublayer2\n",
    "            \n",
    "        super().__init__(\n",
    "            circuit.dom, circuit.cod, circuit.boxes, circuit.offsets)\n",
    "\n",
    "class GeneralQCAnsatz(CircuitAnsatz):\n",
    "    def __init__(self, ob_map: Mapping[Ty, int], n_layers: int, n_single_qubit_params: int = 0, gate_config = {\"gate_layers\": [H, Rx], \"var_layers\": 1}, r = (1, 3), reuse_params=True, discard: bool = False) -> None:\n",
    "        self.gate_config = gate_config\n",
    "        self.reuse_params = reuse_params\n",
    "        self.r = r\n",
    "        \n",
    "        class GeneralQCInterface(GeneralQCLayer):\n",
    "            \"\"\" Interface for GeneralQCLayer \"\"\"\n",
    "\n",
    "            def strToGate(gateStr):\n",
    "                if gateStr == \"H\":\n",
    "                    return H\n",
    "                if gateStr == \"X\":\n",
    "                    return X\n",
    "                if gateStr == \"Y\":\n",
    "                    return Y\n",
    "                if gateStr == \"Z\":\n",
    "                    return Z\n",
    "                if gateStr == \"Rx\":\n",
    "                    return Rx\n",
    "                if gateStr == \"Ry\":\n",
    "                    return Ry\n",
    "                if gateStr == \"Rz\":\n",
    "                    return Rz\n",
    "                \n",
    "                return Id(1)\n",
    "            \n",
    "            assert(len(self.gate_config[\"gate_layers\"]) == 2)\n",
    "\n",
    "            g1, g2 = self.gate_config[\"gate_layers\"]\n",
    "\n",
    "            if isinstance(g1, str):\n",
    "                g1 = strToGate(g1)\n",
    "            if isinstance(g2, str):\n",
    "                g2 = strToGate(g2)\n",
    "\n",
    "            gate_layers = [g1, g2]\n",
    "            r1, r2 = self.r\n",
    "            reuse_params = self.reuse_params\n",
    "\n",
    "        super().__init__(ob_map, n_layers, n_single_qubit_params, GeneralQCInterface, discard, [Rx, Rz])\n",
    "\n",
    "    def params_shape(self, n_qubits: int) -> Tuple[int, ...]:\n",
    "        if self.reuse_params:\n",
    "            k = 1\n",
    "        else:\n",
    "            k = 2\n",
    "        \n",
    "        return (self.n_layers, k*self.gate_config[\"var_layers\"]*n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76391143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSA(Circuit):\n",
    "    def __init__(self, n_qubits, params):\n",
    "        from discopy.quantum.gates import H, Rx, Rz, CX, Ry\n",
    "\n",
    "        if n_qubits == 1:\n",
    "            if len(params) == 0:\n",
    "                circuit = Id(1)\n",
    "            else:\n",
    "                circuit = Rx(params[0]) >> Rz(params[1]) >> Rx(params[2])\n",
    "        elif len(np.shape(params)) != 2\\\n",
    "                or np.shape(params)[1] != n_qubits - 1:\n",
    "            raise ValueError(\n",
    "                \"Expected params of shape (depth, {})\".format(n_qubits - 1))\n",
    "        else:\n",
    "            depth = np.shape(params)[0]\n",
    "            circuit = Id(n_qubits)\n",
    "\n",
    "            for thetas in params:\n",
    "                hadamards = Id().tensor(*(n_qubits * [H]))\n",
    "                rotations = Id(n_qubits).then(*(\n",
    "                    (Id(i) @ CX @ Id(n_qubits - 2 - i)) >> (Id(i + 1) @ Rz(thetas[i]) @ Id(n_qubits - 2 - i) >> (Id(i + 1) @ H @ Id(n_qubits - 2 - i)))\n",
    "                    for i in range(n_qubits - 1)))\n",
    "                circuit >>= hadamards >> rotations\n",
    "\n",
    "        super().__init__(circuit.dom, circuit.cod, circuit.boxes, circuit.offsets)\n",
    "\n",
    "class SimpleSAAnsatz(CircuitAnsatz):\n",
    "    def __init__(self, ob_map: Mapping[Ty, int], n_layers: int, n_single_qubit_params: int = 0, discard: bool = False) -> None:\n",
    "        super().__init__(ob_map, n_layers, n_single_qubit_params, SimpleSA, discard, [Rx, Rz], H)\n",
    "\n",
    "    def params_shape(self, n_qubits: int) -> Tuple[int, ...]:\n",
    "        return (self.n_layers, n_qubits - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "375e08f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import BobcatParser, Rewriter, remove_cups\n",
    "from discopy import grammar\n",
    "from discopy.quantum.gates import X, Z\n",
    "from discopy import rigid\n",
    "\n",
    "def sentences_to_circuits(sentences, ansatz, convert_bigraph=True, not_representation=\"X\", all_sentences=None, return_valids_mask=True):\n",
    "    ### SENTENCES TO DIAGRAMS ###\n",
    "\n",
    "    if all_sentences is None:\n",
    "        all_sentences = sentences\n",
    "    \n",
    "    # syntax tree parsing\n",
    "    parser = BobcatParser()\n",
    "    raw_diagrams = parser.sentences2diagrams([text.replace(\" not \", \" \") for text in sentences])\n",
    "\n",
    "    # filter valid diagrams type S\n",
    "    n_sent = len(sentences)\n",
    "\n",
    "    valids_mask = np.array([d.cod.name == Ty('s').name for d in raw_diagrams])\n",
    "    data = [sentences[i] for i in range(n_sent) if valids_mask[i]]\n",
    "    use_diagrams = [raw_diagrams[i] for i in range(n_sent) if valids_mask[i]]\n",
    "\n",
    "    # grammatical rewrite rules\n",
    "    rewriter = Rewriter()\n",
    "    rewritten_diagrams = [rewriter(diagram) for diagram in use_diagrams]\n",
    "\n",
    "    # bigraph method\n",
    "    normalised_diagrams = [convertToTrailingCups(diagram.normal_form()) for diagram in rewritten_diagrams]\n",
    "\n",
    "    removed_diagrams = [remove_cups(diagram) for diagram in normalised_diagrams]\n",
    "\n",
    "    # stemming and lemmatization\n",
    "    stemmed_diagrams = [Rewriter([StemRewriteRule(all_sentences)])(diagram) for diagram in removed_diagrams]\n",
    "\n",
    "    # final diagrams\n",
    "    diagrams = [diagram for diagram in stemmed_diagrams]\n",
    "\n",
    "    ### DIAGRAMS to CIRCUITS ###\n",
    "\n",
    "    # string diagrams to raw quantum circuits\n",
    "    circuits = [ansatz(diagram) for diagram in diagrams]\n",
    "\n",
    "    # apply NOT box to circuits\n",
    "    for i, circuit in enumerate(circuits):\n",
    "        if data[i].find(\" not \") != -1:\n",
    "            if (not_representation == \"ZX\"):\n",
    "                circuits[i] = circuit >> Z >> X\n",
    "            else:\n",
    "                circuits[i] = circuit >> X\n",
    "    \n",
    "    if return_valids_mask:\n",
    "        return data, diagrams, circuits, valids_mask\n",
    "    else:\n",
    "        return data, diagrams, circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e0399",
   "metadata": {},
   "source": [
    "# Compile Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73c474ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "working_dir = \"../data/experiment_results/journal/generalQC/\"\n",
    "experiment_book = []\n",
    "\n",
    "for experiment_name in os.listdir(working_dir):\n",
    "    experiment_dir = working_dir+experiment_name+\"/\"\n",
    "    if os.path.isdir(experiment_dir):\n",
    "        if \"test_800_accs.csv\" in os.listdir(experiment_dir) and \"faulty\" not in experiment_name:\n",
    "            experiment_dict = {\"name\": experiment_name}\n",
    "\n",
    "            with open(experiment_dir+\"params.txt\", \"r\") as f:\n",
    "                content = f.read().replace(\"\\'\", \"\\\"\")\n",
    "                params_dict = json.loads(content)\n",
    "                f.close()\n",
    "            \n",
    "            experiment_dict[\"params\"] = params_dict\n",
    "\n",
    "            info = experiment_name.split(\"_\")\n",
    "\n",
    "            experiment_dict[\"data_version\"] = info[0]\n",
    "\n",
    "            if info[1] == \"SimpleSA\":\n",
    "                experiment_dict[\"ansatz\"] = \"SimpleSAAnsatz\"\n",
    "            else:\n",
    "                experiment_dict[\"ansatz\"] = \"GeneralQCAnsatz\"\n",
    "\n",
    "                experiment_dict[\"gate_layers\"] = [info[1], info[2][1:]]\n",
    "\n",
    "                experiment_dict[\"var_layers\"] = (info[1]+info[2]).count(\"R\")\n",
    "            \n",
    "            experiment_dict[\"n_layers\"] = 1\n",
    "\n",
    "            for i in info[1:]:\n",
    "                if \"iter\" in i:\n",
    "                    experiment_dict[\"iter\"] = int(i[:-4])\n",
    "                    break\n",
    "            \n",
    "            if \"no\" in info:\n",
    "                experiment_dict[\"parameterize_nouns\"] = False\n",
    "            elif \"with\" in info:\n",
    "                experiment_dict[\"parameterize_nouns\"] = True\n",
    "        \n",
    "            experiment_book.append(experiment_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee3f1945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'v5_H_CRx_800iter_with_nouns_take2',\n",
       "  'params': {'amaz__n.r@s_0': 1.3548387429641024,\n",
       "   'amaz__n.r@s_1': -0.40968958996573457,\n",
       "   'amaz__n@n.l_0': 0.46446769165749563,\n",
       "   'amaz__n@n.l_1': -0.7897502222614423,\n",
       "   'aw__n.r@s_0': 0.19890282546648633,\n",
       "   'aw__n.r@s_1': 0.7505664359118243,\n",
       "   'aw__n@n.l_0': -0.13677572719082645,\n",
       "   'aw__n@n.l_1': 1.1432242545339935,\n",
       "   'bad__n.r@s_0': 0.818491469732785,\n",
       "   'bad__n.r@s_1': 1.3232750569429426,\n",
       "   'bad__n@n.l_0': 0.309841347177341,\n",
       "   'bad__n@n.l_1': 0.7369628206871559,\n",
       "   'bland__n.r@s_0': 0.5690313574389705,\n",
       "   'bland__n.r@s_1': -0.6009589913097046,\n",
       "   'bland__n@n.l_0': 0.7095547429955995,\n",
       "   'bland__n@n.l_1': 0.6482175256399125,\n",
       "   'cafe__n_0': 0.5563202593375558,\n",
       "   'cafe__n_1': 1.1102817042496054,\n",
       "   'cafe__n_2': 0.7869632558429331,\n",
       "   'cook__n.r@s@n.l_0': -0.18361276867135554,\n",
       "   'cook__n.r@s@n.l_1': 0.8894794651507758,\n",
       "   'cook__n.r@s@n.l_2': -0.5676865473678465,\n",
       "   'cook__n_0': -0.0871869484560759,\n",
       "   'cook__n_1': 1.2751344781156584,\n",
       "   'cook__n_2': 1.2346011446565368,\n",
       "   'delici__n.r@s_0': -0.3423389184605571,\n",
       "   'delici__n.r@s_1': 1.6282186553162734,\n",
       "   'delici__n@n.l_0': 0.24570905491932157,\n",
       "   'delici__n@n.l_1': 1.5912927597039428,\n",
       "   'dish__n_0': 0.8948996173029191,\n",
       "   'dish__n_1': 1.4077772467661651,\n",
       "   'dish__n_2': 0.16687831072824214,\n",
       "   'dislik__n.r@s@n.l_0': 0.8829097040054295,\n",
       "   'dislik__n.r@s@n.l_1': 0.00532752405338696,\n",
       "   'dislik__n.r@s@n.l_2': 1.259579185412011,\n",
       "   'excel__n.r@s_0': 1.2005939772946728,\n",
       "   'excel__n.r@s_1': 1.4495307385940652,\n",
       "   'excel__n@n.l_0': -0.15977988988759687,\n",
       "   'excel__n@n.l_1': 1.5698727473821368,\n",
       "   'fast__n.r@s_0': -0.5676401997916904,\n",
       "   'fast__n.r@s_1': 0.9011817671038247,\n",
       "   'fast__n@n.l_0': -0.2447996461074705,\n",
       "   'fast__n@n.l_1': 1.2094808549154972,\n",
       "   'food__n_0': 0.7984729419032632,\n",
       "   'food__n_1': 0.6620077416706938,\n",
       "   'food__n_2': 1.8116695287930913,\n",
       "   'foodservic__n_0': 0.7485384081177493,\n",
       "   'foodservic__n_1': -0.25380560227309784,\n",
       "   'foodservic__n_2': 1.0551077244302316,\n",
       "   'good__n.r@s_0': 1.4939861404294996,\n",
       "   'good__n.r@s_1': 0.6021445441545323,\n",
       "   'good__n@n.l_0': 1.1655299551499223,\n",
       "   'good__n@n.l_1': 1.1612402462796767,\n",
       "   'great__n.r@s_0': 0.4161791483178209,\n",
       "   'great__n.r@s_1': -0.7386298346601656,\n",
       "   'great__n@n.l_0': 0.7100167408127303,\n",
       "   'great__n@n.l_1': 0.6649127362322125,\n",
       "   'had__n.r@s@n.l_0': 0.2757785639962078,\n",
       "   'had__n.r@s@n.l_1': 1.0716603778022131,\n",
       "   'had__n.r@s@n.l_2': -0.4846258128685169,\n",
       "   'hate__n.r@s@n.l_0': 0.9555105420258994,\n",
       "   'hate__n.r@s@n.l_1': 0.7385129608573815,\n",
       "   'hate__n.r@s@n.l_2': -0.9057692650143729,\n",
       "   'horribl__n.r@s_0': -0.14614460680956895,\n",
       "   'horribl__n.r@s_1': 1.0608822627791792,\n",
       "   'horribl__n@n.l_0': 1.3187968187672219,\n",
       "   'horribl__n@n.l_1': 1.211942158170499,\n",
       "   'i__n_0': 0.21338768301272723,\n",
       "   'i__n_1': -0.0945974008880786,\n",
       "   'i__n_2': 0.5127230302020738,\n",
       "   'impecc__n.r@s_0': 1.1739587942015013,\n",
       "   'impecc__n.r@s_1': 1.4745559646369295,\n",
       "   'impecc__n@n.l_0': 0.7492396312356675,\n",
       "   'impecc__n@n.l_1': 1.7215177165413316,\n",
       "   'kind__n.r@s_0': 0.11764097689842447,\n",
       "   'kind__n.r@s_1': 1.353241630848178,\n",
       "   'kind__n@n.l_0': -0.4370549142130727,\n",
       "   'kind__n@n.l_1': 1.4926648400944056,\n",
       "   'like__n.r@s@n.l_0': 1.440179378317805,\n",
       "   'like__n.r@s@n.l_1': 0.9496250236165473,\n",
       "   'like__n.r@s@n.l_2': 0.3311509913979628,\n",
       "   'lousi__n.r@s_0': -0.5957916438516884,\n",
       "   'lousi__n.r@s_1': 0.3389969562821663,\n",
       "   'lousi__n@n.l_0': 1.462202672430934,\n",
       "   'lousi__n@n.l_1': 1.0086413896754833,\n",
       "   'love__n.r@s@n.l_0': -0.0057825184822388595,\n",
       "   'love__n.r@s@n.l_1': 0.19527395265870492,\n",
       "   'love__n.r@s@n.l_2': 0.018917314579316745,\n",
       "   'meal__n_0': -0.47592664474156304,\n",
       "   'meal__n_1': 1.4522443155636653,\n",
       "   'meal__n_2': 1.7279864860411926,\n",
       "   'nice__n.r@s_0': 1.364573740293923,\n",
       "   'nice__n.r@s_1': -0.7172955437720991,\n",
       "   'nice__n@n.l_0': 0.5093513355804887,\n",
       "   'nice__n@n.l_1': 1.4903730455737827,\n",
       "   'poor__n.r@s_0': 0.3004832870566322,\n",
       "   'poor__n.r@s_1': 0.9269724385652787,\n",
       "   'poor__n@n.l_0': 1.3905779791901631,\n",
       "   'poor__n@n.l_1': 0.14381426031641034,\n",
       "   'restaur__n_0': 1.3387837858870708,\n",
       "   'restaur__n_1': -0.4508138074464033,\n",
       "   'restaur__n_2': -0.32055348757053803,\n",
       "   'rude__n.r@s_0': 0.4711446798352404,\n",
       "   'rude__n.r@s_1': 0.2842256503448778,\n",
       "   'rude__n@n.l_0': 1.108575011018078,\n",
       "   'rude__n@n.l_1': 1.0144538904889677,\n",
       "   'servic__n_0': 1.4036251888656066,\n",
       "   'servic__n_1': 0.401340086563827,\n",
       "   'servic__n_2': -0.29059213196627265,\n",
       "   'show__n.r@s@n.l_0': 0.38355023800214605,\n",
       "   'show__n.r@s@n.l_1': 0.9672719364401363,\n",
       "   'show__n.r@s@n.l_2': -0.5009609769615575,\n",
       "   'slow__n.r@s_0': -0.1742595099873864,\n",
       "   'slow__n.r@s_1': -0.4854346057870236,\n",
       "   'slow__n@n.l_0': -0.16757109120975303,\n",
       "   'slow__n@n.l_1': 1.1282247274923358,\n",
       "   'staff__n_0': 1.3397285067597315,\n",
       "   'staff__n_1': -0.5514125961016505,\n",
       "   'staff__n_2': 0.6246033807705262,\n",
       "   'tasti__n.r@s_0': 0.7133738453446998,\n",
       "   'tasti__n.r@s_1': 0.6206325760326519,\n",
       "   'tasti__n@n.l_0': 1.135508931039068,\n",
       "   'tasti__n@n.l_1': 0.22839160241876752,\n",
       "   'terribl__n.r@s_0': 0.10107926501439181,\n",
       "   'terribl__n.r@s_1': 1.113422009652562,\n",
       "   'terribl__n@n.l_0': 0.1729725509900462,\n",
       "   'terribl__n@n.l_1': 0.7887820793671689,\n",
       "   'unappet__n.r@s_0': -0.9174708518668786,\n",
       "   'unappet__n.r@s_1': -0.8149983148924347,\n",
       "   'unappet__n@n.l_0': 0.17682451123898327,\n",
       "   'unappet__n@n.l_1': 1.3270151715747143,\n",
       "   'unpleas__n.r@s_0': 0.6928653739285406,\n",
       "   'unpleas__n.r@s_1': 1.4960003503961838,\n",
       "   'unpleas__n@n.l_0': 0.254206322992376,\n",
       "   'unpleas__n@n.l_1': 0.8170090820074032,\n",
       "   'waiter__n_0': -0.343463890393225,\n",
       "   'waiter__n_1': 1.4355148049816058,\n",
       "   'waiter__n_2': -0.0833260818781938,\n",
       "   'yummi__n.r@s_0': 0.7206916533045415,\n",
       "   'yummi__n.r@s_1': 0.732688436038152,\n",
       "   'yummi__n@n.l_0': 0.9966147569363768,\n",
       "   'yummi__n@n.l_1': 0.8315387990620471},\n",
       "  'data_version': 'v5',\n",
       "  'ansatz': 'GeneralQCAnsatz',\n",
       "  'gate_layers': ['H', 'Rx'],\n",
       "  'var_layers': 1,\n",
       "  'n_layers': 1,\n",
       "  'iter': 800,\n",
       "  'parameterize_nouns': True},\n",
       " {'name': 'v5_Rx_CRz_800iter_with_nouns_take2',\n",
       "  'params': {'amaz__n.r@s_0': 1.4751027369238066,\n",
       "   'amaz__n.r@s_1': -0.8504430274560033,\n",
       "   'amaz__n.r@s_2': 0.5160386014006054,\n",
       "   'amaz__n.r@s_3': -0.13904314162497977,\n",
       "   'amaz__n@n.l_0': 1.2301212532411014,\n",
       "   'amaz__n@n.l_1': -1.308863978161642,\n",
       "   'amaz__n@n.l_2': 0.15378703251827625,\n",
       "   'amaz__n@n.l_3': 0.6947260563118709,\n",
       "   'aw__n.r@s_0': 1.4488567118359361,\n",
       "   'aw__n.r@s_1': 1.2207713709993877,\n",
       "   'aw__n.r@s_2': -1.3907640595082658,\n",
       "   'aw__n.r@s_3': 2.76330010719903,\n",
       "   'aw__n@n.l_0': 1.6684353082601349,\n",
       "   'aw__n@n.l_1': 0.7156002091733213,\n",
       "   'aw__n@n.l_2': 4.071382021192193,\n",
       "   'aw__n@n.l_3': 3.3337055517325487,\n",
       "   'bad__n.r@s_0': -0.27713836081879795,\n",
       "   'bad__n.r@s_1': 0.7553793550603685,\n",
       "   'bad__n.r@s_2': -1.3003785286669798,\n",
       "   'bad__n.r@s_3': -1.5063375684835827,\n",
       "   'bad__n@n.l_0': 1.544329692850111,\n",
       "   'bad__n@n.l_1': 0.601227941770039,\n",
       "   'bad__n@n.l_2': 2.3495885177295124,\n",
       "   'bad__n@n.l_3': 3.3754436798140572,\n",
       "   'bland__n.r@s_0': 1.7881447323681692,\n",
       "   'bland__n.r@s_1': -0.7251779859689536,\n",
       "   'bland__n.r@s_2': 1.6478983538906466,\n",
       "   'bland__n.r@s_3': 0.1428085679423684,\n",
       "   'bland__n@n.l_0': -0.6097837458991265,\n",
       "   'bland__n@n.l_1': 0.711204258713537,\n",
       "   'bland__n@n.l_2': 1.1627097800516766,\n",
       "   'bland__n@n.l_3': -0.45250388824465904,\n",
       "   'cafe__n_0': 0.5302394299521558,\n",
       "   'cafe__n_1': 2.6822861861302982,\n",
       "   'cafe__n_2': -1.4087294810372772,\n",
       "   'cook__n.r@s@n.l_0': 1.5048717000047427,\n",
       "   'cook__n.r@s@n.l_1': -0.8362792446397733,\n",
       "   'cook__n.r@s@n.l_2': -1.4506591062463001,\n",
       "   'cook__n.r@s@n.l_3': -0.5834517758333672,\n",
       "   'cook__n.r@s@n.l_4': -1.2639388771092976,\n",
       "   'cook__n.r@s@n.l_5': 0.8776697253789829,\n",
       "   'cook__n_0': 0.10954163179674961,\n",
       "   'cook__n_1': 0.7521355071550131,\n",
       "   'cook__n_2': -0.5525357832734643,\n",
       "   'delici__n.r@s_0': -0.017153892187710136,\n",
       "   'delici__n.r@s_1': -1.0318394705633802,\n",
       "   'delici__n.r@s_2': 0.40179391830421685,\n",
       "   'delici__n.r@s_3': 0.9899077961822893,\n",
       "   'delici__n@n.l_0': 0.8855132862880732,\n",
       "   'delici__n@n.l_1': -2.561838797318887,\n",
       "   'delici__n@n.l_2': 1.1785148250374313,\n",
       "   'delici__n@n.l_3': -1.005412736435891,\n",
       "   'dish__n_0': -3.909154572200996,\n",
       "   'dish__n_1': -1.11854300360024,\n",
       "   'dish__n_2': -2.405663645290029,\n",
       "   'dislik__n.r@s@n.l_0': 1.9961217473031634,\n",
       "   'dislik__n.r@s@n.l_1': 0.7599345392118018,\n",
       "   'dislik__n.r@s@n.l_2': 0.13564506963855888,\n",
       "   'dislik__n.r@s@n.l_3': 0.38731207594589256,\n",
       "   'dislik__n.r@s@n.l_4': -1.9660907413278204,\n",
       "   'dislik__n.r@s@n.l_5': -1.9888164385161935,\n",
       "   'excel__n.r@s_0': 0.2694866326082366,\n",
       "   'excel__n.r@s_1': -1.9590605036450972,\n",
       "   'excel__n.r@s_2': -0.47205820983540014,\n",
       "   'excel__n.r@s_3': -0.43154819088805974,\n",
       "   'excel__n@n.l_0': 2.916554332827551,\n",
       "   'excel__n@n.l_1': 1.1917820387068963,\n",
       "   'excel__n@n.l_2': 2.969136232727646,\n",
       "   'excel__n@n.l_3': 1.0779063347315356,\n",
       "   'fast__n.r@s_0': 1.1145380117025279,\n",
       "   'fast__n.r@s_1': 0.9221553241033412,\n",
       "   'fast__n.r@s_2': -2.567709435549032,\n",
       "   'fast__n.r@s_3': -1.007028175449735,\n",
       "   'fast__n@n.l_0': -1.0105217970617526,\n",
       "   'fast__n@n.l_1': -0.6131322273472568,\n",
       "   'fast__n@n.l_2': -0.008917617736219944,\n",
       "   'fast__n@n.l_3': 0.5796289408584707,\n",
       "   'food__n_0': -1.1781583407299063,\n",
       "   'food__n_1': -0.8868997512774413,\n",
       "   'food__n_2': -0.4413369311035076,\n",
       "   'foodservic__n_0': 0.2511045670599451,\n",
       "   'foodservic__n_1': 1.398493873561665,\n",
       "   'foodservic__n_2': -0.2937096308928123,\n",
       "   'good__n.r@s_0': -0.5185731476031757,\n",
       "   'good__n.r@s_1': -2.0240170672997984,\n",
       "   'good__n.r@s_2': 0.061616693191894244,\n",
       "   'good__n.r@s_3': 0.39561338374138555,\n",
       "   'good__n@n.l_0': 0.7916836423857967,\n",
       "   'good__n@n.l_1': -0.18475537472648082,\n",
       "   'good__n@n.l_2': 0.2915657931483,\n",
       "   'good__n@n.l_3': 3.4205451124897603,\n",
       "   'great__n.r@s_0': -0.698720862176186,\n",
       "   'great__n.r@s_1': 1.1021123952787635,\n",
       "   'great__n.r@s_2': 1.140456056326286,\n",
       "   'great__n.r@s_3': -1.219233863356051,\n",
       "   'great__n@n.l_0': -2.546294729109132,\n",
       "   'great__n@n.l_1': 2.7926053993874635,\n",
       "   'great__n@n.l_2': 0.5638487963788021,\n",
       "   'great__n@n.l_3': 2.509654429667549,\n",
       "   'had__n.r@s@n.l_0': -0.0801608136412218,\n",
       "   'had__n.r@s@n.l_1': 1.8333779694903916,\n",
       "   'had__n.r@s@n.l_2': 1.4311247811160306,\n",
       "   'had__n.r@s@n.l_3': 1.0363444185810389,\n",
       "   'had__n.r@s@n.l_4': -2.2541385637507814,\n",
       "   'had__n.r@s@n.l_5': 1.869779264765001,\n",
       "   'hate__n.r@s@n.l_0': -1.6335478540962303,\n",
       "   'hate__n.r@s@n.l_1': -1.7567747883453864,\n",
       "   'hate__n.r@s@n.l_2': 0.4790634043947076,\n",
       "   'hate__n.r@s@n.l_3': 0.9202815053059037,\n",
       "   'hate__n.r@s@n.l_4': 4.5395488915206474,\n",
       "   'hate__n.r@s@n.l_5': -2.9957343251618855,\n",
       "   'horribl__n.r@s_0': -1.423631805746497,\n",
       "   'horribl__n.r@s_1': 0.23234140105332354,\n",
       "   'horribl__n.r@s_2': -1.295805749198951,\n",
       "   'horribl__n.r@s_3': -1.0778436201705568,\n",
       "   'horribl__n@n.l_0': 4.062597982559608,\n",
       "   'horribl__n@n.l_1': 0.8364452623419752,\n",
       "   'horribl__n@n.l_2': 1.9759035352960166,\n",
       "   'horribl__n@n.l_3': 0.3637754298868222,\n",
       "   'i__n_0': -0.035617268462529904,\n",
       "   'i__n_1': 1.4976493877183623,\n",
       "   'i__n_2': 1.1107999450141657,\n",
       "   'impecc__n.r@s_0': 0.9879190379971636,\n",
       "   'impecc__n.r@s_1': -3.251458904956345,\n",
       "   'impecc__n.r@s_2': -0.2842120126015316,\n",
       "   'impecc__n.r@s_3': -2.6442928235242444,\n",
       "   'impecc__n@n.l_0': -0.0036824444401602234,\n",
       "   'impecc__n@n.l_1': 1.29876906757287,\n",
       "   'impecc__n@n.l_2': 0.4981613565204558,\n",
       "   'impecc__n@n.l_3': -1.7628227686118527,\n",
       "   'kind__n.r@s_0': -0.34206399264099774,\n",
       "   'kind__n.r@s_1': -1.4318206083424334,\n",
       "   'kind__n.r@s_2': 0.7175562430859219,\n",
       "   'kind__n.r@s_3': 0.6831466742332277,\n",
       "   'kind__n@n.l_0': 0.31376590841834284,\n",
       "   'kind__n@n.l_1': -0.780504223508357,\n",
       "   'kind__n@n.l_2': 1.1695873254537292,\n",
       "   'kind__n@n.l_3': 1.7220297619570668,\n",
       "   'like__n.r@s@n.l_0': -0.25702164507357594,\n",
       "   'like__n.r@s@n.l_1': 2.4939136256272665,\n",
       "   'like__n.r@s@n.l_2': 2.204195182214008,\n",
       "   'like__n.r@s@n.l_3': 2.3401753111576378,\n",
       "   'like__n.r@s@n.l_4': -1.0312564761057255,\n",
       "   'like__n.r@s@n.l_5': -1.7268487143820654,\n",
       "   'lousi__n.r@s_0': 2.8424379456461573,\n",
       "   'lousi__n.r@s_1': 1.7625338252182376,\n",
       "   'lousi__n.r@s_2': 2.217049219603169,\n",
       "   'lousi__n.r@s_3': 3.7121260015848963,\n",
       "   'lousi__n@n.l_0': 0.14188982892311255,\n",
       "   'lousi__n@n.l_1': -1.409953337852917,\n",
       "   'lousi__n@n.l_2': -1.3102126542280574,\n",
       "   'lousi__n@n.l_3': -0.23304768387072025,\n",
       "   'love__n.r@s@n.l_0': -0.48442606834904955,\n",
       "   'love__n.r@s@n.l_1': 1.0106102548485678,\n",
       "   'love__n.r@s@n.l_2': 0.7300798394936092,\n",
       "   'love__n.r@s@n.l_3': 1.1179793302229486,\n",
       "   'love__n.r@s@n.l_4': -0.8514719788421765,\n",
       "   'love__n.r@s@n.l_5': -2.3730961066747502,\n",
       "   'meal__n_0': -0.193304558515373,\n",
       "   'meal__n_1': 2.0852887488325473,\n",
       "   'meal__n_2': -1.2173187192646724,\n",
       "   'nice__n.r@s_0': -0.5655674305326109,\n",
       "   'nice__n.r@s_1': -2.0151418932176757,\n",
       "   'nice__n.r@s_2': 0.09129747100077945,\n",
       "   'nice__n.r@s_3': -0.8173057035057338,\n",
       "   'nice__n@n.l_0': -3.62972898532451,\n",
       "   'nice__n@n.l_1': -0.12371997382339255,\n",
       "   'nice__n@n.l_2': 1.8526266202771589,\n",
       "   'nice__n@n.l_3': 4.688695109838964,\n",
       "   'poor__n.r@s_0': 0.2526780767825872,\n",
       "   'poor__n.r@s_1': 3.6754075974717386,\n",
       "   'poor__n.r@s_2': -1.9219920706473466,\n",
       "   'poor__n.r@s_3': -0.22857054389625162,\n",
       "   'poor__n@n.l_0': -0.36306353444107875,\n",
       "   'poor__n@n.l_1': 0.9222612637237585,\n",
       "   'poor__n@n.l_2': -2.471310686172324,\n",
       "   'poor__n@n.l_3': 0.6742015523367071,\n",
       "   'restaur__n_0': 2.820764940059825,\n",
       "   'restaur__n_1': 2.4692453583238274,\n",
       "   'restaur__n_2': -1.3387841548928998,\n",
       "   'rude__n.r@s_0': -0.20957136470366386,\n",
       "   'rude__n.r@s_1': 1.1767706346430664,\n",
       "   'rude__n.r@s_2': -1.1754215714001917,\n",
       "   'rude__n.r@s_3': 2.819597806193844,\n",
       "   'rude__n@n.l_0': 3.1498059409398405,\n",
       "   'rude__n@n.l_1': 1.1254505166857984,\n",
       "   'rude__n@n.l_2': 0.6403682550116211,\n",
       "   'rude__n@n.l_3': -1.8187948653684711,\n",
       "   'servic__n_0': 0.7281439641813932,\n",
       "   'servic__n_1': -1.462156658407193,\n",
       "   'servic__n_2': -0.8431551343670268,\n",
       "   'show__n.r@s@n.l_0': 0.02467296416811907,\n",
       "   'show__n.r@s@n.l_1': 2.226680370960274,\n",
       "   'show__n.r@s@n.l_2': -0.13112944044154215,\n",
       "   'show__n.r@s@n.l_3': 3.563598615884665,\n",
       "   'show__n.r@s@n.l_4': 0.9217724403897752,\n",
       "   'show__n.r@s@n.l_5': 0.4569015482377409,\n",
       "   'slow__n.r@s_0': 0.8956498411862857,\n",
       "   'slow__n.r@s_1': 0.2148561757160394,\n",
       "   'slow__n.r@s_2': -1.0165617062932009,\n",
       "   'slow__n.r@s_3': -0.4569668492370893,\n",
       "   'slow__n@n.l_0': 3.1772885373575805,\n",
       "   'slow__n@n.l_1': -2.5029596116641795,\n",
       "   'slow__n@n.l_2': -1.2025857965890072,\n",
       "   'slow__n@n.l_3': 3.345990829439825,\n",
       "   'staff__n_0': 0.8985933999352356,\n",
       "   'staff__n_1': -0.8987811087118776,\n",
       "   'staff__n_2': 0.7907810937701517,\n",
       "   'tasti__n.r@s_0': 0.26611985331670074,\n",
       "   'tasti__n.r@s_1': -3.0702870171246017,\n",
       "   'tasti__n.r@s_2': -2.5492797704344436,\n",
       "   'tasti__n.r@s_3': -2.0668196657715407,\n",
       "   'tasti__n@n.l_0': -0.3390474200083209,\n",
       "   'tasti__n@n.l_1': -1.3026217982179507,\n",
       "   'tasti__n@n.l_2': 0.15201996595530967,\n",
       "   'tasti__n@n.l_3': 0.15905992386198467,\n",
       "   'terribl__n.r@s_0': 2.8921462670424507,\n",
       "   'terribl__n.r@s_1': 1.243745970189753,\n",
       "   'terribl__n.r@s_2': 2.2140606662028235,\n",
       "   'terribl__n.r@s_3': 1.8854720260739593,\n",
       "   'terribl__n@n.l_0': -0.28900766319032467,\n",
       "   'terribl__n@n.l_1': 0.7771470104664002,\n",
       "   'terribl__n@n.l_2': -1.787308982759146,\n",
       "   'terribl__n@n.l_3': -0.08636441659198837,\n",
       "   'unappet__n.r@s_0': 0.6288637243834074,\n",
       "   'unappet__n.r@s_1': 1.224134067189191,\n",
       "   'unappet__n.r@s_2': -0.763298042404849,\n",
       "   'unappet__n.r@s_3': 3.8434003918884008,\n",
       "   'unappet__n@n.l_0': -0.6935245623350694,\n",
       "   'unappet__n@n.l_1': 2.4084469608021584,\n",
       "   'unappet__n@n.l_2': 1.8312092676660074,\n",
       "   'unappet__n@n.l_3': 1.1877712974494918,\n",
       "   'unpleas__n.r@s_0': -0.46730661135799556,\n",
       "   'unpleas__n.r@s_1': -1.728366870577656,\n",
       "   'unpleas__n.r@s_2': 1.446598622251389,\n",
       "   'unpleas__n.r@s_3': -0.8678654959217112,\n",
       "   'unpleas__n@n.l_0': 1.3367075546084157,\n",
       "   'unpleas__n@n.l_1': 3.750357785566897,\n",
       "   'unpleas__n@n.l_2': -0.3166374129125197,\n",
       "   'unpleas__n@n.l_3': 2.1976988999421607,\n",
       "   'waiter__n_0': -0.8402891026110482,\n",
       "   'waiter__n_1': 0.4055968531768286,\n",
       "   'waiter__n_2': 2.5693829986202426,\n",
       "   'yummi__n.r@s_0': -0.039966653947844676,\n",
       "   'yummi__n.r@s_1': 0.5080413881466829,\n",
       "   'yummi__n.r@s_2': -1.6515468414211854,\n",
       "   'yummi__n.r@s_3': 0.19441265210982242,\n",
       "   'yummi__n@n.l_0': -1.8154878846003073,\n",
       "   'yummi__n@n.l_1': 1.297245264842687,\n",
       "   'yummi__n@n.l_2': -0.9792733688631982,\n",
       "   'yummi__n@n.l_3': -1.1114641109574668},\n",
       "  'data_version': 'v5',\n",
       "  'ansatz': 'GeneralQCAnsatz',\n",
       "  'gate_layers': ['Rx', 'Rz'],\n",
       "  'var_layers': 2,\n",
       "  'n_layers': 1,\n",
       "  'iter': 800,\n",
       "  'parameterize_nouns': True},\n",
       " {'name': 'v5_Rx_CX_800iter_with_nouns_take2',\n",
       "  'params': {'amaz__n.r@s_0': 0.8740001483319453,\n",
       "   'amaz__n.r@s_1': 0.12687195331382,\n",
       "   'amaz__n@n.l_0': 0.19397043609862302,\n",
       "   'amaz__n@n.l_1': 0.0979458134576344,\n",
       "   'aw__n.r@s_0': 0.47912124870694645,\n",
       "   'aw__n.r@s_1': 0.46576955460163333,\n",
       "   'aw__n@n.l_0': 0.41212657356629184,\n",
       "   'aw__n@n.l_1': 0.7319152180687032,\n",
       "   'bad__n.r@s_0': 1.0752604226560716,\n",
       "   'bad__n.r@s_1': 1.4627264873435868,\n",
       "   'bad__n@n.l_0': 0.971509293772913,\n",
       "   'bad__n@n.l_1': 0.459421103493273,\n",
       "   'bland__n.r@s_0': 0.8092279807149467,\n",
       "   'bland__n.r@s_1': -0.3013964453043198,\n",
       "   'bland__n@n.l_0': 1.099990143952883,\n",
       "   'bland__n@n.l_1': 0.8671494116720748,\n",
       "   'cafe__n_0': 0.7934274525026375,\n",
       "   'cafe__n_1': 0.7509707631578086,\n",
       "   'cafe__n_2': 0.6821810586720813,\n",
       "   'cook__n.r@s@n.l_0': 0.8431917456089657,\n",
       "   'cook__n.r@s@n.l_1': 0.3780404435542563,\n",
       "   'cook__n.r@s@n.l_2': -0.20472656020739904,\n",
       "   'cook__n_0': 0.20109248390239687,\n",
       "   'cook__n_1': 0.2882683548249628,\n",
       "   'cook__n_2': 1.0116615962400548,\n",
       "   'delici__n.r@s_0': 0.18447544355539242,\n",
       "   'delici__n.r@s_1': 0.763096379366581,\n",
       "   'delici__n@n.l_0': 0.47341122555227905,\n",
       "   'delici__n@n.l_1': 0.43830383805298473,\n",
       "   'dish__n_0': 0.852490756724867,\n",
       "   'dish__n_1': 0.6563689305429752,\n",
       "   'dish__n_2': 0.7170477588649815,\n",
       "   'dislik__n.r@s@n.l_0': -0.457122330201037,\n",
       "   'dislik__n.r@s@n.l_1': 0.45212414453617084,\n",
       "   'dislik__n.r@s@n.l_2': 0.49956833474316686,\n",
       "   'excel__n.r@s_0': 0.04819669153472184,\n",
       "   'excel__n.r@s_1': 0.8992803136466068,\n",
       "   'excel__n@n.l_0': 0.8048647989919377,\n",
       "   'excel__n@n.l_1': 0.7176709285964842,\n",
       "   'fast__n.r@s_0': 0.9834487207499365,\n",
       "   'fast__n.r@s_1': 1.037572583772353,\n",
       "   'fast__n@n.l_0': 0.6886061492740165,\n",
       "   'fast__n@n.l_1': 0.6853366577971891,\n",
       "   'food__n_0': 0.28212234831244914,\n",
       "   'food__n_1': 0.2739763221861125,\n",
       "   'food__n_2': 0.1618422847969297,\n",
       "   'foodservic__n_0': -0.1414266437450915,\n",
       "   'foodservic__n_1': 0.22927793056010937,\n",
       "   'foodservic__n_2': -0.2577921874006285,\n",
       "   'good__n.r@s_0': 0.6688255675161503,\n",
       "   'good__n.r@s_1': 0.8002719599462198,\n",
       "   'good__n@n.l_0': -0.28683326281246146,\n",
       "   'good__n@n.l_1': 0.8409814720321425,\n",
       "   'great__n.r@s_0': -0.5710215865547095,\n",
       "   'great__n.r@s_1': 0.03368524847371791,\n",
       "   'great__n@n.l_0': -0.3104294857472481,\n",
       "   'great__n@n.l_1': -0.2227729920319202,\n",
       "   'had__n.r@s@n.l_0': 0.6211858509502584,\n",
       "   'had__n.r@s@n.l_1': 0.624991124995042,\n",
       "   'had__n.r@s@n.l_2': 0.05916687674059774,\n",
       "   'hate__n.r@s@n.l_0': 0.46312871903327907,\n",
       "   'hate__n.r@s@n.l_1': 0.46217089492022484,\n",
       "   'hate__n.r@s@n.l_2': 0.4978592020088437,\n",
       "   'horribl__n.r@s_0': 0.9199890877925947,\n",
       "   'horribl__n.r@s_1': 0.5877877125844337,\n",
       "   'horribl__n@n.l_0': 1.0015615685880759,\n",
       "   'horribl__n@n.l_1': 0.48842608818699657,\n",
       "   'i__n_0': 1.4639223003818151,\n",
       "   'i__n_1': 0.018107444422485713,\n",
       "   'i__n_2': 1.226221047375593,\n",
       "   'impecc__n.r@s_0': 0.5590539577015892,\n",
       "   'impecc__n.r@s_1': 0.9402175568835278,\n",
       "   'impecc__n@n.l_0': 0.8356669851405012,\n",
       "   'impecc__n@n.l_1': 0.5458581156218363,\n",
       "   'kind__n.r@s_0': 1.1147870128001018,\n",
       "   'kind__n.r@s_1': -0.08629260079192547,\n",
       "   'kind__n@n.l_0': 0.6840998539688974,\n",
       "   'kind__n@n.l_1': 0.5269645442672399,\n",
       "   'like__n.r@s@n.l_0': 1.0447038072731996,\n",
       "   'like__n.r@s@n.l_1': 1.0135506287779932,\n",
       "   'like__n.r@s@n.l_2': 0.7438064958502821,\n",
       "   'lousi__n.r@s_0': 0.5272602932343964,\n",
       "   'lousi__n.r@s_1': 0.5306283875743655,\n",
       "   'lousi__n@n.l_0': 0.9048935477664746,\n",
       "   'lousi__n@n.l_1': 0.07193533107483205,\n",
       "   'love__n.r@s@n.l_0': 0.747025910063353,\n",
       "   'love__n.r@s@n.l_1': 0.5978678750863857,\n",
       "   'love__n.r@s@n.l_2': 0.2911363832949068,\n",
       "   'meal__n_0': 0.37992713697369956,\n",
       "   'meal__n_1': 0.3062471919667797,\n",
       "   'meal__n_2': 1.2137546982657033,\n",
       "   'nice__n.r@s_0': 0.08165521864711658,\n",
       "   'nice__n.r@s_1': 0.9407601452738195,\n",
       "   'nice__n@n.l_0': 0.5529153490196639,\n",
       "   'nice__n@n.l_1': 0.3925579872795755,\n",
       "   'poor__n.r@s_0': 1.1688636737501907,\n",
       "   'poor__n.r@s_1': 0.5551609071936224,\n",
       "   'poor__n@n.l_0': 1.0916653167882597,\n",
       "   'poor__n@n.l_1': 0.7143140387260867,\n",
       "   'restaur__n_0': 0.8159760540537966,\n",
       "   'restaur__n_1': 0.513758374323386,\n",
       "   'restaur__n_2': 0.5186272525294546,\n",
       "   'rude__n.r@s_0': 0.6805039888274831,\n",
       "   'rude__n.r@s_1': 0.49058073511870487,\n",
       "   'rude__n@n.l_0': 0.8201316418491946,\n",
       "   'rude__n@n.l_1': 0.6313378838819124,\n",
       "   'servic__n_0': 0.3737860677461016,\n",
       "   'servic__n_1': 0.737331878187826,\n",
       "   'servic__n_2': 0.036232253566631366,\n",
       "   'show__n.r@s@n.l_0': 0.3180962694829463,\n",
       "   'show__n.r@s@n.l_1': 0.4927331373198076,\n",
       "   'show__n.r@s@n.l_2': 0.056467393819759314,\n",
       "   'slow__n.r@s_0': 0.9917489383225739,\n",
       "   'slow__n.r@s_1': 0.5133640526657532,\n",
       "   'slow__n@n.l_0': 1.0618476480104269,\n",
       "   'slow__n@n.l_1': 0.5877110203634317,\n",
       "   'staff__n_0': 0.9079902065032455,\n",
       "   'staff__n_1': 0.30338146283296696,\n",
       "   'staff__n_2': 0.31631854615940125,\n",
       "   'tasti__n.r@s_0': 0.06827026566236771,\n",
       "   'tasti__n.r@s_1': -0.06925849475750744,\n",
       "   'tasti__n@n.l_0': 0.2775993668244869,\n",
       "   'tasti__n@n.l_1': 0.3092393491020609,\n",
       "   'terribl__n.r@s_0': 0.8586770364209921,\n",
       "   'terribl__n.r@s_1': -0.18728156751803682,\n",
       "   'terribl__n@n.l_0': 1.0120242529309331,\n",
       "   'terribl__n@n.l_1': 0.4710722251276545,\n",
       "   'unappet__n.r@s_0': -0.006709099672764292,\n",
       "   'unappet__n.r@s_1': -0.42003496788815126,\n",
       "   'unappet__n@n.l_0': 0.23310956166048236,\n",
       "   'unappet__n@n.l_1': 0.25803119450846634,\n",
       "   'unpleas__n.r@s_0': 0.5919772221482771,\n",
       "   'unpleas__n.r@s_1': 0.4605015990449011,\n",
       "   'unpleas__n@n.l_0': 0.49754518944457504,\n",
       "   'unpleas__n@n.l_1': -0.3371024005270628,\n",
       "   'waiter__n_0': 0.016207471056038085,\n",
       "   'waiter__n_1': 0.9070739444993332,\n",
       "   'waiter__n_2': 0.6933530896341633,\n",
       "   'yummi__n.r@s_0': 0.1333045172356806,\n",
       "   'yummi__n.r@s_1': 0.7742832774586685,\n",
       "   'yummi__n@n.l_0': 0.8037232183464305,\n",
       "   'yummi__n@n.l_1': 0.3098309449762661},\n",
       "  'data_version': 'v5',\n",
       "  'ansatz': 'GeneralQCAnsatz',\n",
       "  'gate_layers': ['Rx', 'X'],\n",
       "  'var_layers': 1,\n",
       "  'n_layers': 1,\n",
       "  'iter': 800,\n",
       "  'parameterize_nouns': True}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take = [\"v5_H_CRx_800iter_with_nouns_take2\", \"v5_Rx_CRz_800iter_with_nouns_take2\", \"v5_Rx_CX_800iter_with_nouns_take2\"]\n",
    "\n",
    "experiment_book_2 = [e for e in experiment_book if e[\"name\"] in take]\n",
    "\n",
    "experiment_book = [e for e in experiment_book_2]\n",
    "\n",
    "experiment_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25feee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytket.extensions.qiskit import AerBackend\n",
    "\n",
    "backend = AerBackend()\n",
    "\n",
    "backend_config = {\n",
    "    'backend': backend,\n",
    "    'compilation': backend.default_compilation_pass(2),\n",
    "    'n_shots': 8192  # maximum recommended shots, reduces sampling error\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6886a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from discopy.quantum import Circuit, Id, Measure\n",
    "\n",
    "def randint(rng, low=-1 << 63, high=1 << 63-1):\n",
    "    return rng.integers(low, high)\n",
    "\n",
    "def normalise(predictions):\n",
    "    # apply smoothing to predictions\n",
    "    predictions = np.abs(predictions) + 1e-9\n",
    "    return predictions / predictions.sum()\n",
    "\n",
    "def make_pred_fn(circuits, parameters, rng):\n",
    "    measured_circuits = [c >> Id().tensor(*[Measure()] * len(c.cod)) for c in circuits]\n",
    "    circuit_fns = [c.lambdify(*parameters) for c in measured_circuits]\n",
    "\n",
    "    def predict(params):\n",
    "        outputs = Circuit.eval(*(c_fn(*params) for c_fn in circuit_fns),\n",
    "                               **backend_config, seed=randint(rng))\n",
    "        return np.array([normalise(output.array) for output in outputs])\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "519f3b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import AtomicType, IQPAnsatz, Sim15Ansatz\n",
    "from sympy import default_sort_key\n",
    "\n",
    "\n",
    "def decapitate_circuit(circuit):\n",
    "    from discopy import Bra, Measure\n",
    "\n",
    "    cutoff = None\n",
    "\n",
    "    for i, box in enumerate(circuit.boxes):\n",
    "        if box in [Bra, Measure, Bra(0), Bra(0, 0)]:\n",
    "            cutoff = i\n",
    "            break\n",
    "    \n",
    "    return circuit[:i]\n",
    "\n",
    "def build_circuit_library(experiment_dict, data_src, targets_src):\n",
    "    name = experiment_dict[\"name\"]\n",
    "    params = experiment_dict[\"params\"]\n",
    "    ansatz_name = experiment_dict[\"ansatz\"]\n",
    "    n_layers = experiment_dict[\"n_layers\"]\n",
    "    spsa_n_iter = experiment_dict[\"iter\"]\n",
    "    parameterize_nouns = experiment_dict[\"parameterize_nouns\"]\n",
    "    \n",
    "    parameters = list(params.keys())\n",
    "    param_vals = [params[key] for key in parameters]\n",
    "\n",
    "    # build ansatz\n",
    "    N = AtomicType.NOUN\n",
    "    S = AtomicType.SENTENCE\n",
    "\n",
    "    if ansatz_name == \"IQPAnsatz\":\n",
    "        ansatz = IQPAnsatz({N: 1, S: 1}, n_layers=n_layers)\n",
    "    elif ansatz_name == \"SimpleSAAnsatz\":\n",
    "        ansatz = SimpleSAAnsatz({N: 1, S: 1}, n_layers=n_layers, n_single_qubit_params=3*int(parameterize_nouns))\n",
    "    elif ansatz_name == \"GeneralQCAnsatz\":\n",
    "        gate_layers = experiment_dict[\"gate_layers\"]\n",
    "        var_layers = experiment_dict[\"var_layers\"]\n",
    "        \n",
    "        ansatz = GeneralQCAnsatz({N: 1, S: 1}, n_layers=1, n_single_qubit_params=3*int(parameterize_nouns), gate_config={\"gate_layers\": gate_layers, \"var_layers\": var_layers}, r=(1, -1))\n",
    "    \n",
    "    data, diagrams, circuits, valids_mask = sentences_to_circuits(data_src, ansatz)\n",
    "    targets = [target for i, target in enumerate(targets_src) if valids_mask[i]]\n",
    "\n",
    "    circuit_heads = []\n",
    "\n",
    "    # instantiate complete circuits\n",
    "\n",
    "    SEED = 0\n",
    "    rng = np.random.default_rng(SEED)\n",
    "\n",
    "    use_parameters = sorted({s for circ in circuits for s in circ.free_symbols},key=default_sort_key)\n",
    "    use_param_strs = [str(param_sym) for param_sym in use_parameters]\n",
    "    use_param_vals = [val for i, val in enumerate(param_vals) if parameters[i] in use_param_strs]\n",
    "\n",
    "    measured_circuits = [c >> Id().tensor(*[Measure()] * len(c.cod)) for c in circuits]\n",
    "    circuit_fns = [c.lambdify(*use_parameters) for c in measured_circuits]\n",
    "    instantiated_circuits = [c_fn(*use_param_vals) for c_fn in circuit_fns]\n",
    "\n",
    "    outputs = Circuit.eval(*instantiated_circuits, **backend_config, seed=randint(rng))\n",
    "    res = np.array([normalise(output.array) for output in outputs])\n",
    "\n",
    "    circuit_heads = [decapitate_circuit(c) for c in circuits]\n",
    "\n",
    "    # measured_circuit_heads = [c >> Id().tensor(*[Measure()] * len(c.cod)) for c in circuit_heads]\n",
    "    circuit_head_fns = [c.lambdify(*use_parameters) for c in circuit_heads]\n",
    "    instantiated_circuit_heads = [c_fn(*use_param_vals) for c_fn in circuit_head_fns]\n",
    "\n",
    "    return data, targets, parameters, param_vals, diagrams, ansatz, circuits, instantiated_circuits, circuit_heads, instantiated_circuit_heads, outputs, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d122ad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computational_bases(n_qubits):\n",
    "    return np.array([[int(j==i) for j in range(2**n_qubits)] for i in range(2**n_qubits)], dtype='cfloat')\n",
    "\n",
    "def tensor_prod(v2, v1):\n",
    "    return np.tensordot(v1, v2, axes=0).flatten()\n",
    "\n",
    "def bloch_bases(n_qubits, n_alpha, n_beta):\n",
    "    z_1 = computational_bases(1)\n",
    "    b_plus = lambda alpha, beta: np.cos(alpha/2)*z_1[0] + np.sin(alpha/2)*np.exp((0+1j)*beta)*z_1[1]\n",
    "    b_minus = lambda alpha, beta: np.sin(alpha/2)*z_1[0] - np.cos(alpha/2)*np.exp((0+1j)*beta)*z_1[1]\n",
    "\n",
    "    alpha_dom = np.linspace(0, np.pi, n_alpha)\n",
    "    beta_dom = np.linspace(0, 2*np.pi, n_beta)\n",
    "\n",
    "    def b_bases_paramx(thetas):\n",
    "        b_bases = computational_bases(0)\n",
    "\n",
    "        for i in range(n_qubits):\n",
    "            b_pluses = [tensor_prod(b_plus(thetas[2*i], thetas[2*i+1]), b_basis) for b_basis in b_bases]\n",
    "            b_minuses = [tensor_prod(b_minus(thetas[2*i], thetas[2*i+1]), b_basis) for b_basis in b_bases]\n",
    "\n",
    "            b_bases = np.vstack([b_pluses, b_minuses])\n",
    "        \n",
    "        return b_bases\n",
    "    \n",
    "    bloch_space = [None for _ in range((n_alpha*n_beta)**(n_qubits))]\n",
    "\n",
    "    def recursive_search(level, curr_idx=[], curr_thetas=[]):\n",
    "        for i, alpha0 in enumerate(alpha_dom):\n",
    "            for j, beta0 in enumerate(beta_dom):\n",
    "                if level == 1:\n",
    "                    index = 0\n",
    "                    for a_i, b_j in curr_idx+[[i, j]]:\n",
    "                        index = index*n_alpha*n_beta+a_i*n_alpha+b_j\n",
    "                    \n",
    "                    bloch_space[index] = b_bases_paramx(curr_thetas+[alpha0, beta0])\n",
    "                else:\n",
    "                    recursive_search(level-1, curr_idx+[[i, j]], curr_thetas+[alpha0, beta0])\n",
    "\n",
    "    recursive_search(n_qubits)\n",
    "\n",
    "    return bloch_space\n",
    "\n",
    "def trace(mat):\n",
    "    return np.cfloat(sum([mat[i][i] for i in range(len(mat))]))\n",
    "\n",
    "def abstrace(mat):\n",
    "    return float(sum([np.absolute(mat[i][i]) for i in range(len(mat))]))\n",
    "\n",
    "def KD(a, b, rho):\n",
    "    Pi_a = np.outer(a, np.conjugate(a))\n",
    "    Pi_b = np.outer(b, np.conjugate(b))\n",
    "    return trace(np.matmul(Pi_b, np.matmul(Pi_a, rho)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5723c753",
   "metadata": {},
   "source": [
    "# Build Haar Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46b65ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit, execute\n",
    "from qiskit import Aer\n",
    "\n",
    "def build_clifford_set(n_qubits, cliffords=[]):\n",
    "    single_qubit_cliffords = [\n",
    "        '',\n",
    "        'H', 'S',\n",
    "        'HS', 'SH', 'SS',\n",
    "        'HSH', 'HSS', 'SHS', 'SSH', 'SSS',\n",
    "        'HSHS', 'HSSH', 'HSSS', 'SHSS', 'SSHS',\n",
    "        'HSHSS', 'HSSHS', 'SHSSH', 'SHSSS', 'SSHSS',\n",
    "        'HSHSSH', 'HSHSSS', 'HSSHSS'\n",
    "    ]\n",
    "\n",
    "    if n_qubits == 0:\n",
    "        return cliffords\n",
    "    else:\n",
    "        if len(cliffords) == 0:\n",
    "            return build_clifford_set(n_qubits-1, [[sc] for sc in single_qubit_cliffords])\n",
    "        else:\n",
    "            temp_cliffords = [[c+[sc] for sc in single_qubit_cliffords] for c in cliffords]\n",
    "            new_cliffords = [item for sublist in temp_cliffords for item in sublist]\n",
    "\n",
    "            return build_clifford_set(n_qubits-1, new_cliffords)\n",
    "\n",
    "def build_clifford_states(n_qubits):\n",
    "    backend = Aer.get_backend('unitary_simulator')\n",
    "\n",
    "    multi_qubit_cliffords = build_clifford_set(n_qubits)\n",
    "    mqc_svs = [clifford_str_to_sv(c, backend) for c in multi_qubit_cliffords]\n",
    "\n",
    "    return mqc_svs\n",
    "\n",
    "def clifford_str_to_sv(clifford_str, backend):\n",
    "    n_qubits = len(clifford_str)\n",
    "\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "\n",
    "    for i in range(n_qubits):\n",
    "        for j, gate in enumerate(clifford_str[i]):\n",
    "            if gate == \"H\":\n",
    "                qc.h(n_qubits-i-1)\n",
    "            if gate == \"S\":\n",
    "                qc.s(n_qubits-i-1)\n",
    "\n",
    "    job = execute(qc, backend, shots=8192)\n",
    "    job_result = job.result()\n",
    "\n",
    "    return job_result.get_unitary(qc).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bc0503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_qubit_cliffords = {}\n",
    "\n",
    "for n_qubits in [2, 3, 4]:\n",
    "    multi_qubit_cliffords[n_qubits] = build_clifford_states(n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90479e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../data/coherence_utils/mqc_vals.pkl\", \"wb\") as f:\n",
    "    pickle.dump(multi_qubit_cliffords, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43d363d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_bases = {2: computational_bases(2), 3: computational_bases(3), 4: computational_bases(4)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9175158",
   "metadata": {},
   "source": [
    "# Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42a00c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fe2e56b5614d489f8309facd89d1a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tagging sentences:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581a1da4c1ac43078e7ae82d857d6377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing tagged sentences:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b4b30f3d1d463988ac2450958642a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse trees to diagrams:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v5_H_CRx_800iter_with_nouns_take2 4 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 576/576 [00:01<00:00, 501.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v5_H_CRx_800iter_with_nouns_take2 8 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13824/13824 [00:10<00:00, 1278.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v5_H_CRx_800iter_with_nouns_take2 16 194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 331776/331776 [7:47:44<00:00, 11.82it/s]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v5_H_CRx_800iter_with_nouns_take2 [0.9488166982199158, 0.473843898414561, 0.8435027853335292, 0.5136525628275147, 0.7936134708081328, 0.5409357528270662, 0.7468086246309338, 0.473843898414561, 0.534348473191101, 0.7549495090933971, 0.5015250756391929, 0.5917287078552889, 0.838309676349251, 0.7154313794324165, 0.9584708516452676, 0.39120119050115515, 0.7198033113769524, 0.9065064301018869, 0.5075806547995791, 0.6613957685342982, 0.6138931439384933, 0.6342430089583354, 0.7900246905127861, 0.7132078161213022, 0.5075806547995791, 0.529394073452506, 0.821958247350146, 0.5337909845006302, 0.6555389617397298, 0.9300051107149226, 0.8334983165453402, 0.9965965363557363, 0.4401826894214433, 0.6775685616548026, 0.8471252410730203, 0.7875589829203018, 0.7899779429379268, 0.8184498048620332, 0.6249646767986049, 0.8544865655540776, 0.7053452034675999, 0.7733027300208452, 0.7682539546679039, 0.7650694486005374, 0.6935895365175622, 0.7875593089571472, 0.26124622657240504, 0.26124622657240504, 0.5086160017261792, 0.4831506864105987, 0.2659531045074783, 0.2659531045074783, 0.45153102540138096, 0.3773448624197596, 0.6775031694506615, 0.8355586504095196, 0.6050795054250944, 0.6676473637518131, 0.4358959677367622, 0.42655492894232105, 0.6802223969406542, 0.43620617574147236, 0.40029278170446125, 0.7457944830102915, 2.3229916331820526, 1.0822517020334002, 1.075132580846028, 0.7522678340000506, 2.649959570793217, 1.2785638728244062, 1.986573353559874, 1.6474970511513898, 2.0617447880753437, 2.000789988956885, 2.0999332475637495, 2.021755367890086, 2.4119183578794603, 2.3154187196012623, 1.7168509771044524, 2.322119931575055, 0.8888381397243151, 1.0550882376003838, 2.2957572501989345, 1.008505049529693, 0.9062931465475108, 1.106486161515988, 0.9873096769629366, 1.0954720516455445, 1.9303084441024292, 1.0373609877908512, 0.9594750341649851, 0.8693900830980857, 0.7762780178480935, 2.291667770284517, 0.9982507063404255, 1.221998065580394, 1.0755216828177065, 1.7339097627986184, 2.0802616873517357, 1.8367042895113201, 2.2190618471147845, 1.8954578084289038, 2.3258637458700084, 2.184633936817452, 2.2145975864429466, 1.2358295908924481, 1.268866162305225, 0.6902630496449939, 1.7226110972562816, 1.3264639866478931, 1.8301022012737242, 2.0805333581634433, 2.2339960657624447, 2.1565076861345074, 2.0487796977257386, 2.121901408829814, 2.2865733116845393, 2.525360124975989, 2.0885058198870925, 1.9022063362862085, 0.8656576411090707, 1.1291755131411214, 2.0886097471207963, 1.09723866220088, 0.8226188112063526, 1.1262031958085257, 1.0029160165672584, 1.0465767477120733, 1.952260803501797, 1.0047573007742967, 1.1269703521571786, 0.9642196109571052, 0.8950300974895911, 2.2394038852294322, 0.9103121132413788, 1.3021840634221082, 1.0645004146903878, 1.7479405494201061, 2.072020331177871, 1.8320225488599335, 2.252939952363306, 1.9462725888510968, 2.3367012953318915, 2.192248677670294, 2.442944139193471, 2.160642673589538, 0.9650018614704114, 2.330299092711912, 2.088846461904844, 0.8498235244049408, 0.8003882016455869, 2.7184092843269623, 2.130527172103828, 0.815047395450982, 2.397382169750497, 0.9833515627372833, 2.497919277952209, 2.1317910582036435, 0.7975107992134306, 2.260178947706793, 2.025312746809516, 1.3733681409517116, 2.1325485667619386, 2.2980655394402705, 1.2898070315523902, 2.2066073576931786, 1.3700108340712875, 2.2721605519987684, 1.1604438622306585, 2.665210413842774, 1.496644938158794, 2.4090493759314286, 1.1717217520845478, 2.6777801406484745, 1.7069467482826797, 2.203911677658004, 1.9914145339813436, 1.9925034983510084, 2.211280056005357, 2.014259834289467, 1.2728045126134402, 2.2733531254548995, 1.0229488220837866, 1.1473084224890757, 1.8844966315574345, 1.1766105053260463, 2.320604154778389, 1.0390253355718695, 1.6450807976136572, 1.0292084560636225, 1.9088496681716693, 1.2428325689815074, 2.2420237092197985, 1.1074548220321059] 1.3336313674914262\n",
      "28076.85172343254\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a408931780154f6b8505de53ac22d4c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tagging sentences:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0cdadff7724818aaeea1e4442790e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing tagged sentences:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041423fbb6ef489f97cc4ca00acb488a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse trees to diagrams:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v5_Rx_CRz_800iter_with_nouns_take2 4 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 576/576 [00:01<00:00, 499.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v5_Rx_CRz_800iter_with_nouns_take2 8 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13824/13824 [00:10<00:00, 1265.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v5_Rx_CRz_800iter_with_nouns_take2 16 194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 331776/331776 [5:24:32<00:00, 17.04it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v5_Rx_CRz_800iter_with_nouns_take2 [0.5207270154185528, 0.62599452525456, 0.5265711482120804, 0.5731209224306407, 0.3930805315326101, 0.5742659023006581, 0.4644710721654869, 0.62599452525456, 0.6806226762784321, 0.45348799472050616, 0.65408106433682, 0.49633826401991754, 0.36875788140811155, 0.854311530512015, 0.8210498261881461, 0.7990360557264662, 0.48456316226092033, 0.2637076446480694, 0.719571320976595, 0.5136591540689155, 0.6762073550958666, 0.38447091801339967, 0.5424569967710744, 0.20569777685433843, 0.719571320976595, 0.7974300178127742, 0.43133273196497685, 0.6192201860291344, 0.5093811922768796, 0.44868968406580706, 0.5308757508681605, 0.5500382838969868, 0.7098692725846072, 0.39503493915176824, 0.5575759585474516, 0.43863002238272664, 0.41594925026162144, 0.36124741287180845, 0.47151067522696666, 0.49441053285539704, 0.4847478437483522, 0.5799486313826957, 0.413733199008114, 0.5940614480693949, 0.4733855678811605, 0.5161274962116434, 0.8020201841355783, 0.8020201841355783, 0.7011133497977007, 0.7050611077493312, 0.7107943499408256, 0.7107943499408256, 0.6032605893218794, 0.8106573423097561, 0.8115352541787934, 0.7363873311926707, 0.9022859884220831, 0.7075896555993701, 0.6998994348851477, 0.45918976228777525, 0.5180745485465927, 0.4632337582338354, 0.37579477915045356, 0.529655268883996, 1.4082330815092308, 1.1559731942532228, 1.1494304418048265, 0.9098451779376212, 1.6160021687231632, 1.167073058463751, 1.7282686582429165, 1.7484288520153983, 1.520122205733079, 1.538217221373171, 1.6141704937342811, 1.5993913852267867, 1.5143394173612685, 1.564274287624281, 1.5903277589279765, 1.5892441653875566, 0.6656672850198126, 0.8003810668537703, 1.5741134493948725, 0.8629303130275972, 1.0345803131110811, 0.9809255554737081, 0.944094244326312, 0.8734160458984035, 1.4754998526510776, 1.3467372235417066, 0.8705226217801848, 0.5127380181136164, 0.7378800215849087, 1.6026214690628415, 1.206630512213048, 0.9116908558474859, 0.9921344845471001, 1.6804604618413594, 1.6034063232938718, 1.7498668056750732, 1.248935131823191, 1.4621830081538891, 1.6664474478872384, 1.444174493710724, 1.4009157559554046, 1.1385066145765517, 1.1477885456798516, 0.8696892946955841, 1.6140042365924065, 1.177317890654257, 1.5870934958216896, 1.7309925508136816, 1.6099832811287607, 1.6056116242370946, 1.6333917057317262, 1.5637642540810455, 1.4598223931116348, 1.58662417220284, 1.6526472337029583, 1.5093677643757755, 0.6650859102558484, 0.8144796775708905, 1.7078955110953207, 0.7740085347003693, 0.9975449421840578, 1.0852065366785002, 0.9689846337276895, 0.9848783746780001, 1.4698332602442368, 1.268664542754972, 0.9690725212862206, 0.6158041204953508, 0.8929750615782509, 1.651185660011729, 0.7471276813247961, 0.8216770062917255, 0.8636165480879623, 1.6298803508502366, 1.845969355171917, 1.4822417782678516, 1.4180934157608773, 1.523606246234954, 1.5606894729119922, 1.559175241743844, 1.4173398833963975, 1.481192941251031, 0.9311359364281104, 1.4863422400383304, 1.4690573983283395, 0.8894590286954684, 0.49025001220434516, 1.0679810178816254, 1.6343391709023556, 0.7477311043532542, 1.741970454628008, 0.48346952819145117, 1.0874978186153443, 1.6249208388937182, 0.5203588514856412, 1.4255098661170438, 1.5408522116411807, 0.9953431625256227, 1.6984495429503979, 1.5470300812475573, 1.0104168590308713, 1.7883629496497853, 0.9681380389769849, 1.1932416877331122, 1.2177714625452252, 1.8035462653575967, 0.8894556564647733, 1.247830450674938, 1.2187903807192169, 1.6083321528713628, 1.6021743835029822, 1.4473440482438698, 1.6098648551479384, 1.5540751069764043, 1.6167154213015469, 1.7151951642036691, 1.2065230036234087, 1.809516375330736, 0.707109618883912, 1.2131276946747358, 1.7755684316328013, 0.7598995883215719, 1.5339161598171345, 1.0329757368916415, 1.5458056622341425, 0.7841528514460773, 1.349083011300187, 1.020378636823772, 1.5296806911496121, 1.0747717205811194] 1.0523967775646526\n",
      "19484.494427204132\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc83233d99b488a8ba3d2bf6f0e0650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tagging sentences:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d82a22ba3748778facbc4d4a32bf0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing tagged sentences:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677e83c08c2e410fa8cfbbbbf3dee843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse trees to diagrams:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v5_Rx_CX_800iter_with_nouns_take2 4 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 576/576 [00:01<00:00, 461.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v5_Rx_CX_800iter_with_nouns_take2 8 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13824/13824 [00:11<00:00, 1202.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v5_Rx_CX_800iter_with_nouns_take2 16 194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 331776/331776 [5:18:40<00:00, 17.35it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v5_Rx_CX_800iter_with_nouns_take2 [0.4786980186037614, 0.6815225608269775, 0.6477599188114552, 0.6419884363369576, 0.8527999960708663, 0.4700036128768505, 0.6465815228527001, 0.6815225608269775, 0.5639495998933256, 0.5588886580508388, 0.8812274559684629, 0.6810213941040746, 0.7915254441396227, 0.8133015050466007, 0.3942606063483863, 0.6513516001826458, 0.6247738892845204, 0.9033068696844214, 0.6068965297658239, 0.6494016222731206, 0.7465372106681263, 0.7733049480913143, 0.6015480564635086, 0.5957553473256642, 0.6068965297658239, 0.6539746235428638, 0.675840726693137, 0.8864274516612627, 0.6423989714774143, 0.8538810356072694, 0.9534061759537042, 0.39412562178068, 0.6043743786361748, 0.6527568352231305, 0.7924227011597524, 0.7843350730560439, 0.7529921428206345, 0.8186775852761852, 0.7303068466435059, 0.70607047442025, 0.5875064510928454, 0.5549380942419644, 0.6437116874604178, 0.5581762018027457, 0.5900000493328229, 0.5301201554625524, 0.7087011447619581, 0.7087011447619581, 0.7174907646231535, 0.6775936899812933, 0.7175895009354893, 0.7175895009354893, 0.6164191567652006, 0.6322694039016801, 0.49922205376303314, 0.6297044793503069, 0.9724435012999704, 0.9945988529331481, 0.9074954555011078, 1.3354098124624856, 0.8930955601506088, 0.8426020940614145, 1.3109085233620814, 0.9962099070078678, 2.1400274072206216, 1.6481117322959915, 1.6618074959080607, 2.588196452076076, 2.035988329102893, 1.015568503933602, 2.462762332444728, 2.4765131105916516, 2.482077581737928, 2.4257169751062326, 2.625625946993245, 2.648182913232719, 2.097968030898248, 1.9314751396177812, 2.540639239028109, 2.475110641389531, 2.670568075275199, 1.8541510437050965, 1.9641824044808238, 1.4795713935072727, 2.33680839024682, 1.4771900057209053, 1.5546171403814142, 2.481454888043723, 1.8529498719764548, 1.5950009095155429, 1.2241249472154356, 2.4077908599365077, 2.290150708702031, 2.0925588489784146, 1.5065996625918696, 1.4280948183301514, 1.3342961873051142, 2.2058064781255253, 2.0749621578213246, 2.3455671200356853, 2.5478872264288075, 2.5239011074238316, 1.990011220782619, 2.2257214124247433, 2.1085204331962273, 1.6158334591164558, 1.6683228213413723, 2.5308110601153, 2.1596658707136536, 1.0613583505507722, 2.4652911972977147, 2.4317242978080693, 2.3754672317345755, 2.363102755533439, 2.5963426185862177, 2.606895308276959, 2.0367352227057287, 2.0994243937006574, 2.500401168415949, 2.4425478513226793, 2.6184713048071533, 1.8336314523288264, 1.918781787768691, 1.4938697579327642, 2.3525081797226015, 1.505761021109711, 1.5484178234312596, 2.4051104335258056, 1.9053200178856127, 1.6426505041369446, 1.172890615607719, 2.448672054216713, 2.3606924477701132, 2.0246479762068037, 1.0817593559859846, 1.741091678901581, 1.4335015267377058, 2.221333730467463, 2.200554857534602, 2.35583081154743, 2.5412592974100208, 2.434626594007426, 2.0424233454235736, 2.260119881555604, 2.1249854549379705, 2.6486467043065955, 2.512353661548773, 2.1115370337920107, 2.611391886726744, 2.537043704406419, 2.5592072728164372, 2.055586574602915, 2.548605039925041, 2.5368120856146272, 2.3978904041645053, 2.570588573064061, 2.0749493916235586, 2.5502975026984576, 1.8758324930006756, 1.7697481895727536, 2.4687089620372373, 1.0881010381058926, 2.253815679748427, 2.4241642581151384, 1.2373558231667072, 2.463540883713269, 1.4229405358951275, 2.6236236640552697, 1.1113670206502828, 2.1395976212716765, 1.4087126013568358, 2.5947524089959093, 1.1447822227877091, 1.913225768217758, 2.454713534818867, 1.9108626666212412, 2.244762391274614, 2.4618750944383927, 1.9866968975113404, 2.2626910812456416, 1.683403661370159, 2.4969955571078244, 1.7746159081376716, 1.6287300319876388, 2.4725415626325673, 1.7297585496530585, 2.502739352246885, 2.5071922527169725, 2.4276652735007787, 2.483846548312216, 2.4320000000164548, 1.6421018336254347, 2.421765899772339, 1.8364362415652427] 1.6404998955186139\n",
      "19133.364622354507\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "coherence_dict = {}\n",
    "\n",
    "for k, experiment_dict in enumerate(experiment_book):\n",
    "    if experiment_dict[\"name\"] in coherence_dict:\n",
    "        experiment_book[k][\"NCLs\"] = np.array(coherence_dict[experiment_dict[\"name\"]][\"ncls\"])\n",
    "        experiment_book[k][\"coherence\"] = coherence_dict[experiment_dict[\"name\"]][\"avg\"]\n",
    "        continue\n",
    "    \n",
    "    data, targets, parameters, param_vals, diagrams, ansatz, circuits, instantiated_circuits, circuit_heads, instantiated_circuit_heads, outputs, res = build_circuit_library(experiment_dict, test_data_src, test_targets_src)\n",
    "    sv_data = [c.to_tk().get_statevector() for c in instantiated_circuit_heads]\n",
    "\n",
    "    NCLs = [None for _ in sv_data]\n",
    "\n",
    "    sv_lengths = [len(sv) for sv in sv_data]\n",
    "    sv_sorted = [x for _, x in sorted(zip(sv_lengths, sv_data), key=lambda pair: pair[0])]\n",
    "    sv_lengths_sorted = [x for x, _ in sorted(zip(sv_lengths, sv_data), key=lambda pair: pair[0])]\n",
    "\n",
    "    dim = sv_lengths_sorted[0]\n",
    "    i = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    while i < len(sv_data):\n",
    "        i0 = i\n",
    "        while i < len(sv_data) and sv_lengths_sorted[i] == dim:\n",
    "            i += 1\n",
    "        \n",
    "        sv_use = sv_sorted[i0:i]\n",
    "        print(experiment_dict[\"name\"], dim, i)\n",
    "\n",
    "        n_qubits = int(math.log2(dim))\n",
    "\n",
    "        if n_qubits not in multi_qubit_cliffords:\n",
    "            multi_qubit_cliffords[n_qubits] = build_clifford_states(n_qubits)\n",
    "        if n_qubits not in z_bases:\n",
    "            z_bases[n_qubits] = computational_bases(n_qubits)\n",
    "        \n",
    "        rhos = [np.outer(sv, sv.conjugate()) for sv in sv_use]\n",
    "\n",
    "        y_max = np.array([-1 for _ in sv_use], dtype='float')\n",
    "\n",
    "        for haar_basis in tqdm(multi_qubit_cliffords[n_qubits]):\n",
    "            kd_sums = np.array([-1 for _ in sv_use], dtype='float')\n",
    "            \n",
    "            for hs in haar_basis:\n",
    "                Pi_b = np.outer(hs, hs.conjugate())\n",
    "                kd_sum = np.array([abstrace(np.matmul(Pi_b, rho)) for rho in rhos], dtype='float')\n",
    "                kd_sums += kd_sum\n",
    "                \n",
    "            y_max = y_max*(y_max > kd_sums) + kd_sums*(kd_sums >= y_max)\n",
    "\n",
    "        for j in range(i0, i):\n",
    "            NCLs[j] = y_max[j-i0]\n",
    "        if i < len(sv_data):\n",
    "            dim = sv_lengths_sorted[i]\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    experiment_book[k][\"NCLs\"] = NCLs\n",
    "    experiment_book[k][\"coherence\"] = sum(NCLs)/len(NCLs)\n",
    "    coherence_dict[experiment_dict[\"name\"]] = {\"name\": NCLs, \"avg\": sum(NCLs)/len(NCLs)}\n",
    "\n",
    "    print(experiment_dict[\"name\"], NCLs, sum(NCLs)/len(NCLs))\n",
    "    print(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ce66cc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v5_H_CRx_800iter_with_nouns_take2\n",
      "v5_Rx_CRz_800iter_with_nouns_take2\n",
      "v5_Rx_CX_800iter_with_nouns_take2\n"
     ]
    }
   ],
   "source": [
    "for e in experiment_book:\n",
    "    print(e[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad7145a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
