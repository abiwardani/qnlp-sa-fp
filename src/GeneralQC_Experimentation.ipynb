{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Parameters\n",
    "\n",
    "# Experiment conditions\n",
    "use_all_rewriter_rules = True\n",
    "convert_bigraph = True\n",
    "not_representation = \"X\"\n",
    "not_placement = \"end\"\n",
    "gate_layers = [\"Rx\", \"Rz\"]\n",
    "var_layers = 2\n",
    "parameterize_nouns = True\n",
    "# independent_sublayer = False\n",
    "\n",
    "# load params\n",
    "train_params = True\n",
    "load_params = True\n",
    "# param_vals = [0.6931585473443279, 1.0462424956825283, 1.234537983904686, -2.90716906835752, 1.5753587952252757, -2.4968837592255424, 1.6118961289272027, -1.016327328767516, -0.3142432619217712, 1.208915586540799, -1.1801888411140051, -0.4286556908790561, 3.3779520000989214, 0.04286513605390508, 0.5295399156230165, -1.7699845136573933, 0.5233673239374572, -1.748569530235187, -0.26823991786760143, 1.813789372735753, -2.9079130258111228, -0.6129166776514257, 0.3401131125827421, -0.4365025178512618, 1.008439272978763, 0.06192528487377933, 1.6121247434136865, 3.3613679248015504, 4.1316674342901845, 0.8303800120316661, -1.227825304579597, -1.8130593574910823, 0.1848756781930945, -0.676881418623572, 2.591255835558515, 1.6673593894980614, 1.6540892586338571, 1.6143400480675385, 0.23932506846152854, 1.5181285887362506, 0.13241105800686212, 3.8061670131492367, 0.5807166437788316, 1.5116799923186133, 1.5599307705774368, 0.29362851954048996, 0.19967375719855993, 1.914813488873402, 2.7644195885210587, 0.37942837286458875, 2.3383678677227993, -0.5843868818793704, 3.921194364357892, 1.4174799045629805, 2.741638785064822, -0.6304530862551285, -1.8933985338953436, 1.5624277556723982, -1.1677654133728248, -0.8250732415577517, 1.7230226879389658, -0.8418763059243636, 0.9691454037142504, 0.7252400407386729, -1.9797782989274328, 2.2739476528314704, 0.4612678359290186, 2.2453750405051833, 1.2218764611893127, -1.4926462511306318, 1.5083681953206398, -0.6209409896223363, -0.8575621399562154, -1.1360971020856898, -0.32413810704072976, 2.317045883444611, 0.052405537301035005]\n",
    "# params_saved = {'I__n_0': 1.3926202901320248, 'I__n_1': 0.008448820382520305, 'I__n_2': -0.6385783774702594, 'aw__n.r@s_0': 0.004878974097219957, 'aw__n@n.l_0': -1.710437394121971, 'bad__n.r@s_0': -2.0120646244526674, 'bad__n@n.l_0': -0.33328110433838587, 'bland__n.r@s_0': -1.8011492489090937, 'bland__n@n.l_0': -0.23301002690494765, 'cook__n.r@s@n.l_0': 2.666641605278804, 'cook__n.r@s@n.l_1': 0.35782850997078663, 'delici__n.r@s_0': -0.6784353476839611, 'delici__n@n.l_0': 2.5048532581915994, 'dislik__n.r@s@n.l_0': 1.264688638132907, 'dislik__n.r@s@n.l_1': 2.9058201511277773, 'fast__n.r@s_0': 1.5228059027468268, 'fast__n@n.l_0': 1.4318617692717328, 'food__n_0': 0.9510109180885518, 'food__n_1': 1.983545943335132, 'food__n_2': 0.31422754989262236, 'good__n.r@s_0': -0.37898061288101936, 'good__n@n.l_0': 0.3417258364922475, 'great__n.r@s_0': 0.32023762451562343, 'great__n@n.l_0': 1.9344786256084243, 'had__n.r@s@n.l_0': 0.9175005505289897, 'had__n.r@s@n.l_1': -1.126491771734188, 'hate__n.r@s@n.l_0': 1.0178843262642405, 'hate__n.r@s@n.l_1': -0.8544213946932742, 'impecc__n.r@s_0': 0.8774940821788493, 'impecc__n@n.l_0': 0.28533233081922516, 'like__n.r@s@n.l_0': 0.5238711836690633, 'like__n.r@s@n.l_1': -0.0248482491004431, 'love__n.r@s@n.l_0': -1.0716522290529822, 'love__n.r@s@n.l_1': 0.4674574117431039, 'meal__n_0': -0.2913227630628624, 'meal__n_1': -0.7824158049259571, 'meal__n_2': 2.364685233940882, 'nice__n.r@s_0': -0.31906215984000647, 'nice__n@n.l_0': -2.087345064482271, 'restaur__n_0': 1.9567881690203595, 'restaur__n_1': 0.35319298096689367, 'restaur__n_2': -0.5953604557383299, 'rude__n.r@s_0': 2.1631540155864895, 'rude__n@n.l_0': -0.9753409914505806, 'servic__n_0': -2.7047864895209357, 'servic__n_1': 1.2351321590961786, 'servic__n_2': -1.5851261977616085, 'show__n.r@s@n.l_0': -0.6393012792612347, 'show__n.r@s@n.l_1': 1.7857303585103954, 'slow__n.r@s_0': 0.5156333465906835, 'slow__n@n.l_0': 1.0425765284092643, 'staff__n_0': 0.08543447326700565, 'staff__n_1': -2.068154595353262, 'staff__n_2': 0.019379227469981927, 'tasti__n.r@s_0': 1.2246133069435021, 'tasti__n@n.l_0': 1.9668107229444722, 'terribl__n.r@s_0': -0.3440262636975905, 'terribl__n@n.l_0': 2.1861199395142688, 'unappet__n.r@s_0': 1.440589048292482, 'unappet__n@n.l_0': -0.31598863954665035, 'wa__s@s.l_0': -1.0244128683696618}\n",
    "spsa_a = 0.2\n",
    "spsa_c = 0.06\n",
    "spsa_n_iter = 800\n",
    "k0 = 750\n",
    "\n",
    "# SVM type\n",
    "kernel = 'rbf'\n",
    "default_svm = True\n",
    "optimize_svm = False\n",
    "\n",
    "# Experiment metadata\n",
    "experiment_name = \"v5_Rx_CRz_800iter_with_nouns_take2\"\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment folder already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "if (experiment_name == \"\"):\n",
    "    experiment_name = \"misc\"\n",
    "\n",
    "path = '../data/experiment_results/journal/generalQC/'+experiment_name\n",
    "\n",
    "try:\n",
    "    os.mkdir(path)\n",
    "except:\n",
    "    print(\"experiment folder already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(path+'/params_raw.txt') as f:\n",
    "        data = f.read().replace('\\'', '\\\"')\n",
    "        params_saved = json.loads(data)\n",
    "except:\n",
    "    params_saved = None\n",
    "    train_params = True\n",
    "    load_params = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/quantum/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/quantum/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/quantum/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_data(filename):\n",
    "    labels, sentences = [], []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            labels.append([1, 0] if line[0] == '1' else [0, 1])\n",
    "            sentences.append(line[1:].strip())\n",
    "    return np.array(labels), sentences\n",
    "\n",
    "test_targets_src, test_data_src = read_data('../data/datasets/restaurant_v5_test.txt')\n",
    "dev_targets_src, dev_data_src = read_data('../data/datasets/restaurant_v5_dev.txt')\n",
    "train_targets_src, train_data_src = read_data('../data/datasets/restaurant_v5_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for converting to bigraph\n",
    "\n",
    "from discopy.rigid import Id as RigidId\n",
    "\n",
    "def checkTrailingCups(diagram):\n",
    "    scanWords = True\n",
    "    \n",
    "    for box in diagram.boxes:\n",
    "        if not box.dom and not scanWords:\n",
    "            return False\n",
    "        else:\n",
    "            scanWords = scanWords and not box.dom\n",
    "    \n",
    "    return True\n",
    "\n",
    "def convertToTrailingCups(diagram):\n",
    "    if (checkTrailingCups(diagram)):\n",
    "        return diagram\n",
    "\n",
    "    words = []\n",
    "    cups = []\n",
    "    \n",
    "    for box in diagram.boxes:\n",
    "        if not box.dom:\n",
    "            words = words + [box]\n",
    "        else:\n",
    "            cups = [box] + cups\n",
    "    \n",
    "    new_diag = words[0]\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if i != 0:\n",
    "            new_diag = new_diag >> RigidId(new_diag.cod) @ word\n",
    "    \n",
    "    for i, cup in enumerate(cups):\n",
    "        if i != len(cups)-1:\n",
    "            new_diag = new_diag >> RigidId(new_diag.cod[:-2]) @ cup\n",
    "        else:\n",
    "            new_diag = new_diag >> cup @ RigidId(new_diag.cod[2:])\n",
    "    \n",
    "    return new_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucntion for stemming and lemmatization of tokens\n",
    "\n",
    "def to_word_tokens(data):\n",
    "    return [word_tokenize(record) for record in data]\n",
    "\n",
    "def build_stem_dictionary(data):\n",
    "    port = PorterStemmer()\n",
    "    wnet = WordNetLemmatizer()\n",
    "    \n",
    "    mapping = {}\n",
    "    \n",
    "    data_as_tokens = to_word_tokens(data)\n",
    "    \n",
    "    for words in data_as_tokens:\n",
    "        for word in words:\n",
    "            if word not in mapping:\n",
    "                stemmed_word = port.stem(word)\n",
    "                lemmatized_word = wnet.lemmatize(stemmed_word)\n",
    "                \n",
    "                mapping[word] = lemmatized_word\n",
    "    \n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for stemming and lemmatization of diagram boxes\n",
    "\n",
    "from lambeq.rewrite import RewriteRule\n",
    "\n",
    "class StemRewriteRule(RewriteRule):\n",
    "    def __init__(self, data):\n",
    "        self.mapping = build_stem_dictionary(data)\n",
    "    \n",
    "    def matches(self, box):\n",
    "        return box.name in self.mapping\n",
    "\n",
    "    def rewrite(self, box):\n",
    "        new_name = self.mapping[box.name]\n",
    "        return type(box)(name=new_name, dom=box.dom, cod=box.cod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq.ansatz import CircuitAnsatz\n",
    "from abc import abstractmethod\n",
    "# from collections.abc import Mapping\n",
    "from itertools import cycle\n",
    "from typing import Callable, Optional, Tuple, Mapping\n",
    "from discopy.quantum.circuit import (Circuit, Functor, Id, qubit)\n",
    "from discopy.quantum.gates import Bra, Ket, Controlled, Rotation\n",
    "from discopy.quantum.gates import H, X, Y, Z, Rx, Ry, Rz\n",
    "from discopy.rigid import Box, Diagram, Ty\n",
    "from discopy.tensor import Dim, Tensor\n",
    "import numpy as np\n",
    "from sympy import Symbol, symbols\n",
    "\n",
    "class GeneralQCLayer(Circuit):\n",
    "    \n",
    "    def __init__(self, n_qubits, params):\n",
    "        from discopy.quantum.gates import Rx, Rz\n",
    "\n",
    "        if len(self.gate_layers) != 2:\n",
    "            raise ValueError(\"Expected gate_layers as tuple of strings\")\n",
    "        \n",
    "        g1, g2 = self.gate_layers\n",
    "\n",
    "        if (g1 == None) or (g2 == None):\n",
    "            raise ValueError(\"gate_layers must be in discopy.quantum.gates\")\n",
    "        # if not (abs(self.r1) == 1 and abs(self.r2) == 1) and ((abs(self.r1) == 1 or abs(self.r2) == 1) and (self.r2 % self.r1 == 0 or self.r1 % self.r2 == 0)) or (n_qubits % self.r1 == 0 and self.r1 != 1) or (n_qubits % self.r2 == 0 and self.r2 != 1):\n",
    "        #     raise ValueError(\"n_qubits, r1, and r2 must be co-prime\")\n",
    "\n",
    "        params_shape = np.shape(params)\n",
    "\n",
    "        if n_qubits == 1:\n",
    "            if len(params) == 0:\n",
    "                circuit = Id(1)\n",
    "            else:\n",
    "                circuit = Rx(params[0]) >> Rz(params[1]) >> Rx(params[2])\n",
    "        elif (len(params_shape) != 2):\n",
    "            raise ValueError(\n",
    "                \"Expected params of shape (depth, {})\".format(n_qubits))\n",
    "        else:\n",
    "            g1_thetas = 0\n",
    "            g2_thetas = 0\n",
    "\n",
    "            if g1.free_symbols != {}:\n",
    "                # g1 is fixed\n",
    "                g1_thetas = n_qubits\n",
    "            if g2.free_symbols != {}:\n",
    "                g2_thetas = n_qubits\n",
    "\n",
    "            if self.reuse_params:\n",
    "                self.k = 1\n",
    "            else:\n",
    "                self.k = 2\n",
    "            \n",
    "            n_thetas = self.k*(g1_thetas + g2_thetas)\n",
    "\n",
    "            if (params_shape[1] != n_thetas):\n",
    "                raise ValueError(\n",
    "                    \"Expected component params with length {}\".format(n_thetas))\n",
    "\n",
    "            # ANSATZ ALGORITHM\n",
    "            circuit = Id(n_qubits)\n",
    "            \n",
    "            # for {theta} in labelled params\n",
    "            for thetas in params:\n",
    "                \n",
    "                # sublayer 1 non-entangling block\n",
    "                if g1_thetas == 0:\n",
    "                    # if g1 is fixed\n",
    "                    sublayer1 = Id().tensor(*([g1 for _ in range(n_qubits)]))\n",
    "                else:\n",
    "                    # if g1 is trainable\n",
    "                    sublayer1 = Id().tensor(*([g1(theta) for theta in thetas[:g1_thetas]]))\n",
    "                \n",
    "                # sublayer 1 entangled block\n",
    "                ctrl = 0\n",
    "\n",
    "                for i in range(n_qubits):\n",
    "                    # shift target := control - r1\n",
    "                    tgt = (ctrl - self.r1) % n_qubits\n",
    "                    \n",
    "                    if g2_thetas == 0:\n",
    "                        sublayer1 = sublayer1._apply_controlled(g2, ctrl, tgt)\n",
    "                        # sublayer1 = sublayer1.CX(ctrl, tgt)\n",
    "                    else:\n",
    "                        sublayer1 = sublayer1._apply_controlled(g2(thetas[g1_thetas + i]), ctrl, tgt)\n",
    "                        # sublayer1 = sublayer1.CRx(thetas[g1_thetas + i], ctrl, tgt)\n",
    "                    \n",
    "                    ctrl = tgt\n",
    "                \n",
    "                # sublayer 2 non-entangling block\n",
    "                if g1_thetas == 0:\n",
    "                    sublayer2 = Id().tensor(*([g1 for _ in range(n_qubits)]))\n",
    "                else:\n",
    "                    if self.reuse_params:\n",
    "                        sublayer2 = Id().tensor(*([g1(theta) for theta in thetas[:g1_thetas]]))\n",
    "                    else:\n",
    "                        sublayer2 = Id().tensor(*([g1(theta) for theta in thetas[g1_thetas+g2_thetas:2*g1_thetas+g2_thetas]]))\n",
    "                \n",
    "                # sublayer 2 entangled block\n",
    "                ctrl = 0\n",
    "\n",
    "                for i in range(n_qubits):\n",
    "                    # shift target := control - r2\n",
    "                    tgt = (ctrl - self.r2) % n_qubits\n",
    "\n",
    "                    if g2_thetas == 0:\n",
    "                        sublayer2 = sublayer2._apply_controlled(g2, ctrl, tgt)\n",
    "                        # sublayer2 = sublayer2.CX(ctrl, tgt)\n",
    "                    else:\n",
    "                        if self.reuse_params:\n",
    "                            sublayer2 = sublayer2._apply_controlled(g2(thetas[g1_thetas + i]), ctrl, tgt)\n",
    "                        else:\n",
    "                            sublayer2 = sublayer2._apply_controlled(g2(thetas[2*g1_thetas+g2_thetas+i]), ctrl, tgt)\n",
    "                            # sublayer2 = sublayer2.CRx(thetas[2*g1_thetas+g2_thetas+i], ctrl, tgt)\n",
    "                    \n",
    "                    ctrl = tgt\n",
    "            \n",
    "                # compose circuit\n",
    "                circuit >>= sublayer1 >> sublayer2\n",
    "            \n",
    "        super().__init__(\n",
    "            circuit.dom, circuit.cod, circuit.boxes, circuit.offsets)\n",
    "\n",
    "class GeneralQCAnsatz(CircuitAnsatz):\n",
    "    def __init__(self, ob_map: Mapping[Ty, int], n_layers: int, n_single_qubit_params: int = 0, gate_config = {\"gate_layers\": [H, Rx], \"var_layers\": 1}, r = (1, 3), reuse_params=True, discard: bool = False) -> None:\n",
    "        self.gate_config = gate_config\n",
    "        self.reuse_params = reuse_params\n",
    "        self.r = r\n",
    "        \n",
    "        class GeneralQCInterface(GeneralQCLayer):\n",
    "            \"\"\" Interface for GeneralQCLayer \"\"\"\n",
    "\n",
    "            def strToGate(gateStr):\n",
    "                if gateStr == \"H\":\n",
    "                    return H\n",
    "                if gateStr == \"X\":\n",
    "                    return X\n",
    "                if gateStr == \"Y\":\n",
    "                    return Y\n",
    "                if gateStr == \"Z\":\n",
    "                    return Z\n",
    "                if gateStr == \"Rx\":\n",
    "                    return Rx\n",
    "                if gateStr == \"Ry\":\n",
    "                    return Ry\n",
    "                if gateStr == \"Rz\":\n",
    "                    return Rz\n",
    "                \n",
    "                return Id(1)\n",
    "            \n",
    "            assert(len(self.gate_config[\"gate_layers\"]) == 2)\n",
    "\n",
    "            g1, g2 = self.gate_config[\"gate_layers\"]\n",
    "\n",
    "            if isinstance(g1, str):\n",
    "                g1 = strToGate(g1)\n",
    "            if isinstance(g2, str):\n",
    "                g2 = strToGate(g2)\n",
    "\n",
    "            gate_layers = [g1, g2]\n",
    "            r1, r2 = self.r\n",
    "            reuse_params = self.reuse_params\n",
    "\n",
    "        super().__init__(ob_map, n_layers, n_single_qubit_params, GeneralQCInterface, discard, [Rx, Rz])\n",
    "\n",
    "    def params_shape(self, n_qubits: int) -> Tuple[int, ...]:\n",
    "        if self.reuse_params:\n",
    "            k = 1\n",
    "        else:\n",
    "            k = 2\n",
    "        \n",
    "        return (self.n_layers, k*self.gate_config[\"var_layers\"]*n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSA(Circuit):\n",
    "    def __init__(self, n_qubits, params):\n",
    "        from discopy.quantum.gates import H, Rx, Rz, CX, Ry\n",
    "\n",
    "        if n_qubits == 1:\n",
    "            if len(params) == 0:\n",
    "                circuit = Id(1)\n",
    "            else:\n",
    "                circuit = Rx(params[0]) >> Rz(params[1]) >> Rx(params[2])\n",
    "        elif len(np.shape(params)) != 2\\\n",
    "                or np.shape(params)[1] != n_qubits - 1:\n",
    "            raise ValueError(\n",
    "                \"Expected params of shape (depth, {})\".format(n_qubits - 1))\n",
    "        else:\n",
    "            depth = np.shape(params)[0]\n",
    "            circuit = Id(n_qubits)\n",
    "\n",
    "            for thetas in params:\n",
    "                hadamards = Id().tensor(*(n_qubits * [H]))\n",
    "                rotations = Id(n_qubits).then(*(\n",
    "                    (Id(i) @ CX @ Id(n_qubits - 2 - i)) >> (Id(i + 1) @ Rz(thetas[i]) @ Id(n_qubits - 2 - i) >> (Id(i + 1) @ H @ Id(n_qubits - 2 - i)))\n",
    "                    for i in range(n_qubits - 1)))\n",
    "                circuit >>= hadamards >> rotations\n",
    "\n",
    "        super().__init__(circuit.dom, circuit.cod, circuit.boxes, circuit.offsets)\n",
    "\n",
    "class SimpleSAAnsatz(CircuitAnsatz):\n",
    "    def __init__(self, ob_map: Mapping[Ty, int], n_layers: int, n_single_qubit_params: int = 0, discard: bool = False) -> None:\n",
    "        super().__init__(ob_map, n_layers, n_single_qubit_params, SimpleSA, discard, [Rx, Rz], H)\n",
    "\n",
    "    def params_shape(self, n_qubits: int) -> Tuple[int, ...]:\n",
    "        return (self.n_layers, n_qubits - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import BobcatParser, Rewriter, remove_cups\n",
    "from discopy import grammar\n",
    "from discopy.quantum.gates import X, Z\n",
    "from discopy import rigid\n",
    "\n",
    "def sentences_to_circuits(sentences, ansatz, convert_bigraph=True, not_representation=\"X\", all_sentences=None, return_valids_mask=True):\n",
    "    ### SENTENCES TO DIAGRAMS ###\n",
    "\n",
    "    if all_sentences is None:\n",
    "        all_sentences = sentences\n",
    "    \n",
    "    # syntax tree parsing\n",
    "    parser = BobcatParser()\n",
    "    raw_diagrams = parser.sentences2diagrams([text.replace(\" not \", \" \") for text in sentences])\n",
    "\n",
    "    # filter valid diagrams type S\n",
    "    n_sent = len(sentences)\n",
    "\n",
    "    valids_mask = np.array([d.cod.name == Ty('s').name for d in raw_diagrams])\n",
    "    data = [sentences[i] for i in range(n_sent) if valids_mask[i]]\n",
    "    use_diagrams = [raw_diagrams[i] for i in range(n_sent) if valids_mask[i]]\n",
    "\n",
    "    # grammatical rewrite rules\n",
    "    rewriter = Rewriter()\n",
    "    rewritten_diagrams = [rewriter(diagram) for diagram in use_diagrams]\n",
    "\n",
    "    # bigraph method\n",
    "    normalised_diagrams = [convertToTrailingCups(diagram.normal_form()) for diagram in rewritten_diagrams]\n",
    "\n",
    "    removed_diagrams = [remove_cups(diagram) for diagram in normalised_diagrams]\n",
    "\n",
    "    # stemming and lemmatization\n",
    "    stemmed_diagrams = [Rewriter([StemRewriteRule(all_sentences)])(diagram) for diagram in removed_diagrams]\n",
    "\n",
    "    # final diagrams\n",
    "    diagrams = [diagram for diagram in stemmed_diagrams]\n",
    "\n",
    "    ### DIAGRAMS to CIRCUITS ###\n",
    "\n",
    "    # string diagrams to raw quantum circuits\n",
    "    circuits = [ansatz(diagram) for diagram in diagrams]\n",
    "\n",
    "    # apply NOT box to circuits\n",
    "    for i, circuit in enumerate(circuits):\n",
    "        if data[i].find(\" not \") != -1:\n",
    "            if (not_representation == \"ZX\"):\n",
    "                circuits[i] = circuit >> Z >> X\n",
    "            else:\n",
    "                circuits[i] = circuit >> X\n",
    "    \n",
    "    if return_valids_mask:\n",
    "        return data, diagrams, circuits, valids_mask\n",
    "    else:\n",
    "        return data, diagrams, circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import AtomicType, IQPAnsatz\n",
    "from discopy.quantum.gates import H, X, Y, Z, Rx, Ry, Rz\n",
    "\n",
    "# Define atomic types\n",
    "N = AtomicType.NOUN\n",
    "S = AtomicType.SENTENCE\n",
    "\n",
    "# Convert string diagram to quantum circuit\n",
    "ansatz = GeneralQCAnsatz({N: 1, S: 1}, n_layers=1, n_single_qubit_params=3*int(parameterize_nouns), gate_config={\"gate_layers\": gate_layers, \"var_layers\": var_layers}, r=(1, -1))\n",
    "# ansatz = SimpleSAAnsatz({N: 1, S: 1}, n_layers=1, n_single_qubit_params=3*int(parameterize_nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00dabbe6b60547f2b4a369d30b28c785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tagging sentences:   0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca37f67c108640f1bf360d6efab76af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing tagged sentences:   0%|          | 0/532 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bbea03f1fe4ec4a12404d9f4542a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse trees to diagrams:   0%|          | 0/532 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a545665b2748e993509e1733a9804a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tagging sentences:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f437e0b399f04d17929c9be72c82b1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing tagged sentences:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5921359b79834236b89cfdae1ae797eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse trees to diagrams:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a8ca15bc364fde9694afa072131acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tagging sentences:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf762b7bd8f41f4a690054181966e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing tagged sentences:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97ac309af704939bd1784b6d43ba694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse trees to diagrams:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data, train_diagrams, train_circuits, train_valids_mask = sentences_to_circuits(train_data_src, ansatz)\n",
    "dev_data, dev_diagrams, dev_circuits, dev_valids_mask = sentences_to_circuits(dev_data_src, ansatz)\n",
    "test_data, test_diagrams, test_circuits, test_valids_mask = sentences_to_circuits(test_data_src, ansatz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = [train_targets_src[i] for i, mask in enumerate(train_valids_mask) if mask]\n",
    "dev_targets = [dev_targets_src[i] for i, mask in enumerate(dev_valids_mask) if mask]\n",
    "test_targets = [test_targets_src[i] for i, mask in enumerate(test_valids_mask) if mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytket.extensions.qiskit import AerBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure backend\n",
    "backend = AerBackend()\n",
    "\n",
    "backend_config = {\n",
    "    'backend': backend,\n",
    "    'compilation': backend.default_compilation_pass(2),\n",
    "    'n_shots': 8192  # maximum recommended shots, reduces sampling error\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discopy.quantum import Circuit, Id, Measure\n",
    "\n",
    "def randint(rng, low=-1 << 63, high=1 << 63-1):\n",
    "    return rng.integers(low, high)\n",
    "\n",
    "def normalise(predictions):\n",
    "    # apply smoothing to predictions\n",
    "    predictions = np.abs(predictions) + 1e-9\n",
    "    return predictions / predictions.sum()\n",
    "\n",
    "def make_pred_fn(circuits, parameters, rng):\n",
    "    measured_circuits = [c >> Id().tensor(*[Measure()] * len(c.cod)) for c in circuits]\n",
    "    circuit_fns = [c.lambdify(*parameters) for c in measured_circuits]\n",
    "\n",
    "    def predict(params):\n",
    "        outputs = Circuit.eval(*(c_fn(*params) for c_fn in circuit_fns),\n",
    "                               **backend_config, seed=randint(rng))\n",
    "        return np.array([normalise(output.array) for output in outputs])\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from noisyopt import OptimizeResult\n",
    "\n",
    "def myMinimizeSPSA(func, x0, args=(), bounds=None, niter=100, paired=True,\n",
    "                 a=1.0, c=1.0, k0=0,\n",
    "                 disp=False, callback=None):\n",
    "    \"\"\"\n",
    "    Minimization of an objective function by a simultaneous perturbation\n",
    "    stochastic approximation algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func: callable\n",
    "        objective function to be minimized\n",
    "    x0: array-like\n",
    "        starting point\n",
    "    args: tuple\n",
    "        extra arguments to be supplied to func\n",
    "    bounds: array-like\n",
    "        bounds on the variables\n",
    "    scaling: array-like\n",
    "        scaling by which to multiply step size and tolerances along different dimensions\n",
    "    niter: int\n",
    "        maximum number of iterations of the algorithm\n",
    "    paired: boolean\n",
    "        calculate gradient for same random seeds\n",
    "    a: float\n",
    "       algorithm scaling parameter for step size\n",
    "    c: float\n",
    "       algorithm scaling parameter for evaluation step size\n",
    "    disp: boolean\n",
    "        whether to output status updates during the optimization\n",
    "    callback: callable\n",
    "        called after each iteration, as callback(xk), where xk is the current parameter vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scipy.optimize.OptimizeResult object\n",
    "    \"\"\"\n",
    "    A = 0.01 * niter\n",
    "    # A = 0.01 * niter\n",
    "    alpha = 0.602\n",
    "    gamma = 0.101\n",
    "\n",
    "    if bounds is None:\n",
    "        project = lambda x: x\n",
    "    else:\n",
    "        bounds = np.asarray(bounds)\n",
    "        project = lambda x: np.clip(x, bounds[:, 0], bounds[:, 1])\n",
    "\n",
    "    if args is not None:\n",
    "        # freeze function arguments\n",
    "        def funcf(x, **kwargs):\n",
    "            return func(x, *args, **kwargs)\n",
    "\n",
    "    N = len(x0)\n",
    "    x = x0\n",
    "    for k in range(k0, niter):\n",
    "        ak = a/(k+1.0+A)**alpha\n",
    "        ck = c/(k+1.0)**gamma\n",
    "        delta = np.random.choice([-1, 1], size=N)\n",
    "        fkwargs = {\"k\": k+1}\n",
    "        if paired:\n",
    "            fkwargs['seed'] = np.random.randint(0, np.iinfo(np.uint32).max, dtype=np.int64)\n",
    "        if bounds is None:\n",
    "            grad = (funcf(x + ck*delta, **fkwargs) - funcf(x - ck*delta, **fkwargs)) / (2*ck*delta)\n",
    "        else:\n",
    "            # ensure evaluation points are feasible\n",
    "            xplus = project(x + ck*delta)\n",
    "            xminus = project(x - ck*delta)\n",
    "            grad = (funcf(xplus, **fkwargs) - funcf(xminus, **fkwargs)) / (xplus-xminus)\n",
    "        x = project(x - ak*grad)\n",
    "\n",
    "        # print 100 status updates if disp=True\n",
    "        if disp and (k % (niter//100)) == 0:\n",
    "            print(x)\n",
    "        if callback is not None:\n",
    "            callback(x)\n",
    "    message = 'terminated after reaching max number of iterations'\n",
    "    return OptimizeResult(fun=funcf(x), x=x, nit=niter, nfev=2*niter, message=message, success=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from noisyopt import minimizeSPSA\n",
    "import time\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy import default_sort_key\n",
    "import gc\n",
    "\n",
    "class Result:\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\n",
    "def make_cost_fn(pred_fn, labels, directory, name):\n",
    "    if name == \"test\":\n",
    "        with open(f'{directory}/{name}_costs.csv', 'w') as f:\n",
    "            pass\n",
    "    \n",
    "    if name == \"test\":\n",
    "        with open(f'{directory}/{name}_accs.csv', 'w') as f:\n",
    "            pass\n",
    "\n",
    "    if name == \"test\":\n",
    "        with open(f'{directory}/params_raw.txt', 'w') as f:\n",
    "            pass\n",
    "    \n",
    "    def cost_fn(params, **kwargs):\n",
    "        predictions = pred_fn(params)\n",
    "\n",
    "        cost = -np.sum(labels * np.log(predictions)) / len(labels)  # binary cross-entropy loss\n",
    "        \n",
    "        with open(f'{directory}/{name}_costs.csv', 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "\n",
    "            writer.writerow([cost])\n",
    "\n",
    "            f.close()\n",
    "\n",
    "        acc = np.sum(np.round(predictions) == labels) / len(labels) / 2  # half due to double-counting\n",
    "        \n",
    "        with open(f'{directory}/{name}_accs.csv', 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "\n",
    "            writer.writerow([acc])\n",
    "\n",
    "            f.close()\n",
    "        \n",
    "        with open(f'{directory}/params_raw.txt', 'w', newline='') as f:\n",
    "            paramstr = \"[\"\n",
    "            \n",
    "            for e in params:\n",
    "                paramstr += str(e) + \", \"\n",
    "\n",
    "            paramstr = paramstr[:-2] + \"]\"\n",
    "\n",
    "            f.write(paramstr)\n",
    "        \n",
    "        if name == \"dev\" or name == \"train\":\n",
    "            with open(f'../data/experiment_results/journal/generalQC/{experiment_name}/start_time.txt', 'r') as f:\n",
    "                start_time = float(f.readline().strip())\n",
    "            \n",
    "            with open(f'../data/experiment_results/journal/generalQC/{experiment_name}/fit_time.txt', 'w') as f:\n",
    "                f.write(str(time.time()-start_time))\n",
    "            \n",
    "            k = kwargs.get(\"k\", 0)\n",
    "            \n",
    "            if k in [130, 200, 500, 600, 700, 800, 900, 1000]:\n",
    "                with open(f'{directory}/params_{k}.txt', 'w', newline='') as f:\n",
    "                    paramstr = \"[\"\n",
    "                    \n",
    "                    for e in params:\n",
    "                        paramstr += str(e) + \", \"\n",
    "\n",
    "                    paramstr = paramstr[:-2] + \"]\"\n",
    "\n",
    "                    f.write(paramstr)\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "        return cost\n",
    "\n",
    "    return cost_fn\n",
    "\n",
    "def run_circuit_optimization(train_circuits, train_targets, dev_circuits, dev_targets, test_circuits, test_targets, experiment_name, train_params=True, load_params=False, params_saved=[], spsa_n_iter=0, k0=0):\n",
    "    # collect params\n",
    "    all_circuits = train_circuits + dev_circuits + test_circuits\n",
    "\n",
    "    # sort the symbols since they are returned as a set\n",
    "    parameters = sorted(\n",
    "        {s for circ in all_circuits for s in circ.free_symbols},\n",
    "        key=default_sort_key)\n",
    "    \n",
    "    # make prediction and cost functions\n",
    "\n",
    "    SEED = 0\n",
    "    rng = np.random.default_rng(SEED)\n",
    "\n",
    "    train_pred_fn = make_pred_fn(train_circuits, parameters, rng)\n",
    "    dev_pred_fn = make_pred_fn(dev_circuits, parameters, rng)\n",
    "    test_pred_fn = make_pred_fn(test_circuits, parameters, rng)\n",
    "\n",
    "    train_cost_fn = make_cost_fn(train_pred_fn, train_targets, '../data/experiment_results/journal/generalQC/'+experiment_name, 'train')\n",
    "    dev_cost_fn = make_cost_fn(dev_pred_fn, dev_targets, '../data/experiment_results/journal/generalQC/'+experiment_name, 'dev')\n",
    "\n",
    "    if (load_params):\n",
    "        # load params\n",
    "\n",
    "        # x0 = [params_saved[str(param)] for param in parameters]\n",
    "        x0 = params_saved\n",
    "        result = Result(x0)\n",
    "    else:\n",
    "        x0 = np.array(rng.random(len(parameters)))\n",
    "        np.random.seed(SEED)\n",
    "\n",
    "    # train params using SPSA\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with open(f'../data/experiment_results/journal/generalQC/{experiment_name}/start_time.txt', 'w') as f:\n",
    "        try:\n",
    "            with open(path+'/fit_time.txt') as f2:\n",
    "                delta = float(f2.readline().strip())\n",
    "        except:\n",
    "            delta = 0\n",
    "        \n",
    "        f.write(str(start_time-delta))\n",
    "\n",
    "    # result = minimizeSPSA(train_cost_fn, x0=x0, a=spsa_a, c=spsa_c, niter=10, callback=dev_cost_fn)\n",
    "    if train_params:\n",
    "        result = myMinimizeSPSA(train_cost_fn, x0=x0, a=spsa_a, c=spsa_c, k0=k0, niter=spsa_n_iter, callback=dev_cost_fn)\n",
    "    else:\n",
    "        result = Result(x0)\n",
    "    \n",
    "    finish_time = time.time()\n",
    "\n",
    "    # with open(f'../data/experiment_results/journal/generalQC/{experiment_name}/fit_time.txt', 'w') as f:\n",
    "    #     f.write(str(finish_time-start_time))\n",
    "\n",
    "    train_costs = []\n",
    "    train_accs = []\n",
    "    dev_costs = []\n",
    "    dev_accs = []\n",
    "\n",
    "    with open(f'../data/experiment_results/journal/generalQC/{experiment_name}/train_costs.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "        for row in csv_reader:\n",
    "            train_costs.append(float(row[0]))\n",
    "\n",
    "    with open(f'../data/experiment_results/journal/generalQC/{experiment_name}/train_accs.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "        for row in csv_reader:\n",
    "            train_accs.append(float(row[0]))\n",
    "\n",
    "    with open(f'../data/experiment_results/journal/generalQC/{experiment_name}/dev_costs.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "        for row in csv_reader:\n",
    "            dev_costs.append(float(row[0]))\n",
    "\n",
    "    with open(f'../data/experiment_results/journal/generalQC/{experiment_name}/dev_accs.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "        for row in csv_reader:\n",
    "            dev_accs.append(float(row[0]))\n",
    "\n",
    "    if train_params:\n",
    "        print(\"Fit time:\", finish_time-start_time)\n",
    "        \n",
    "        fig, ((ax_tl, ax_tr), (ax_bl, ax_br)) = plt.subplots(2, 2, sharex=True, sharey='row', figsize=(10, 6))\n",
    "        ax_tl.set_title('Training set')\n",
    "        ax_tr.set_title('Development set')\n",
    "        ax_bl.set_xlabel('Iterations')\n",
    "        ax_br.set_xlabel('Iterations')\n",
    "        ax_bl.set_ylabel('Accuracy')\n",
    "        ax_tl.set_ylabel('Loss')\n",
    "\n",
    "        colours = iter(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "        ax_tl.plot(train_costs[1::2], color=next(colours))  # training evaluates twice per iteration\n",
    "        ax_bl.plot(train_accs[1::2], color=next(colours))   # so take every other entry\n",
    "        ax_tr.plot(dev_costs, color=next(colours))\n",
    "        ax_br.plot(dev_accs, color=next(colours))\n",
    "\n",
    "    # print test accuracy\n",
    "    test_cost_fn = make_cost_fn(test_pred_fn, test_targets, '../data/experiment_results/journal/generalQC/'+experiment_name, 'test')\n",
    "    test_cost_fn(result.x)\n",
    "\n",
    "    with open(f'../data/experiment_results/journal/generalQC/{experiment_name}/test_accs.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "        for row in csv_reader:\n",
    "            print('Test accuracy:', row[0])\n",
    "            test_accs = [row[0]]\n",
    "            break\n",
    "    \n",
    "    paramdict = {}\n",
    "    paramdict_print = {}\n",
    "\n",
    "    for value, parameter in zip(result.x, parameters):\n",
    "        paramdict[parameter] = value\n",
    "        paramdict_print[str(parameter)] = value\n",
    "\n",
    "    with open('../data/experiment_results/journal/generalQC/'+experiment_name+'/params.txt', 'w') as f:\n",
    "        f.write(str(paramdict_print))\n",
    "\n",
    "    # print(paramdict_print)\n",
    "\n",
    "    opt_results = {\n",
    "        \"params\": {\n",
    "            \"parameters\": parameters, \n",
    "            \"x\": result.x\n",
    "            }, \n",
    "        \"train\": {\n",
    "            \"pred_fn\": train_pred_fn, \n",
    "            \"cost_fn\": train_cost_fn, \n",
    "            \"accs\": train_accs[1::2], \n",
    "            \"costs\": train_costs[1::2]\n",
    "            }, \n",
    "        \"dev\": {\n",
    "            \"pred_fn\": dev_pred_fn, \n",
    "            \"cost_fn\": dev_cost_fn, \n",
    "            \"accs\": dev_accs, \n",
    "            \"costs\": dev_costs\n",
    "            }, \n",
    "        \"test\": {\n",
    "            \"pred_fn\": test_pred_fn, \n",
    "            \"cost_fn\": test_cost_fn, \n",
    "            \"accs\": test_accs, \n",
    "            \"costs\": None\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return opt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_results = run_circuit_optimization(train_circuits, train_targets, dev_circuits, dev_targets, test_circuits, test_targets, experiment_name, train_params, load_params, params_saved=params_saved, spsa_n_iter=spsa_n_iter, k0=k0)\n",
    "\n",
    "train_pred_fn, train_cost_fn, train_accs, train_costs = list(opt_results[\"train\"].values())\n",
    "dev_pred_fn, dev_cost_fn, dev_accs, dev_costs = list(opt_results[\"dev\"].values())\n",
    "test_pred_fn, test_cost_fn, test_accs, test_costs = list(opt_results[\"test\"].values())\n",
    "parameters, x = list(opt_results[\"params\"].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_saved = [0.8749322865720756, 0.2918365913147208, -0.2952765317895299, -1.0681942487374994, -0.11709903637179135, 0.225240048020288, 1.347454490877112, 1.7725800617164202, -0.9370088437285674, 0.42393777835086793, 0.19100430558566117, -0.7048562692855839, 2.0401355709117914, 1.7381434569549918, 2.3376205006453774, -1.2781774544160915, 3.1215053834784574, 1.2245501707108752, 0.3673433447181647, 1.7048101246859295, 0.5442359856652447, -0.8372303700776679, 1.748709696157966, 0.14679602752190662, 1.1907054464463707, -0.9273284813142214, -0.2899857451077683, -0.6382613183524387, 2.261604071114173, -0.16575745152694063, 0.7681393455139868, 0.46148129726105197, -2.9920672553016803, 2.6294550782042205, 1.9622392335813221, -0.4869486098080297, 1.6097283409072103, 0.911692027955284, 2.2376548583820717, 0.24930417104753172, -0.7993297671454297, 0.3444079670777545, -0.765967196662395, 0.17738482948156412, -0.5851264798683463, 2.711550136784392, -0.01742248277694594, 2.9530595314767996, -0.5390457145415585, 0.9974795084166438, 1.53300773012916, -0.6819692326933454, 1.1989939264774578, -0.22804571776523727, 1.0942326066839356, -0.05561228855034746, -0.5129441231419766, -0.6076440805756912, 1.978640702933142, -0.17230460524058475, -1.1168831068590193, 0.21076499137163826, 0.2626736184954581, -0.027039955605996587, -0.2525845670970584, -0.1110671620529433, 2.2311963754532322, -1.138378944366859, -1.880837766597103, 0.22048206726443284, 1.6525813632235928, -0.5225818754060868, 2.244026510256766, -3.0686338693707014, 0.5769134480099306, -0.30602618092432327, 2.1125723125313836, -0.6078446287577609, 1.983861184229709, 0.36496575889029886, 1.410960057243489, -0.4203370520582946, 1.7197112000261312, 0.2718916832613005, 0.6713749474327398, 1.9388063452980424, -0.30579853639359816, 1.838929493336053, -1.6953900618271127, 0.24199561902857128, 1.4151810130000653, 0.3941581008515613, 1.0455231429539509, -2.5229071108232817, -0.5166501467412892, -0.6257539774996822, 2.450300031521079, -0.38425157571051743, -0.9899343355875563, 1.6449482579978338, 1.3722619793940065, 1.3208847955919982, 1.1788327396658522, -0.6049731748872825, -2.0388354985494597, -1.3889704212622178, -0.8071472513102501, 1.85373354775974, -1.588621485809049]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_circuits = train_circuits + dev_circuits + test_circuits\n",
    "parameters = sorted(\n",
    "    {s for circ in all_circuits for s in circ.free_symbols},\n",
    "    key=default_sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {}\n",
    "\n",
    "for p, v in zip(parameters, params_saved):\n",
    "    params_dict[str(p)] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amaz__n.r@s_0': 0.8749322865720756,\n",
       " 'amaz__n.r@s_1': 0.2918365913147208,\n",
       " 'amaz__n@n.l_0': -0.2952765317895299,\n",
       " 'amaz__n@n.l_1': -1.0681942487374994,\n",
       " 'aw__n.r@s_0': -0.11709903637179135,\n",
       " 'aw__n.r@s_1': 0.225240048020288,\n",
       " 'aw__n@n.l_0': 1.347454490877112,\n",
       " 'aw__n@n.l_1': 1.7725800617164202,\n",
       " 'bad__n.r@s_0': -0.9370088437285674,\n",
       " 'bad__n.r@s_1': 0.42393777835086793,\n",
       " 'bad__n@n.l_0': 0.19100430558566117,\n",
       " 'bad__n@n.l_1': -0.7048562692855839,\n",
       " 'bland__n.r@s_0': 2.0401355709117914,\n",
       " 'bland__n.r@s_1': 1.7381434569549918,\n",
       " 'bland__n@n.l_0': 2.3376205006453774,\n",
       " 'bland__n@n.l_1': -1.2781774544160915,\n",
       " 'cook__n.r@s@n.l_0': 3.1215053834784574,\n",
       " 'cook__n.r@s@n.l_1': 1.2245501707108752,\n",
       " 'cook__n.r@s@n.l_2': 0.3673433447181647,\n",
       " 'delici__n.r@s_0': 1.7048101246859295,\n",
       " 'delici__n.r@s_1': 0.5442359856652447,\n",
       " 'delici__n@n.l_0': -0.8372303700776679,\n",
       " 'delici__n@n.l_1': 1.748709696157966,\n",
       " 'dislik__n.r@s@n.l_0': 0.14679602752190662,\n",
       " 'dislik__n.r@s@n.l_1': 1.1907054464463707,\n",
       " 'dislik__n.r@s@n.l_2': -0.9273284813142214,\n",
       " 'excel__n.r@s_0': -0.2899857451077683,\n",
       " 'excel__n.r@s_1': -0.6382613183524387,\n",
       " 'excel__n@n.l_0': 2.261604071114173,\n",
       " 'excel__n@n.l_1': -0.16575745152694063,\n",
       " 'fast__n.r@s_0': 0.7681393455139868,\n",
       " 'fast__n.r@s_1': 0.46148129726105197,\n",
       " 'fast__n@n.l_0': -2.9920672553016803,\n",
       " 'fast__n@n.l_1': 2.6294550782042205,\n",
       " 'good__n.r@s_0': 1.9622392335813221,\n",
       " 'good__n.r@s_1': -0.4869486098080297,\n",
       " 'good__n@n.l_0': 1.6097283409072103,\n",
       " 'good__n@n.l_1': 0.911692027955284,\n",
       " 'great__n.r@s_0': 2.2376548583820717,\n",
       " 'great__n.r@s_1': 0.24930417104753172,\n",
       " 'great__n@n.l_0': -0.7993297671454297,\n",
       " 'great__n@n.l_1': 0.3444079670777545,\n",
       " 'had__n.r@s@n.l_0': -0.765967196662395,\n",
       " 'had__n.r@s@n.l_1': 0.17738482948156412,\n",
       " 'had__n.r@s@n.l_2': -0.5851264798683463,\n",
       " 'hate__n.r@s@n.l_0': 2.711550136784392,\n",
       " 'hate__n.r@s@n.l_1': -0.01742248277694594,\n",
       " 'hate__n.r@s@n.l_2': 2.9530595314767996,\n",
       " 'horribl__n.r@s_0': -0.5390457145415585,\n",
       " 'horribl__n.r@s_1': 0.9974795084166438,\n",
       " 'horribl__n@n.l_0': 1.53300773012916,\n",
       " 'horribl__n@n.l_1': -0.6819692326933454,\n",
       " 'impecc__n.r@s_0': 1.1989939264774578,\n",
       " 'impecc__n.r@s_1': -0.22804571776523727,\n",
       " 'impecc__n@n.l_0': 1.0942326066839356,\n",
       " 'impecc__n@n.l_1': -0.05561228855034746,\n",
       " 'kind__n.r@s_0': -0.5129441231419766,\n",
       " 'kind__n.r@s_1': -0.6076440805756912,\n",
       " 'kind__n@n.l_0': 1.978640702933142,\n",
       " 'kind__n@n.l_1': -0.17230460524058475,\n",
       " 'like__n.r@s@n.l_0': -1.1168831068590193,\n",
       " 'like__n.r@s@n.l_1': 0.21076499137163826,\n",
       " 'like__n.r@s@n.l_2': 0.2626736184954581,\n",
       " 'lousi__n.r@s_0': -0.027039955605996587,\n",
       " 'lousi__n.r@s_1': -0.2525845670970584,\n",
       " 'lousi__n@n.l_0': -0.1110671620529433,\n",
       " 'lousi__n@n.l_1': 2.2311963754532322,\n",
       " 'love__n.r@s@n.l_0': -1.138378944366859,\n",
       " 'love__n.r@s@n.l_1': -1.880837766597103,\n",
       " 'love__n.r@s@n.l_2': 0.22048206726443284,\n",
       " 'nice__n.r@s_0': 1.6525813632235928,\n",
       " 'nice__n.r@s_1': -0.5225818754060868,\n",
       " 'nice__n@n.l_0': 2.244026510256766,\n",
       " 'nice__n@n.l_1': -3.0686338693707014,\n",
       " 'poor__n.r@s_0': 0.5769134480099306,\n",
       " 'poor__n.r@s_1': -0.30602618092432327,\n",
       " 'poor__n@n.l_0': 2.1125723125313836,\n",
       " 'poor__n@n.l_1': -0.6078446287577609,\n",
       " 'rude__n.r@s_0': 1.983861184229709,\n",
       " 'rude__n.r@s_1': 0.36496575889029886,\n",
       " 'rude__n@n.l_0': 1.410960057243489,\n",
       " 'rude__n@n.l_1': -0.4203370520582946,\n",
       " 'show__n.r@s@n.l_0': 1.7197112000261312,\n",
       " 'show__n.r@s@n.l_1': 0.2718916832613005,\n",
       " 'show__n.r@s@n.l_2': 0.6713749474327398,\n",
       " 'slow__n.r@s_0': 1.9388063452980424,\n",
       " 'slow__n.r@s_1': -0.30579853639359816,\n",
       " 'slow__n@n.l_0': 1.838929493336053,\n",
       " 'slow__n@n.l_1': -1.6953900618271127,\n",
       " 'tasti__n.r@s_0': 0.24199561902857128,\n",
       " 'tasti__n.r@s_1': 1.4151810130000653,\n",
       " 'tasti__n@n.l_0': 0.3941581008515613,\n",
       " 'tasti__n@n.l_1': 1.0455231429539509,\n",
       " 'terribl__n.r@s_0': -2.5229071108232817,\n",
       " 'terribl__n.r@s_1': -0.5166501467412892,\n",
       " 'terribl__n@n.l_0': -0.6257539774996822,\n",
       " 'terribl__n@n.l_1': 2.450300031521079,\n",
       " 'unappet__n.r@s_0': -0.38425157571051743,\n",
       " 'unappet__n.r@s_1': -0.9899343355875563,\n",
       " 'unappet__n@n.l_0': 1.6449482579978338,\n",
       " 'unappet__n@n.l_1': 1.3722619793940065,\n",
       " 'unpleas__n.r@s_0': 1.3208847955919982,\n",
       " 'unpleas__n.r@s_1': 1.1788327396658522,\n",
       " 'unpleas__n@n.l_0': -0.6049731748872825,\n",
       " 'unpleas__n@n.l_1': -2.0388354985494597,\n",
       " 'yummi__n.r@s_0': -1.3889704212622178,\n",
       " 'yummi__n.r@s_1': -0.8071472513102501,\n",
       " 'yummi__n@n.l_0': 1.85373354775974,\n",
       " 'yummi__n@n.l_1': -1.588621485809049}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "dev\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "all_circuit_fns = {}\n",
    "all_predictions = {}\n",
    "\n",
    "for name in [\"train\", \"dev\", \"test\"]:\n",
    "    print(name)\n",
    "\n",
    "    SEED = 0\n",
    "    rng = np.random.default_rng(SEED)\n",
    "\n",
    "    if name == \"train\":\n",
    "        circuits = train_circuits\n",
    "    elif name == \"dev\":\n",
    "        circuits = dev_circuits\n",
    "    else:\n",
    "        circuits = test_circuits\n",
    "\n",
    "    measured_circuits = [c >> Id().tensor(*[Measure()] * len(c.cod)) for c in circuits]\n",
    "    circuit_fns = [c.lambdify(*parameters) for c in measured_circuits]\n",
    "\n",
    "    outputs = Circuit.eval(*(c_fn(*params_saved) for c_fn in circuit_fns),\n",
    "                                **backend_config, seed=randint(rng))\n",
    "    results = np.array([normalise(output.array) for output in outputs])\n",
    "    predictions = np.round(results)\n",
    "\n",
    "    all_results[name] = results\n",
    "    all_circuit_fns[name] = [c_fn(*params_saved) for c_fn in circuit_fns]\n",
    "    all_predictions[name] = predictions\n",
    "\n",
    "    # with open(f'../data/experiment_results/journal/generalQC/{experiment_name}/{name}_outputs.csv', 'w', newline='') as f:\n",
    "    #     writer = csv.writer(f)\n",
    "\n",
    "    #     for e in results:\n",
    "    #         writer.writerow(e)\n",
    "\n",
    "    #     f.close()\n",
    "    \n",
    "    # with open(f'../data/experiment_results/journal/generalQC/{experiment_name}/{name}_predictions.txt', 'w', newline='') as f:\n",
    "    #     writer = csv.writer(f)\n",
    "\n",
    "    #     for e in predictions:\n",
    "    #         writer.writerow([int(e[0])])\n",
    "\n",
    "    #     f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results, dev_results, test_results = [all_results[name] for name in [\"train\", \"dev\", \"test\"]]\n",
    "train_circ_fns, dev_circ_fns, test_circ_fns = [all_circuit_fns[name] for name in [\"train\", \"dev\", \"test\"]]\n",
    "train_preds, dev_preds, test_preds = [all_predictions[name] for name in [\"train\", \"dev\", \"test\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Circuit(dom=Ty(), cod=bit, boxes=[Ket(0, 0, 0), H, H, H, Controlled(Rx(3.12), distance=2), Controlled(Rx(1.22), distance=-1), Controlled(Rx(0.367), distance=-1), H, H, H, Controlled(Rx(3.12), distance=1), Controlled(Rx(1.22), distance=1), Controlled(Rx(0.367), distance=-2), Ket(0), Controlled(Rx(1.37), distance=1), Controlled(Rx(1.64), distance=-1), H, H, Controlled(Rx(1.37), distance=1), Controlled(Rx(1.64), distance=-1), H, H, Bra(0, 0), Bra(0), Measure()], offsets=[0, 0, 1, 2, 0, 1, 0, 0, 1, 2, 0, 1, 0, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 0, 0])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_circ_fns[89]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "[0.50195312 0.49804688]\n",
      "[0 1]\n",
      "The restaurant was terrible\n",
      "27\n",
      "---\n",
      "[0.9287234 0.0712766]\n",
      "[0 1]\n",
      "The restaurant was awful\n",
      "28\n",
      "---\n",
      "[0.91545926 0.08454074]\n",
      "[0 1]\n",
      "The food was bland\n",
      "32\n",
      "---\n",
      "[0.9397132 0.0602868]\n",
      "[0 1]\n",
      "The service was slow\n",
      "36\n",
      "---\n",
      "[0.36666667 0.63333333]\n",
      "[1 0]\n",
      "The restaurant cooked nice food\n",
      "45\n",
      "---\n",
      "[0.35281147 0.64718853]\n",
      "[1 0]\n",
      "The restaurant cooked tasty food\n",
      "46\n",
      "---\n",
      "[0.49102636 0.50897364]\n",
      "[1 0]\n",
      "The restaurant was not terrible\n",
      "55\n",
      "---\n",
      "[0.07302553 0.92697447]\n",
      "[1 0]\n",
      "The restaurant was not awful\n",
      "56\n",
      "---\n",
      "[0.07121938 0.92878062]\n",
      "[1 0]\n",
      "The food was not awful\n",
      "59\n",
      "---\n",
      "[0.08591157 0.91408843]\n",
      "[1 0]\n",
      "The food was not bland\n",
      "60\n",
      "---\n",
      "[0.06788408 0.93211592]\n",
      "[1 0]\n",
      "The service was not awful\n",
      "64\n",
      "---\n",
      "[0.07093755 0.92906245]\n",
      "[1 0]\n",
      "The service was not slow\n",
      "65\n",
      "---\n",
      "[0.93017617 0.06982383]\n",
      "[0 1]\n",
      "The service was rude\n",
      "83\n",
      "---\n",
      "[0.36845865 0.63154135]\n",
      "[1 0]\n",
      "The restaurant cooked delicious food\n",
      "88\n",
      "---\n",
      "[0.50296192 0.49703808]\n",
      "[0 1]\n",
      "The meal was terrible\n",
      "119\n",
      "---\n",
      "[0.9269072 0.0730928]\n",
      "[0 1]\n",
      "The meal was awful\n",
      "120\n",
      "---\n",
      "[0.91433125 0.08566875]\n",
      "[0 1]\n",
      "The meal was bland\n",
      "121\n",
      "---\n",
      "[0.92863787 0.07136213]\n",
      "[0 1]\n",
      "The staff was awful\n",
      "126\n",
      "---\n",
      "[0.93559377 0.06440623]\n",
      "[0 1]\n",
      "The staff was rude\n",
      "127\n",
      "---\n",
      "[0.34543011 0.65456989]\n",
      "[1 0]\n",
      "The restaurant cooked great meals\n",
      "141\n",
      "---\n",
      "[0.36283187 0.63716813]\n",
      "[1 0]\n",
      "The restaurant cooked nice meals\n",
      "142\n",
      "---\n",
      "[0.36047135 0.63952865]\n",
      "[1 0]\n",
      "The restaurant cooked tasty meals\n",
      "143\n",
      "---\n",
      "[0.14772729 0.85227271]\n",
      "[1 0]\n",
      "The restaurant showed great staff\n",
      "148\n",
      "---\n",
      "[0.27559058 0.72440942]\n",
      "[1 0]\n",
      "The restaurant showed fast staff\n",
      "149\n",
      "---\n",
      "[0.07053673 0.92946327]\n",
      "[1 0]\n",
      "The service was not rude\n",
      "157\n",
      "---\n",
      "[0.49199779 0.50800221]\n",
      "[1 0]\n",
      "The meal was not terrible\n",
      "161\n",
      "---\n",
      "[0.0830008 0.9169992]\n",
      "[1 0]\n",
      "The meal was not bland\n",
      "162\n",
      "---\n",
      "[0.49059829 0.50940171]\n",
      "[1 0]\n",
      "The staff was not terrible\n",
      "168\n",
      "---\n",
      "[0.06953291 0.93046709]\n",
      "[1 0]\n",
      "The staff was not awful\n",
      "169\n",
      "---\n",
      "[0.51391208 0.48608792]\n",
      "[0 1]\n",
      "The cafe was terrible\n",
      "197\n",
      "---\n",
      "[0.92910748 0.07089252]\n",
      "[0 1]\n",
      "The cafe was awful\n",
      "198\n",
      "---\n",
      "[0.91778254 0.08221746]\n",
      "[0 1]\n",
      "The cooking was bland\n",
      "202\n",
      "---\n",
      "[0.92837345 0.07162655]\n",
      "[0 1]\n",
      "The foodservice was slow\n",
      "206\n",
      "---\n",
      "[0.50955414 0.49044586]\n",
      "[0 1]\n",
      "The cafe had awful cooking\n",
      "210\n",
      "---\n",
      "[0.28990229 0.71009771]\n",
      "[1 0]\n",
      "The cafe cooked nice cooking\n",
      "215\n",
      "---\n",
      "[0.34935718 0.65064282]\n",
      "[1 0]\n",
      "The cafe cooked tasty cooking\n",
      "216\n",
      "---\n",
      "[0.48440111 0.51559889]\n",
      "[1 0]\n",
      "The cafe was not terrible\n",
      "225\n",
      "---\n",
      "[0.07531325 0.92468675]\n",
      "[1 0]\n",
      "The cafe was not awful\n",
      "226\n",
      "---\n",
      "[0.07554196 0.92445804]\n",
      "[1 0]\n",
      "The cooking was not awful\n",
      "229\n",
      "---\n",
      "[0.08639338 0.91360662]\n",
      "[1 0]\n",
      "The cooking was not bland\n",
      "230\n",
      "---\n",
      "[0.07356658 0.92643342]\n",
      "[1 0]\n",
      "The foodservice was not awful\n",
      "234\n",
      "---\n",
      "[0.06724638 0.93275362]\n",
      "[1 0]\n",
      "The foodservice was not slow\n",
      "235\n",
      "---\n",
      "[0.54615384 0.45384616]\n",
      "[0 1]\n",
      "I had awful foodservice\n",
      "245\n",
      "---\n",
      "[0.93260799 0.06739201]\n",
      "[0 1]\n",
      "The foodservice was rude\n",
      "253\n",
      "---\n",
      "[0.37556155 0.62443845]\n",
      "[1 0]\n",
      "The cafe cooked delicious cooking\n",
      "258\n",
      "---\n",
      "[0.51923609 0.48076391]\n",
      "[0 1]\n",
      "The dish was terrible\n",
      "289\n",
      "---\n",
      "[0.92266738 0.07733262]\n",
      "[0 1]\n",
      "The dish was awful\n",
      "290\n",
      "---\n",
      "[0.91675589 0.08324411]\n",
      "[0 1]\n",
      "The dish was bland\n",
      "291\n",
      "---\n",
      "[0.92954575 0.07045425]\n",
      "[0 1]\n",
      "The waiters was awful\n",
      "296\n",
      "---\n",
      "[0.93418293 0.06581707]\n",
      "[0 1]\n",
      "The waiters was rude\n",
      "297\n",
      "---\n",
      "[0.5620915 0.4379085]\n",
      "[0 1]\n",
      "The cafe had awful waiters\n",
      "308\n",
      "---\n",
      "[0.38633194 0.61366806]\n",
      "[1 0]\n",
      "The cafe cooked great dishes\n",
      "311\n",
      "---\n",
      "[0.35029941 0.64970059]\n",
      "[1 0]\n",
      "The cafe cooked nice dishes\n",
      "312\n",
      "---\n",
      "[0.35027323 0.64972677]\n",
      "[1 0]\n",
      "The cafe cooked tasty dishes\n",
      "313\n",
      "---\n",
      "[0.1794872 0.8205128]\n",
      "[1 0]\n",
      "The cafe showed great waiters\n",
      "318\n",
      "---\n",
      "[0.34821431 0.65178569]\n",
      "[1 0]\n",
      "The cafe showed fast waiters\n",
      "319\n",
      "---\n",
      "[0.06917124 0.93082876]\n",
      "[1 0]\n",
      "The foodservice was not rude\n",
      "327\n",
      "---\n",
      "[0.49448428 0.50551572]\n",
      "[1 0]\n",
      "The dish was not terrible\n",
      "331\n",
      "---\n",
      "[0.09411294 0.90588706]\n",
      "[1 0]\n",
      "The dish was not bland\n",
      "332\n",
      "---\n",
      "[0.4910249 0.5089751]\n",
      "[1 0]\n",
      "The waiters was not terrible\n",
      "338\n",
      "---\n",
      "[0.07342611 0.92657389]\n",
      "[1 0]\n",
      "The waiters was not awful\n",
      "339\n",
      "---\n",
      "[0.30130294 0.69869706]\n",
      "[1 0]\n",
      "The restaurant had excellent food\n",
      "371\n",
      "---\n",
      "[0.29617305 0.70382695]\n",
      "[1 0]\n",
      "The restaurant had excellent service\n",
      "372\n",
      "---\n",
      "[0.30333334 0.69666666]\n",
      "[1 0]\n",
      "I had excellent service\n",
      "374\n",
      "---\n",
      "[0.41468683 0.58531317]\n",
      "[1 0]\n",
      "The restaurant cooked excellent meals\n",
      "378\n",
      "---\n",
      "[0.30878049 0.69121951]\n",
      "[1 0]\n",
      "The restaurant showed excellent staff\n",
      "379\n",
      "---\n",
      "[0.3030822 0.6969178]\n",
      "[1 0]\n",
      "The cafe had excellent cooking\n",
      "384\n",
      "---\n",
      "[0.28271406 0.71728594]\n",
      "[1 0]\n",
      "The cafe had excellent foodservice\n",
      "385\n",
      "---\n",
      "[0.29146538 0.70853462]\n",
      "[1 0]\n",
      "I had excellent foodservice\n",
      "387\n",
      "---\n",
      "[0.44633369 0.55366631]\n",
      "[1 0]\n",
      "The cafe cooked excellent dishes\n",
      "391\n",
      "---\n",
      "[0.29674396 0.70325604]\n",
      "[1 0]\n",
      "The cafe showed excellent waiters\n",
      "392\n",
      "---\n",
      "[0.61073824 0.38926176]\n",
      "[0 1]\n",
      "The restaurant had horrible food\n",
      "398\n",
      "---\n",
      "[0.5608108 0.4391892]\n",
      "[0 1]\n",
      "The cafe had horrible cooking\n",
      "414\n",
      "---\n",
      "[0.5034965 0.4965035]\n",
      "[0 1]\n",
      "The cafe had horrible foodservice\n",
      "415\n",
      "---\n",
      "[0.92180851 0.07819149]\n",
      "[0 1]\n",
      "The restaurant was lousy\n",
      "428\n",
      "---\n",
      "[0.07776009 0.92223991]\n",
      "[1 0]\n",
      "The restaurant was not lousy\n",
      "431\n",
      "---\n",
      "[0.06700548 0.93299452]\n",
      "[1 0]\n",
      "The food was not lousy\n",
      "432\n",
      "---\n",
      "[0.07821599 0.92178401]\n",
      "[1 0]\n",
      "The service was not lousy\n",
      "433\n",
      "---\n",
      "[0.92992953 0.07007047]\n",
      "[0 1]\n",
      "The meal was lousy\n",
      "438\n",
      "---\n",
      "[0.92581721 0.07418279]\n",
      "[0 1]\n",
      "The staff was lousy\n",
      "439\n",
      "---\n",
      "[0.07541639 0.92458361]\n",
      "[1 0]\n",
      "The staff was not lousy\n",
      "443\n",
      "---\n",
      "[0.92372204 0.07627796]\n",
      "[0 1]\n",
      "The cafe was lousy\n",
      "446\n",
      "---\n",
      "[0.0741573 0.9258427]\n",
      "[1 0]\n",
      "The cafe was not lousy\n",
      "449\n",
      "---\n",
      "[0.0777437 0.9222563]\n",
      "[1 0]\n",
      "The cooking was not lousy\n",
      "450\n",
      "---\n",
      "[0.06882806 0.93117194]\n",
      "[1 0]\n",
      "The foodservice was not lousy\n",
      "451\n",
      "---\n",
      "[0.92849548 0.07150452]\n",
      "[0 1]\n",
      "The dish was lousy\n",
      "456\n",
      "---\n",
      "[0.92294314 0.07705686]\n",
      "[0 1]\n",
      "The waiters was lousy\n",
      "457\n",
      "---\n",
      "[0.07596796 0.92403204]\n",
      "[1 0]\n",
      "The waiters was not lousy\n",
      "461\n",
      "---\n",
      "[0.28054863 0.71945137]\n",
      "[1 0]\n",
      "The food was yummy\n",
      "464\n",
      "---\n",
      "[0.45890411 0.54109589]\n",
      "[1 0]\n",
      "The restaurant cooked yummy food\n",
      "465\n",
      "---\n",
      "[0.67380952 0.32619048]\n",
      "[0 1]\n",
      "The food was not yummy\n",
      "466\n",
      "---\n",
      "[0.26104418 0.73895582]\n",
      "[1 0]\n",
      "I had yummy food\n",
      "467\n",
      "---\n",
      "[0.43058824 0.56941176]\n",
      "[1 0]\n",
      "The restaurant cooked yummy meals\n",
      "469\n",
      "---\n",
      "[0.71019473 0.28980527]\n",
      "[0 1]\n",
      "The meal was not yummy\n",
      "470\n",
      "---\n",
      "[0.28318585 0.71681415]\n",
      "[1 0]\n",
      "The cooking was yummy\n",
      "473\n",
      "---\n",
      "[0.39663462 0.60336538]\n",
      "[1 0]\n",
      "The cafe cooked yummy cooking\n",
      "474\n",
      "---\n",
      "[0.72727272 0.27272728]\n",
      "[0 1]\n",
      "The cooking was not yummy\n",
      "475\n",
      "---\n",
      "[0.28854626 0.71145374]\n",
      "[1 0]\n",
      "I had yummy cooking\n",
      "476\n",
      "---\n",
      "[0.42388759 0.57611241]\n",
      "[1 0]\n",
      "The cafe cooked yummy dishes\n",
      "478\n",
      "---\n",
      "[0.71245634 0.28754366]\n",
      "[0 1]\n",
      "The dish was not yummy\n",
      "479\n",
      "---\n",
      "[0.39184953 0.60815047]\n",
      "[1 0]\n",
      "I had kind service\n",
      "504\n",
      "---\n",
      "[0.23438705 0.76561295]\n",
      "[1 0]\n",
      "The restaurant showed kind staff\n",
      "506\n",
      "---\n",
      "[0.35287611 0.64712389]\n",
      "[1 0]\n",
      "I had kind foodservice\n",
      "512\n",
      "---\n",
      "[0.23691832 0.76308168]\n",
      "[1 0]\n",
      "The cafe showed kind waiters\n",
      "514\n",
      "---\n",
      "[0.48851859 0.51148141]\n",
      "[1 0]\n",
      "The foodservice was not poor\n",
      "528\n"
     ]
    }
   ],
   "source": [
    "for i, (pred, target) in enumerate(zip(train_preds, train_targets)):    \n",
    "    if int(pred[0]) != target[0]:\n",
    "        print(\"---\")\n",
    "        print(train_results[i])\n",
    "        print(train_targets[i])\n",
    "        print(train_data[i])\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "The restaurant was terrible\n",
      "[0.50195312 0.49804688]\n",
      "[0 1]\n",
      "27\n",
      "---\n",
      "The restaurant had awful food\n",
      "[0.49275362 0.50724638]\n",
      "[0 1]\n",
      "40\n",
      "---\n",
      "The restaurant showed awful service\n",
      "[0.47258065 0.52741935]\n",
      "[0 1]\n",
      "52\n",
      "---\n",
      "The restaurant was not terrible\n",
      "[0.49102636 0.50897364]\n",
      "[1 0]\n",
      "55\n",
      "---\n",
      "I had awful service\n",
      "[0.49677419 0.50322581]\n",
      "[0 1]\n",
      "75\n",
      "---\n",
      "The meal was terrible\n",
      "[0.50296192 0.49703808]\n",
      "[0 1]\n",
      "119\n",
      "---\n",
      "The restaurant had awful meals\n",
      "[0.49689441 0.50310559]\n",
      "[0 1]\n",
      "132\n",
      "---\n",
      "The restaurant had awful staff\n",
      "[0.4591195 0.5408805]\n",
      "[0 1]\n",
      "138\n",
      "---\n",
      "The meal was not terrible\n",
      "[0.49199779 0.50800221]\n",
      "[1 0]\n",
      "161\n",
      "---\n",
      "The staff was not terrible\n",
      "[0.49059829 0.50940171]\n",
      "[1 0]\n",
      "168\n",
      "---\n",
      "The cafe was terrible\n",
      "[0.51391208 0.48608792]\n",
      "[0 1]\n",
      "197\n",
      "---\n",
      "The cafe had awful cooking\n",
      "[0.50955414 0.49044586]\n",
      "[0 1]\n",
      "210\n",
      "---\n",
      "The cafe showed awful foodservice\n",
      "[0.46683047 0.53316953]\n",
      "[0 1]\n",
      "222\n",
      "---\n",
      "The cafe was not terrible\n",
      "[0.48440111 0.51559889]\n",
      "[1 0]\n",
      "225\n",
      "---\n",
      "I had awful foodservice\n",
      "[0.54615384 0.45384616]\n",
      "[0 1]\n",
      "245\n",
      "---\n",
      "The dish was terrible\n",
      "[0.51923609 0.48076391]\n",
      "[0 1]\n",
      "289\n",
      "---\n",
      "The dish was not terrible\n",
      "[0.49448428 0.50551572]\n",
      "[1 0]\n",
      "331\n",
      "---\n",
      "The waiters was not terrible\n",
      "[0.4910249 0.5089751]\n",
      "[1 0]\n",
      "338\n",
      "---\n",
      "I had horrible service\n",
      "[0.5 0.5]\n",
      "[0 1]\n",
      "402\n",
      "---\n",
      "The cafe had horrible foodservice\n",
      "[0.5034965 0.4965035]\n",
      "[0 1]\n",
      "415\n",
      "---\n",
      "I had horrible foodservice\n",
      "[0.5 0.5]\n",
      "[0 1]\n",
      "418\n",
      "---\n",
      "The restaurant cooked yummy food\n",
      "[0.45890411 0.54109589]\n",
      "[1 0]\n",
      "465\n",
      "---\n",
      "The service was not kind\n",
      "[0.45359093 0.54640907]\n",
      "[0 1]\n",
      "503\n",
      "---\n",
      "The staff was not kind\n",
      "[0.4517107 0.5482893]\n",
      "[0 1]\n",
      "507\n",
      "---\n",
      "The service was poor\n",
      "[0.48922457 0.51077543]\n",
      "[0 1]\n",
      "518\n",
      "---\n",
      "The service was not poor\n",
      "[0.50377782 0.49622218]\n",
      "[1 0]\n",
      "520\n",
      "---\n",
      "The foodservice was poor\n",
      "[0.49388753 0.50611247]\n",
      "[0 1]\n",
      "526\n",
      "---\n",
      "The foodservice was not poor\n",
      "[0.48851859 0.51148141]\n",
      "[1 0]\n",
      "528\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(train_results):\n",
    "    if abs(res[0]-res[1]) <= 0.1:\n",
    "    # if \"cooked\" in train_data[i]:\n",
    "        print(\"---\")\n",
    "        print(train_data[i])\n",
    "        print(res)\n",
    "        print(train_targets[i])\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_circuits[349].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The staff was amazing'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[349]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_saved = [1.3248752715833159, -0.43965306134652105, 0.4944311630382821, -0.7597867508806558, 0.22886629684727283, 0.7805299072926107, -0.16673919857161296, 1.1132607831532069, 0.7885279983519985, 1.293311585562156, 0.27987787579655454, 0.7069993493063694, 0.539067886058184, -0.6309224626904911, 0.679591271614813, 0.678180997020699, 0.5862837307183423, 1.0803182328688188, 0.7569997844621467, -0.15364929729056903, 0.9194429365315623, -0.597650018748633, -0.11715041983686239, 1.2451710067348718, 1.2645646160373234, -0.3123754470797706, 1.65818212669706, 0.2756725263001081, 1.6212562310847294, 0.8649361459221326, 1.4377407181469517, 0.13691483934745563, 0.852946232624643, 0.03529099543417345, 1.2895426567927977, 1.1706305059138862, 1.4195672672132786, -0.12981641850681036, 1.5399092760013502, -0.537676728410904, 0.9311452384846112, -0.21483617472668398, 1.2394443262962838, 0.8284364132840497, 0.6320442702899073, 1.8416330001738779, 0.7785018794985358, -0.22384213089231134, 1.0850711958110182, 1.464022669048713, 0.6321080155353188, 1.195493426530709, 1.1312767748988901, 0.44614261969860736, -0.7086663632793792, 0.6800532694319438, 0.634949264851426, 0.2458150926154213, 1.1016238491829997, -0.45466234148773044, 0.9255470706451129, 0.768476432238168, -0.9357327363951594, -0.11618113542878246, 1.0908457341599658, 1.2888333473864353, 1.1819786867897124, 0.18342421163194073, -0.0646339295072921, 0.4827595588212873, 1.1439953228207147, 1.444592493256143, 0.719276159854881, 1.691554245160545, 0.14760444827921096, 1.3832051022289646, -0.4070914428322862, 1.5226283114751922, 1.4701428496985915, 0.9196615522357608, 0.3011875200171763, -0.6257551152324748, 0.30903348490137983, 1.4322392010501475, 1.03860486105627, 0.02418095289854763, 0.22523742403949143, -0.011046156801469746, -0.44596317336077657, 1.4822077869444519, 1.698023014660406, 1.3346102689131365, -0.6873320723913127, 0.5393148069612752, 1.460409574192996, 0.33044675843741866, 0.9569359099460651, 1.4205414505709497, 0.11385078893562385, 1.3687472572678574, -0.4807772788271898, -0.29059001618975155, 0.5011081512160269, 0.31418912172566427, 1.0786115396372915, 1.0444173618697543, 1.4335886602463932, 0.4313035579446135, -0.3205556033470591, 0.35358676662135957, 0.9373084650593498, -0.470997505580771, -0.1442960386065999, -0.51539807716781, -0.19753456259053953, 1.1581881988731224, 1.309765035378945, -0.581376067482437, 0.6545668521513127, 0.7433373167254863, 0.6505960474134383, 1.1654724024198546, 0.258355073799554, 0.1310427363951783, 1.1433854810333486, 0.2029360223708327, 0.8187455507479554, -0.9474343232476651, -0.8449617862732212, 0.14686103985819676, 1.3569786429555009, 0.6629019025477542, 1.4660368790153973, 0.2242428516115895, 0.8469725533881897, -0.3734273617740115, 1.4055513336008192, -0.053362610497407315, 0.6907281819237551, 0.7027249646573656, 0.9666512855555903, 0.8015753276812606]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from noisyopt import minimizeSPSA\n",
    "import time\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy import default_sort_key\n",
    "import gc\n",
    "\n",
    "test_name = \"test_800\"\n",
    "\n",
    "# collect params\n",
    "all_circuits = train_circuits + dev_circuits + test_circuits\n",
    "\n",
    "# sort the symbols since they are returned as a set\n",
    "parameters = sorted(\n",
    "    {s for circ in all_circuits for s in circ.free_symbols},\n",
    "    key=default_sort_key)\n",
    "\n",
    "# make prediction and cost functions\n",
    "\n",
    "SEED = 0\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "test_pred_fn = make_pred_fn(test_circuits, parameters, rng)\n",
    "\n",
    "x0 = params_saved\n",
    "result = Result(x0)\n",
    "\n",
    "# print test accuracy\n",
    "test_cost_fn = make_cost_fn(test_pred_fn, test_targets, '../data/experiment_results/journal/generalQC/'+experiment_name, test_name)\n",
    "test_cost_fn(result.x)\n",
    "\n",
    "with open(f'../data/experiment_results/journal/generalQC/{experiment_name}/{test_name}_accs.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "    for row in csv_reader:\n",
    "        print('Test accuracy:', row[0])\n",
    "        test_accs = [row[0]]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
