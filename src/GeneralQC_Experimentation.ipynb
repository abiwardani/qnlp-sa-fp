{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Parameters\n",
    "\n",
    "# Experiment conditions\n",
    "use_all_rewriter_rules = True\n",
    "convert_bigraph = True\n",
    "not_representation = \"X\"\n",
    "not_placement = \"end\"\n",
    "gate_layers = [\"H\", \"CRx\"]\n",
    "# independent_sublayer = False\n",
    "\n",
    "# load params\n",
    "train_params = True\n",
    "load_params = False\n",
    "# params_saved = {'I__n_0': 1.3926202901320248, 'I__n_1': 0.008448820382520305, 'I__n_2': -0.6385783774702594, 'aw__n.r@s_0': 0.004878974097219957, 'aw__n@n.l_0': -1.710437394121971, 'bad__n.r@s_0': -2.0120646244526674, 'bad__n@n.l_0': -0.33328110433838587, 'bland__n.r@s_0': -1.8011492489090937, 'bland__n@n.l_0': -0.23301002690494765, 'cook__n.r@s@n.l_0': 2.666641605278804, 'cook__n.r@s@n.l_1': 0.35782850997078663, 'delici__n.r@s_0': -0.6784353476839611, 'delici__n@n.l_0': 2.5048532581915994, 'dislik__n.r@s@n.l_0': 1.264688638132907, 'dislik__n.r@s@n.l_1': 2.9058201511277773, 'fast__n.r@s_0': 1.5228059027468268, 'fast__n@n.l_0': 1.4318617692717328, 'food__n_0': 0.9510109180885518, 'food__n_1': 1.983545943335132, 'food__n_2': 0.31422754989262236, 'good__n.r@s_0': -0.37898061288101936, 'good__n@n.l_0': 0.3417258364922475, 'great__n.r@s_0': 0.32023762451562343, 'great__n@n.l_0': 1.9344786256084243, 'had__n.r@s@n.l_0': 0.9175005505289897, 'had__n.r@s@n.l_1': -1.126491771734188, 'hate__n.r@s@n.l_0': 1.0178843262642405, 'hate__n.r@s@n.l_1': -0.8544213946932742, 'impecc__n.r@s_0': 0.8774940821788493, 'impecc__n@n.l_0': 0.28533233081922516, 'like__n.r@s@n.l_0': 0.5238711836690633, 'like__n.r@s@n.l_1': -0.0248482491004431, 'love__n.r@s@n.l_0': -1.0716522290529822, 'love__n.r@s@n.l_1': 0.4674574117431039, 'meal__n_0': -0.2913227630628624, 'meal__n_1': -0.7824158049259571, 'meal__n_2': 2.364685233940882, 'nice__n.r@s_0': -0.31906215984000647, 'nice__n@n.l_0': -2.087345064482271, 'restaur__n_0': 1.9567881690203595, 'restaur__n_1': 0.35319298096689367, 'restaur__n_2': -0.5953604557383299, 'rude__n.r@s_0': 2.1631540155864895, 'rude__n@n.l_0': -0.9753409914505806, 'servic__n_0': -2.7047864895209357, 'servic__n_1': 1.2351321590961786, 'servic__n_2': -1.5851261977616085, 'show__n.r@s@n.l_0': -0.6393012792612347, 'show__n.r@s@n.l_1': 1.7857303585103954, 'slow__n.r@s_0': 0.5156333465906835, 'slow__n@n.l_0': 1.0425765284092643, 'staff__n_0': 0.08543447326700565, 'staff__n_1': -2.068154595353262, 'staff__n_2': 0.019379227469981927, 'tasti__n.r@s_0': 1.2246133069435021, 'tasti__n@n.l_0': 1.9668107229444722, 'terribl__n.r@s_0': -0.3440262636975905, 'terribl__n@n.l_0': 2.1861199395142688, 'unappet__n.r@s_0': 1.440589048292482, 'unappet__n@n.l_0': -0.31598863954665035, 'wa__s@s.l_0': -1.0244128683696618}\n",
    "spsa_a = 0.2\n",
    "spsa_c = 0.06\n",
    "spsa_n_iter = 200\n",
    "\n",
    "# SVM type\n",
    "kernel = 'rbf'\n",
    "default_svm = True\n",
    "optimize_svm = False\n",
    "\n",
    "# Experiment metadata\n",
    "experiment_name = \"H_CRx_default\"\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment folder already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "if (experiment_name == \"\"):\n",
    "    experiment_name = \"misc\"\n",
    "\n",
    "path = '../data/experiment_results/generalQC/'+experiment_name\n",
    "\n",
    "try:\n",
    "    os.mkdir(path)\n",
    "except:\n",
    "    print(\"experiment folder already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(path+'/params.txt') as f:\n",
    "        data = f.read().replace('\\'', '\\\"')\n",
    "        params_saved = json.loads(data)\n",
    "except:\n",
    "    params_saved = None\n",
    "    train_params = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_data(filename):\n",
    "    labels, sentences = [], []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            labels.append([1, 0] if line[0] == '1' else [0, 1])\n",
    "            sentences.append(line[1:].strip())\n",
    "    return np.array(labels), sentences\n",
    "\n",
    "test_targets_src, test_data_src = read_data('../data/datasets/restaurant_v3_test.txt')\n",
    "dev_targets_src, dev_data_src = read_data('../data/datasets/restaurant_v3_dev.txt')\n",
    "train_targets_src, train_data_src = read_data('../data/datasets/restaurant_v3_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for converting to bigraph\n",
    "\n",
    "from discopy.rigid import Id as RigidId\n",
    "\n",
    "def checkTrailingCups(diagram):\n",
    "    scanWords = True\n",
    "    \n",
    "    for box in diagram.boxes:\n",
    "        if not box.dom and not scanWords:\n",
    "            return False\n",
    "        else:\n",
    "            scanWords = scanWords and not box.dom\n",
    "    \n",
    "    return True\n",
    "\n",
    "def convertToTrailingCups(diagram):\n",
    "    if (checkTrailingCups(diagram)):\n",
    "        return diagram\n",
    "\n",
    "    words = []\n",
    "    cups = []\n",
    "    \n",
    "    for box in diagram.boxes:\n",
    "        if not box.dom:\n",
    "            words = words + [box]\n",
    "        else:\n",
    "            cups = [box] + cups\n",
    "    \n",
    "    new_diag = words[0]\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if i != 0:\n",
    "            new_diag = new_diag >> RigidId(new_diag.cod) @ word\n",
    "    \n",
    "    for i, cup in enumerate(cups):\n",
    "        if i != len(cups)-1:\n",
    "            new_diag = new_diag >> RigidId(new_diag.cod[:-2]) @ cup\n",
    "        else:\n",
    "            new_diag = new_diag >> cup @ RigidId(new_diag.cod[2:])\n",
    "    \n",
    "    return new_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucntion for stemming and lemmatization of tokens\n",
    "\n",
    "def to_word_tokens(data):\n",
    "    return [word_tokenize(record) for record in data]\n",
    "\n",
    "def build_stem_dictionary(data):\n",
    "    port = PorterStemmer()\n",
    "    wnet = WordNetLemmatizer()\n",
    "    \n",
    "    mapping = {}\n",
    "    \n",
    "    data_as_tokens = to_word_tokens(data)\n",
    "    \n",
    "    for words in data_as_tokens:\n",
    "        for word in words:\n",
    "            if word not in mapping:\n",
    "                stemmed_word = port.stem(word)\n",
    "                lemmatized_word = wnet.lemmatize(stemmed_word)\n",
    "                \n",
    "                mapping[word] = lemmatized_word\n",
    "    \n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for stemming and lemmatization of diagram boxes\n",
    "\n",
    "from lambeq.rewrite import RewriteRule\n",
    "\n",
    "class StemRewriteRule(RewriteRule):\n",
    "    def __init__(self, data):\n",
    "        self.mapping = build_stem_dictionary(data)\n",
    "    \n",
    "    def matches(self, box):\n",
    "        return box.name in self.mapping\n",
    "\n",
    "    def rewrite(self, box):\n",
    "        new_name = self.mapping[box.name]\n",
    "        return type(box)(name=new_name, dom=box.dom, cod=box.cod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq.ansatz import CircuitAnsatz\n",
    "from abc import abstractmethod\n",
    "# from collections.abc import Mapping\n",
    "from itertools import cycle\n",
    "from typing import Callable, Optional, Tuple, Mapping\n",
    "from discopy.quantum.circuit import (Circuit, Functor, Id, qubit)\n",
    "from discopy.quantum.gates import Bra, H, Ket, Rx, Ry, Rz, Controlled, Rotation\n",
    "from discopy.rigid import Box, Diagram, Ty\n",
    "from discopy.tensor import Dim, Tensor\n",
    "import numpy as np\n",
    "from sympy import Symbol, symbols\n",
    "\n",
    "class GeneralQCLayer(Circuit):\n",
    "    \n",
    "    def __init__(self, n_qubits, params):\n",
    "        from discopy.quantum.gates import Rx, Rz\n",
    "\n",
    "        if len(self.gate_layers) != 2:\n",
    "            raise ValueError(\"Expected gate_layers as tuple of strings\")\n",
    "        \n",
    "        g1, g2 = self.gate_layers\n",
    "\n",
    "        if (g1 == None) or (g2 == None):\n",
    "            raise ValueError(\"gate_layers must be in discopy.quantum.gates\")\n",
    "        # if not (abs(self.r1) == 1 and abs(self.r2) == 1) and ((abs(self.r1) == 1 or abs(self.r2) == 1) and (self.r2 % self.r1 == 0 or self.r1 % self.r2 == 0)) or (n_qubits % self.r1 == 0 and self.r1 != 1) or (n_qubits % self.r2 == 0 and self.r2 != 1):\n",
    "        #     raise ValueError(\"n_qubits, r1, and r2 must be co-prime\")\n",
    "\n",
    "        params_shape = np.shape(params)\n",
    "\n",
    "        if n_qubits == 1:\n",
    "            if len(params) == 0:\n",
    "                circuit = Id(1)\n",
    "            else:\n",
    "                circuit = Rx(params[0]) >> Rz(params[1]) >> Rx(params[2])\n",
    "        elif (len(params_shape) != 2):\n",
    "            raise ValueError(\n",
    "                \"Expected params of shape (depth, {})\".format(n_qubits))\n",
    "        else:\n",
    "            g1_thetas = 0\n",
    "            g2_thetas = 0\n",
    "\n",
    "            if g1.free_symbols != {}:\n",
    "                # g1 is fixed\n",
    "                g1_thetas = n_qubits\n",
    "            if g2.free_symbols != {}:\n",
    "                g2_thetas = n_qubits\n",
    "            \n",
    "            n_thetas = 2*(g1_thetas + g2_thetas)\n",
    "\n",
    "            if (params_shape[1] != n_thetas):\n",
    "                raise ValueError(\n",
    "                    \"Expected component params with length {}\".format(n_thetas))\n",
    "\n",
    "            # ANSATZ ALGORITHM\n",
    "            circuit = Id(n_qubits)\n",
    "            \n",
    "            # for {theta} in labelled params\n",
    "            for thetas in params:\n",
    "                \n",
    "                # sublayer 1 non-entangling block\n",
    "                if g1_thetas == 0:\n",
    "                    # if g1 is fixed\n",
    "                    sublayer1 = Id().tensor(*([g1 for _ in range(n_qubits)]))\n",
    "                else:\n",
    "                    # if g1 is trainable\n",
    "                    sublayer1 = Id().tensor(*([g1(theta) for theta in thetas[:g1_thetas]]))\n",
    "                \n",
    "                # sublayer 1 entangled block\n",
    "                ctrl = 0\n",
    "\n",
    "                for i in range(n_qubits):\n",
    "                    # shift target := control - r1\n",
    "                    tgt = (ctrl - self.r1) % n_qubits\n",
    "                    \n",
    "                    if g2_thetas == 0:\n",
    "                        sublayer1 = sublayer1._apply_controlled(g2, ctrl, tgt)\n",
    "                        # sublayer1 = sublayer1.CX(ctrl, tgt)\n",
    "                    else:\n",
    "                        sublayer1 = sublayer1._apply_controlled(g2(thetas[g1_thetas + i]), ctrl, tgt)\n",
    "                        # sublayer1 = sublayer1.CRx(thetas[g1_thetas + i], ctrl, tgt)\n",
    "                    \n",
    "                    ctrl = tgt\n",
    "                \n",
    "                # sublayer 2 non-entangling block\n",
    "                if g1_thetas == 0:\n",
    "                    sublayer2 = Id().tensor(*([g1 for _ in range(n_qubits)]))\n",
    "                else:\n",
    "                    sublayer2 = Id().tensor(*([g1(theta) for theta in thetas[g1_thetas+g2_thetas:2*g1_thetas+g2_thetas]]))\n",
    "                \n",
    "                # sublayer 2 entangled block\n",
    "                ctrl = 0\n",
    "\n",
    "                for i in range(n_qubits):\n",
    "                    # shift target := control - r2\n",
    "                    tgt = (ctrl - self.r2) % n_qubits\n",
    "\n",
    "                    if g2_thetas == 0:\n",
    "                        sublayer2 = sublayer2._apply_controlled(g2, ctrl, tgt)\n",
    "                        # sublayer2 = sublayer2.CX(ctrl, tgt)\n",
    "                    else:\n",
    "                        sublayer2 = sublayer2._apply_controlled(g2(thetas[2*g1_thetas+g2_thetas+i]), ctrl, tgt)\n",
    "                        # sublayer2 = sublayer2.CRx(thetas[2*g1_thetas+g2_thetas+i], ctrl, tgt)\n",
    "                    \n",
    "                    ctrl = tgt\n",
    "            \n",
    "                # compose circuit\n",
    "                circuit >>= sublayer1 >> sublayer2\n",
    "            \n",
    "        super().__init__(\n",
    "            circuit.dom, circuit.cod, circuit.boxes, circuit.offsets)\n",
    "\n",
    "class GeneralQCAnsatz(CircuitAnsatz):\n",
    "    def __init__(self, ob_map: Mapping[Ty, int], n_layers: int, n_single_qubit_params: int = 0, gate_config = {\"gate_layers\": [H, Rx], \"var_layers\": 1}, r = (1, 3), discard: bool = False) -> None:\n",
    "        self.gate_config = gate_config\n",
    "        \n",
    "        class GeneralQCInterface(GeneralQCLayer):\n",
    "            \"\"\" Interface for GeneralQCLayer \"\"\"\n",
    "            gate_layers = self.gate_config[\"gate_layers\"]\n",
    "            r1, r2 = r\n",
    "\n",
    "        super().__init__(ob_map, n_layers, n_single_qubit_params, GeneralQCInterface, discard, [Rx, Rz])\n",
    "\n",
    "    def params_shape(self, n_qubits: int) -> Tuple[int, ...]:\n",
    "        return (self.n_layers, 2*self.gate_config[\"var_layers\"]*n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import BobcatParser, Rewriter, remove_cups\n",
    "from discopy import grammar\n",
    "from discopy.quantum.gates import X, Z\n",
    "from discopy import rigid\n",
    "\n",
    "def sentences_to_circuits(sentences, ansatz, convert_bigraph=True, not_representation=\"X\", all_sentences=None, return_valids_mask=True):\n",
    "    ### SENTENCES TO DIAGRAMS ###\n",
    "\n",
    "    if all_sentences is None:\n",
    "        all_sentences = sentences\n",
    "    \n",
    "    # syntax tree parsing\n",
    "    parser = BobcatParser()\n",
    "    raw_diagrams = parser.sentences2diagrams([text.replace(\" not \", \" \") for text in sentences])\n",
    "\n",
    "    # filter valid diagrams type S\n",
    "    n_sent = len(sentences)\n",
    "\n",
    "    valids_mask = np.array([d.cod.name == Ty('s').name for d in raw_diagrams])\n",
    "    data = [sentences[i] for i in range(n_sent) if valids_mask[i]]\n",
    "    use_diagrams = [raw_diagrams[i] for i in range(n_sent) if valids_mask[i]]\n",
    "\n",
    "    # grammatical rewrite rules\n",
    "    rewriter = Rewriter()\n",
    "    rewritten_diagrams = [rewriter(diagram) for diagram in use_diagrams]\n",
    "\n",
    "    # bigraph method\n",
    "    normalised_diagrams = [convertToTrailingCups(diagram.normal_form()) for diagram in rewritten_diagrams]\n",
    "\n",
    "    removed_diagrams = [remove_cups(diagram) for diagram in normalised_diagrams]\n",
    "\n",
    "    # stemming and lemmatization\n",
    "    stemmed_diagrams = [Rewriter([StemRewriteRule(all_sentences)])(diagram) for diagram in removed_diagrams]\n",
    "\n",
    "    # final diagrams\n",
    "    diagrams = [diagram for diagram in stemmed_diagrams]\n",
    "\n",
    "    ### DIAGRAMS to CIRCUITS ###\n",
    "\n",
    "    # string diagrams to raw quantum circuits\n",
    "    circuits = [ansatz(diagram) for diagram in diagrams]\n",
    "\n",
    "    # apply NOT box to circuits\n",
    "    for i, circuit in enumerate(circuits):\n",
    "        if data[i].find(\" not \") != -1:\n",
    "            if (not_representation == \"ZX\"):\n",
    "                circuits[i] = circuit >> Z >> X\n",
    "            else:\n",
    "                circuits[i] = circuit >> X\n",
    "    \n",
    "    if return_valids_mask:\n",
    "        return data, diagrams, circuits, valids_mask\n",
    "    else:\n",
    "        return data, diagrams, circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import AtomicType, IQPAnsatz\n",
    "from discopy.quantum.gates import H, Rx, Ry\n",
    "\n",
    "# Define atomic types\n",
    "N = AtomicType.NOUN\n",
    "S = AtomicType.SENTENCE\n",
    "\n",
    "# Convert string diagram to quantum circuit\n",
    "ansatz = GeneralQCAnsatz({N: 1, S: 1}, n_layers=1, n_single_qubit_params=0, gate_config={\"gate_layers\": [H, Rx], \"var_layers\": 1}, r=(1, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9232705e6d764c07ad8334499736d6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tagging sentences:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cab01cda1f848b6a0580ccc00e28f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing tagged sentences:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d5546962a9422a9208c32829d4fcc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse trees to diagrams:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0024e68b32cc4d1090dcd0392b46e0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tagging sentences:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355c103789424366b76a02fa09408c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing tagged sentences:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228fef6f3c984792af51aaf57c945e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse trees to diagrams:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b54650a18f24dc6974b5e681506e82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tagging sentences:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3a6dbf598f4f1fa0981a26b858f430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing tagged sentences:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1ca6f1c375416f88f401b5eea11ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse trees to diagrams:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data, train_diagrams, train_circuits, train_valids_mask = sentences_to_circuits(train_data_src, ansatz)\n",
    "dev_data, dev_diagrams, dev_circuits, dev_valids_mask = sentences_to_circuits(dev_data_src, ansatz)\n",
    "test_data, test_diagrams, test_circuits, test_valids_mask = sentences_to_circuits(test_data_src, ansatz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = [train_targets_src[i] for i, mask in enumerate(train_valids_mask) if mask]\n",
    "dev_targets = [dev_targets_src[i] for i, mask in enumerate(dev_valids_mask) if mask]\n",
    "test_targets = [test_targets_src[i] for i, mask in enumerate(test_valids_mask) if mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytket.extensions.qiskit import AerBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure backend\n",
    "backend = AerBackend()\n",
    "\n",
    "backend_config = {\n",
    "    'backend': backend,\n",
    "    'compilation': backend.default_compilation_pass(2),\n",
    "    'n_shots': 8192  # maximum recommended shots, reduces sampling error\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discopy.quantum import Circuit, Id, Measure\n",
    "\n",
    "def randint(rng, low=-1 << 63, high=1 << 63-1):\n",
    "    return rng.integers(low, high)\n",
    "\n",
    "def normalise(predictions):\n",
    "    # apply smoothing to predictions\n",
    "    predictions = np.abs(predictions) + 1e-9\n",
    "    return predictions / predictions.sum()\n",
    "\n",
    "def make_pred_fn(circuits, parameters, rng):\n",
    "    measured_circuits = [c >> Id().tensor(*[Measure()] * len(c.cod)) for c in circuits]\n",
    "    circuit_fns = [c.lambdify(*parameters) for c in measured_circuits]\n",
    "\n",
    "    def predict(params):\n",
    "        outputs = Circuit.eval(*(c_fn(*params) for c_fn in circuit_fns),\n",
    "                               **backend_config, seed=randint(rng))\n",
    "        return np.array([normalise(output.array) for output in outputs])\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from noisyopt import OptimizeResult\n",
    "\n",
    "def myMinimizeSPSA(func, x0, args=(), bounds=None, niter=100, paired=True,\n",
    "                 a=1.0, c=1.0,\n",
    "                 disp=False, callback=None):\n",
    "    \"\"\"\n",
    "    Minimization of an objective function by a simultaneous perturbation\n",
    "    stochastic approximation algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func: callable\n",
    "        objective function to be minimized\n",
    "    x0: array-like\n",
    "        starting point\n",
    "    args: tuple\n",
    "        extra arguments to be supplied to func\n",
    "    bounds: array-like\n",
    "        bounds on the variables\n",
    "    scaling: array-like\n",
    "        scaling by which to multiply step size and tolerances along different dimensions\n",
    "    niter: int\n",
    "        maximum number of iterations of the algorithm\n",
    "    paired: boolean\n",
    "        calculate gradient for same random seeds\n",
    "    a: float\n",
    "       algorithm scaling parameter for step size\n",
    "    c: float\n",
    "       algorithm scaling parameter for evaluation step size\n",
    "    disp: boolean\n",
    "        whether to output status updates during the optimization\n",
    "    callback: callable\n",
    "        called after each iteration, as callback(xk), where xk is the current parameter vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scipy.optimize.OptimizeResult object\n",
    "    \"\"\"\n",
    "    A = 0.01 * niter\n",
    "    alpha = 0.602\n",
    "    gamma = 0.101\n",
    "\n",
    "    if bounds is None:\n",
    "        project = lambda x: x\n",
    "    else:\n",
    "        bounds = np.asarray(bounds)\n",
    "        project = lambda x: np.clip(x, bounds[:, 0], bounds[:, 1])\n",
    "\n",
    "    if args is not None:\n",
    "        # freeze function arguments\n",
    "        def funcf(x, **kwargs):\n",
    "            return func(x, *args, **kwargs)\n",
    "\n",
    "    N = len(x0)\n",
    "    x = x0\n",
    "    for k in range(niter):\n",
    "        ak = a/(k+1.0+A)**alpha\n",
    "        ck = c/(k+1.0)**gamma\n",
    "        delta = np.random.choice([-1, 1], size=N)\n",
    "        fkwargs = dict()\n",
    "        if paired:\n",
    "            fkwargs['seed'] = np.random.randint(0, np.iinfo(np.uint32).max, dtype=np.int64)\n",
    "        if bounds is None:\n",
    "            grad = (funcf(x + ck*delta, **fkwargs) - funcf(x - ck*delta, **fkwargs)) / (2*ck*delta)\n",
    "        else:\n",
    "            # ensure evaluation points are feasible\n",
    "            xplus = project(x + ck*delta)\n",
    "            xminus = project(x - ck*delta)\n",
    "            grad = (funcf(xplus, **fkwargs) - funcf(xminus, **fkwargs)) / (xplus-xminus)\n",
    "        x = project(x - ak*grad)\n",
    "        # print 100 status updates if disp=True\n",
    "        if disp and (k % (niter//100)) == 0:\n",
    "            print(x)\n",
    "        if callback is not None:\n",
    "            callback(x)\n",
    "    message = 'terminated after reaching max number of iterations'\n",
    "    return OptimizeResult(fun=funcf(x), x=x, nit=niter, nfev=2*niter, message=message, success=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from noisyopt import minimizeSPSA\n",
    "import time\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy import default_sort_key\n",
    "import gc\n",
    "\n",
    "class Result:\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\n",
    "def make_cost_fn(pred_fn, labels, directory, name):\n",
    "    with open(f'{directory}/{name}_costs.csv', 'w') as f:\n",
    "        pass\n",
    "    \n",
    "    with open(f'{directory}/{name}_accs.csv', 'w') as f:\n",
    "        pass\n",
    "\n",
    "    with open(f'{directory}/params_raw.txt', 'w') as f:\n",
    "        pass\n",
    "    \n",
    "    def cost_fn(params, **kwargs):\n",
    "        predictions = pred_fn(params)\n",
    "\n",
    "        cost = -np.sum(labels * np.log(predictions)) / len(labels)  # binary cross-entropy loss\n",
    "        \n",
    "        with open(f'{directory}/{name}_costs.csv', 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "\n",
    "            writer.writerow([cost])\n",
    "\n",
    "            f.close()\n",
    "\n",
    "        acc = np.sum(np.round(predictions) == labels) / len(labels) / 2  # half due to double-counting\n",
    "        \n",
    "        with open(f'{directory}/{name}_accs.csv', 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "\n",
    "            writer.writerow([acc])\n",
    "\n",
    "            f.close()\n",
    "        \n",
    "        if name == \"dev\":\n",
    "            gc.collect()\n",
    "\n",
    "        return cost\n",
    "\n",
    "    return cost_fn\n",
    "\n",
    "def run_circuit_optimization(train_circuits, train_targets, dev_circuits, dev_targets, test_circuits, test_targets, experiment_name, load_params=False):\n",
    "    # collect params\n",
    "\n",
    "    all_circuits = train_circuits + dev_circuits + test_circuits\n",
    "\n",
    "    # sort the symbols since they are returned as a set\n",
    "    parameters = sorted(\n",
    "        {s for circ in all_circuits for s in circ.free_symbols},\n",
    "        key=default_sort_key)\n",
    "    \n",
    "    # make prediction and cost functions\n",
    "\n",
    "    SEED = 0\n",
    "    rng = np.random.default_rng(SEED)\n",
    "\n",
    "    train_pred_fn = make_pred_fn(train_circuits, parameters, rng)\n",
    "    dev_pred_fn = make_pred_fn(dev_circuits, parameters, rng)\n",
    "    test_pred_fn = make_pred_fn(test_circuits, parameters, rng)\n",
    "\n",
    "    train_cost_fn = make_cost_fn(train_pred_fn, train_targets, '../data/experiment_results/generalQC/'+experiment_name, 'train')\n",
    "    dev_cost_fn = make_cost_fn(dev_pred_fn, dev_targets, '../data/experiment_results/generalQC/'+experiment_name, 'dev')\n",
    "\n",
    "    if (load_params):\n",
    "        # load params\n",
    "\n",
    "        x0 = [params_saved[str(param)] for param in parameters]\n",
    "        result = Result(x0)\n",
    "    else:\n",
    "        x0 = np.array(rng.random(len(parameters)))\n",
    "        np.random.seed(SEED)\n",
    "\n",
    "    # train params using SPSA\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # result = minimizeSPSA(train_cost_fn, x0=x0, a=spsa_a, c=spsa_c, niter=10, callback=dev_cost_fn)\n",
    "    if train_params:\n",
    "        result = myMinimizeSPSA(train_cost_fn, x0=x0, a=spsa_a, c=spsa_c, niter=spsa_n_iter, callback=dev_cost_fn)\n",
    "    else:\n",
    "        result = Result(x0)\n",
    "    \n",
    "    finish_time = time.time()\n",
    "\n",
    "    with open(f'../data/experiment_results/generalQC/{experiment_name}/fit_time.txt', 'w') as f:\n",
    "        f.write(str(finish_time-start_time))\n",
    "\n",
    "    train_costs = []\n",
    "    train_accs = []\n",
    "    dev_costs = []\n",
    "    dev_accs = []\n",
    "\n",
    "    with open(f'../data/experiment_results/generalQC/{experiment_name}/train_costs.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "        for row in csv_reader:\n",
    "            train_costs.append(float(row[0]))\n",
    "\n",
    "    with open(f'../data/experiment_results/generalQC/{experiment_name}/train_accs.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "        for row in csv_reader:\n",
    "            train_accs.append(float(row[0]))\n",
    "\n",
    "    with open(f'../data/experiment_results/generalQC/{experiment_name}/dev_costs.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "        for row in csv_reader:\n",
    "            dev_costs.append(float(row[0]))\n",
    "\n",
    "    with open(f'../data/experiment_results/generalQC/{experiment_name}/dev_accs.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "        for row in csv_reader:\n",
    "            dev_accs.append(float(row[0]))\n",
    "\n",
    "    if train_params:\n",
    "        print(\"Fit time:\", finish_time-start_time)\n",
    "        \n",
    "        fig, ((ax_tl, ax_tr), (ax_bl, ax_br)) = plt.subplots(2, 2, sharex=True, sharey='row', figsize=(10, 6))\n",
    "        ax_tl.set_title('Training set')\n",
    "        ax_tr.set_title('Development set')\n",
    "        ax_bl.set_xlabel('Iterations')\n",
    "        ax_br.set_xlabel('Iterations')\n",
    "        ax_bl.set_ylabel('Accuracy')\n",
    "        ax_tl.set_ylabel('Loss')\n",
    "\n",
    "        colours = iter(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "        ax_tl.plot(train_costs[1::2], color=next(colours))  # training evaluates twice per iteration\n",
    "        ax_bl.plot(train_accs[1::2], color=next(colours))   # so take every other entry\n",
    "        ax_tr.plot(dev_costs, color=next(colours))\n",
    "        ax_br.plot(dev_accs, color=next(colours))\n",
    "\n",
    "    # print test accuracy\n",
    "    test_cost_fn = make_cost_fn(test_pred_fn, test_targets, '../data/experiment_results/generalQC/'+experiment_name, 'test')\n",
    "    test_cost_fn(result.x)\n",
    "\n",
    "    with open(f'../data/experiment_results/generalQC/{experiment_name}/test_accs.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "        for row in csv_reader:\n",
    "            print('Test accuracy:', row[0])\n",
    "            test_accs = [row[0]]\n",
    "            break\n",
    "    \n",
    "    paramdict = {}\n",
    "    paramdict_print = {}\n",
    "\n",
    "    for value, parameter in zip(result.x, parameters):\n",
    "        paramdict[parameter] = value\n",
    "        paramdict_print[str(parameter)] = value\n",
    "\n",
    "    with open('../data/experiment_results/generalQC/'+experiment_name+'/params.txt', 'w') as f:\n",
    "        f.write(str(paramdict_print))\n",
    "\n",
    "    # print(paramdict_print)\n",
    "\n",
    "    opt_results = {\n",
    "        \"params\": {\n",
    "            \"parameters\": parameters, \n",
    "            \"x\": result.x\n",
    "            }, \n",
    "        \"train\": {\n",
    "            \"pred_fn\": train_pred_fn, \n",
    "            \"cost_fn\": train_cost_fn, \n",
    "            \"accs\": train_accs[1::2], \n",
    "            \"costs\": train_costs[1::2]\n",
    "            }, \n",
    "        \"dev\": {\n",
    "            \"pred_fn\": dev_pred_fn, \n",
    "            \"cost_fn\": dev_cost_fn, \n",
    "            \"accs\": dev_accs, \n",
    "            \"costs\": dev_costs\n",
    "            }, \n",
    "        \"test\": {\n",
    "            \"pred_fn\": test_pred_fn, \n",
    "            \"cost_fn\": test_cost_fn, \n",
    "            \"accs\": test_accs, \n",
    "            \"costs\": None\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return opt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_results = run_circuit_optimization(train_circuits, train_targets, dev_circuits, dev_targets, test_circuits, test_targets, experiment_name, load_params)\n",
    "\n",
    "train_pred_fn, train_cost_fn, train_accs, train_costs = list(opt_results[\"train\"].values())\n",
    "dev_pred_fn, dev_cost_fn, dev_accs, dev_costs = list(opt_results[\"dev\"].values())\n",
    "test_pred_fn, test_cost_fn, test_accs, test_costs = list(opt_results[\"test\"].values())\n",
    "parameters, x = list(opt_results[\"params\"].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
